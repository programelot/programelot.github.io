
<!doctype html>














<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/assets/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/assets/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/assets/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Jekyll, NexT" />





  <link rel="alternate" href="/atom.xml" title="REAL" type="application/atom+xml" />


<link rel="shortcut icon" type="image/x-icon" href="/assets/favicon/favicon.ico" />
<link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png">
<link rel="manifest" href="/assets/favicon/site.webmanifest">
<link rel="mask-icon" href="/assets/favicon/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">














<meta name="description" content="Motivation This study is about optimization especially focused on the matrix multiplication. Matrix mutliplication is a topic that has been studied for decades. This study was done from paper reviews about matrix mutliplication. Naive matrix multiplication Naive matrix multiplication is an operation on matrices. Matrix is a set of numbers like follows. $$\begin{pmatrix} a_{1,1} &amp; a_{1,2} &amp; a_{1,3}\\ a_{2,1} &amp; a_{2,2} &amp; a_{2,3} \end{pmatrix} \times \begin{pmatrix} b_{1,1} &amp; b_{1,2}\\ b_{2,1} &amp; b_{2,2}\\ b_{3,1} &amp; b_{3,2}\\ \end{pmatrix} \\= \begin{pmatrix} a_{1,1} \cdot b_{1,1} + a_{1,2} \cdot b_{2,1} + a_{1,3} \cdot b_{3,1} &amp; a_{1,1} \cdot b_{1,2} + a_{1,2} \cdot b_{2,2} + a_{1,3} \cdot b_{3,2}\\ a_{2,1} \cdot b_{1,1} + a_{2,2} \cdot b_{2,1} + a_{2,3} \cdot b_{3,1} &amp; a_{2,1} \cdot b_{1,2} + a_{2,2} \cdot b_{2,2} + a_{2,3} \cdot b_{3,2}\\ \end{pmatrix}$$ In other words, $$c_{i,j} = \sum\limits_{k=1}^{K}a_{i,k}b_{k,j}$$ For $(i,j) \leftarrow (1,1), \cdots, (n,m)$ in parellel For $k \leftarrow 1, \cdots, K$ $c_{i,j} = c_{i,j} + a_{i,k}b_{k,j}$ Algorithm 1. Naive matrix multiplication This algorithm take $O(n^3)$.out matrix mutliplication. Here are implementations for it. CPU based GPU based Dense matrix Cache optimization The basic optimization will be use cache to reduce memory usage. For $(i,j) \leftarrow (1,1), \cdots, (n,m)$ in parellel $\text{local_cache} \leftarrow 0$ For $k \leftarrow 1, \cdots, K$ $\text{local_cache} = \text{local_cache} + a_{i,k}b_{k,j}$ $c_{i,j} = \text{local_cache}$ Algorithm 2. Naive matrix multiplication This algorithm take $O(n^3)$.out matrix mutliplication. Here is a GPU implementation for it. Blocked matrix multiplication multiplication Matrix can be subdivided into blocks. In other words, $$A = \begin{pmatrix} a_{1,1} &amp; a_{1,2} &amp; a_{1,3} &amp; a_{1,4}\\ a_{2,1} &amp; a_{2,2} &amp; a_{2,3} &amp; a_{2,4}\\ a_{3,1} &amp; a_{3,2} &amp; a_{3,3} &amp; a_{3,4}\\ a_{4,1} &amp; a_{4,2} &amp; a_{4,3} &amp; a_{4,4} \end{pmatrix} = \begin{pmatrix} A_{1,1} &amp; A_{1,2}\\ A_{2,1} &amp; A_{2,2} \end{pmatrix} $$ For $$A_{1,1} = \begin{pmatrix} a_{1,1} &amp; a_{1,2}\\ a_{2,1} &amp; a_{2,2} \end{pmatrix} \hspace{1cm} A_{1,2} = \begin{pmatrix} a_{1,3} &amp; a_{1,4}\\ a_{2,3} &amp; a_{2,4} \end{pmatrix} $$ $$A_{2,1} = \begin{pmatrix} a_{3,1} &amp; a_{3,2}\\ a_{4,1} &amp; a_{4,2} \end{pmatrix} \hspace{1cm} A_{2,2} = \begin{pmatrix} a_{3,3} &amp; a_{3,4}\\ a_{4,3} &amp; a_{4,4} \end{pmatrix} $$ Matrix multiplication can be computed by the blocked way either. Notice that it assumed that blockSize divides well for matrix. However, it can be easily generalized. $$C_{i,j} = \sum\limits_{k=1}^{K/BlockSize}A_{i,k}B_{k,j}$$ For $(i,j) \leftarrow (1,1), \cdots, (n/BlockSize, m/BlockSize)$ in parellel $x_{base} \leftarrow (i - 1) \times BlockSize$ $y_{base} \leftarrow (j - 1) \times BlockSize$ For $(x,y) \leftarrow (1,1), \cdots, (BlockSize, BlockSize)$ in parellel For $k \leftarrow 1, \cdots, K$ $C_{x + x_{base},y + y_{base}} += a_{x + x_{base},k} \times b_{k,y + y_{base}}$ Algorithm 3. Blocked matrix multiplication Here is a GPU implementation for it. Shared memory There is an optimization that can be applied when algorithm divided to blocks and there is a shared memory either. It assumed that there is a shared memory size of $BlockSize \times BlockSize$. For $(i,j) \leftarrow (1,1), \cdots, (n/BlockSize, m/BlockSize)$ in parellel $\text{local_cache} \leftarrow 0$ $x_{base} \leftarrow (i - 1) \times BlockSize$ $y_{base} \leftarrow (j - 1) \times BlockSize$ $A \leftarrow \text{matrix that size of }BlockSize \times BlockSize$ $B \leftarrow \text{matrix that size of }BlockSize \times BlockSize$ For $k \leftarrow 1, \cdots, K/BlockSize$ $k_{base} \leftarrow (k - 1) \times BlockSize$ For $(x,y) \leftarrow (1,1), \cdots, (BlockSize, BlockSize)$ in parellel $A_{x,y} = a_{x + x_{base},y + k_{base}}$ $B_{x,y} = b_{x + k_{base},y + y_{base}}$ For $(x,y) \leftarrow (1,1), \cdots, (BlockSize, BlockSize)$ in parellel For $k \leftarrow 1, \cdots, BlockSize$ $\text{local_cache} = \text{local_cache} + A_{x, k} \times B_{k, y}$ For $(x,y) \leftarrow (1,1), \cdots, (BlockSize, BlockSize)$ in parellel $C_{x + x_{base},y + y_{base}} = \text{local_cache}$ Algorithm 4. Naive matrix multiplication This algorithm take $O(n^3)$. There are some GPU implementations here. Notice that there are possible 4 combinations for shared memory matrices by transposing matrix. Shared memory 1 Shared memory 2 Shared memory 3 Shared memory 4 Strassen&apos;s algorithm Strassen&apos;s algorithm is an algorithm that can compute matrix multiplication fast. In general, strassen&apos;s algorithm takes $O(n^{\log_2 7})$. Notice that strassen&apos;s algorithm works with even number of rows and colums. However, it can be fixed by padding numbers at the end of row and column. $ \begin{array}{lcl} M_1 &amp; = &amp; (A_{1,1} + A_{2,2})(B_{1,1} + B_{2,2}) \\ M_2 &amp; = &amp; (A_{2,1} + A_{2,2})B_{1,1} \\ M_3 &amp; = &amp; A_{1,1}(B_{1,2} - B_{2,2}) \\ M_4 &amp; = &amp; A_{2,2}(B_{2,1} - B_{1,1}) \\ M_5 &amp; = &amp; (A_{1,1} + A_{1,2})B_{2,2} \\ M_6 &amp; = &amp; (A_{2,1} - A_{1,1})(B_{1,1} + B_{1,2}) \\ M_7 &amp; = &amp; (A_{1,2} - A_{2,2})(B_{2,1} + B_{2,2}) \\ \end{array} $ $$ \begin{pmatrix} C_{1,1} &amp; C_{1,2}\\ C_{2,1} &amp; C_{2,2} \end{pmatrix} = \begin{pmatrix} M_1 + M_4 - M_5 + M_7 &amp; M_3 + M_5\\ M_2 + M_4 &amp; M_1 - M_2 + M_3 + M_6 \end{pmatrix} $$ This algorithm uses 7 mutiplications and 18 additions. From the master theorem, it takes $O(n^{\log_2 7})$. Notice that naive matrix multiplication have 8 multiplications and 4 additions in the blocked way. To implement this on GPU, there are some problems need to be solved. First problem is a memory allocation problem. Since it need some extra values, it needs more memory spaces. Noticet that these memory spaces are needed because it will be called recursively. In general, this algorithm requires $\frac{21}{4}n^2$ to compute $n$ by $n$ matrix at depth 1. If algorithm wants to go deeper, it needs more memory spaces like $\frac{21}{4}n^2 = 5.25n^2, (\frac{21}{4} + \frac{147}{16})n^2 = 14.4375n^2$. Which may too big to be used at some point. Therefore, this algorithm decided to compute $M_1$ to $M_7$ one by one to reduce memory usage. It will result in smaller extra memory space like $\frac{3}{4}n^2 = 0.75n^2, (\frac{3}{4} + \frac{15}{16})n^2 = 0.9375n^2$ to compute $n$ by $n$ matrix. Notice that it will less than $n^2$ with any depth. At the same time, to remove memory allocation overhead, it precomputes memory usage and allocate it first then computes with pre-allocated memory spaces. Here is a GPU implementation for it. Now, here is the point to think about second problem which is a computation order. Thinking about the memory usage, if it computes $M2$, there is no need to load $B_{1,1}$ on actual memory to compute $M2$. Therefore, it can be ignored. Like this, if both algorithm uses the same values or simillar values, it can reduce memory usage. Each $M_i$ can be thought as $S_i \times T_i$ then edit distance can be constructed as follow. $\text{Distance}(\text{State},\text{Target}) = \sum\limits_{i=1}^{2}\sum\limits_{j=1}^{2} (\unicode{x1D7D9}(\text{Target} \in A_{i,j}, \text{State} ot\in A_{i,j}) + \unicode{x1D7D9}(\text{State} \in A_{i,j}, \text{Target} ot\in A_{i,j}))$ $\text{EditDistance(S)}_{i,j} = \operatorname{min}(\text{Distance}(S_i, S_j), 1 + \text{Distance}(-S_i, S_j), 1 + \text{Distance}(\emptyset, S_j))$ $\text{EditDistance(T)}_{i,j} = \operatorname{min}(\text{Distance}(T_i, T_j), 1 + \text{Distance}(-T_i, T_j), 1 + \text{Distance}(\emptyset, T_j))$ Notice that there are three cases to update values. Just update values on need($\text{Distance}(S_i, S_j)$) Flip values and update($1 + \text{Distance}(-S_i, S_j)$) Empty values and update($1 + \text{Distance}(\emptyset, S_j)$) Then, it changed to traversal problem. This problem is an ordering problem that order $M_1$ to $M_7$ to minimize sum of edit distance. To select distance, there are three ways. Update matrix and input to new one and add a cost of $(EditDistance(S)(k, k+1) + EditDistance(T)(k, k+1))$. In this case, $S_k$ and $T_k$ will be updated. If $T_k$ has only one element from possible values. Update matrix to new one and add a cost of $(EditDistance(S)(k, k+1))$. In this case, $S_k$ will be updated. If $S_k$ has only one element from possible values. Update input to new one and add a cost of $(EditDistance(T)(k, k+1))$. In this case, $T_k$ will be updated. In this case, it results in optimized implementation at here. Notice that there were 18 setMat and 18 addMat operations but now it has 14 setMat and 18 addMat operations. Interesting thing is it never flip or update values but it just empty out and fill it from the ground. Winograd&apos;s algorithm $ \begin{array}{lcl} S_1 &amp; = &amp; A_{2,1} + A_{2,2} &amp; \hspace{1cm} &amp; S_2 &amp; = &amp; S_1 - A_{1,1} \\ S_3 &amp; = &amp; A_{1,1} - A_{2,1} &amp; \hspace{1cm} &amp; S_4 &amp; = &amp; A_{1,2} - S_2 \\ T_1 &amp; = &amp; B_{1,2} - B_{1,1} &amp; \hspace{1cm} &amp; T_2 &amp; = &amp; B_{2,2} - T_1 \\ T_3 &amp; = &amp; B_{2,2} - B_{1,2} &amp; \hspace{1cm} &amp; T_4 &amp; = &amp; T_2 - B_{2,1} \\ M_1 &amp; = &amp; A_{1,1}B_{1,1} &amp; \hspace{1cm} &amp; M_2 &amp; = &amp; A_{1,2}B_{2,1} \\ M_3 &amp; = &amp; S_4B_{2,2} &amp; \hspace{1cm} &amp; M_4 &amp; = &amp; A_{2,2}T_4 \\ M_5 &amp; = &amp; S_1T_1 &amp; \hspace{1cm} &amp; M_6 &amp; = &amp; S_2T_2 \\ M_7 &amp; = &amp; S_3T_3 &amp; \hspace{1cm} &amp; U_1 &amp; = &amp; M_1 + M_2 \\ U_2 &amp; = &amp; M_1 + M_6 &amp; \hspace{1cm} &amp; U_3 &amp; = &amp; U_2 + M_7 &amp; \hspace{1cm} \\ U_4 &amp; = &amp; U_2 + M_5 &amp; \hspace{1cm} &amp; U_5 &amp; = &amp; U_4 + M_3 &amp; \hspace{1cm} \\ U_6 &amp; = &amp; U_3 - M_4 &amp; \hspace{1cm} &amp; U_7 &amp; = &amp; U_3 + M_5 \\ \end{array} $ $$ \begin{pmatrix} C_{1,1} &amp; C_{1,2}\\ C_{2,1} &amp; C_{2,2} \end{pmatrix} = \begin{pmatrix} U_1 &amp; U_5 \\ U_6 &amp; U_7 \end{pmatrix} $$ This algorithm uses 7 multiplications and 15 additions. Which does less computations. To implement this, there is another problem unlike Strassen&apos;s algorithm. To compute the final value, it needs a strict order. For example, $S4$ needs $S2$ and $S2$ needs $S1$. Which result in $S1 \rightarrow S2 \rightarrow S4$ order. Therefore, there are few ways to use auxiliary memory spaces. In this project, I used one of them. As a result, it uses the same amount of memory with strassen&apos;s algorithm but it uses much less operations. There wer 14 setMat and 18 addMat operations but now it has 8 setMat and 15 addMat operations. Here is the implementation of it.">
<meta name="keywords" content="Jekyll, NexT">
<meta property="og:type" content="website">
<meta property="og:title" content="Matrix Multiplication Optimization">
<meta property="og:url" content="https://programelot.github.io/projects/MMO/">
<meta property="og:site_name" content="REAL">
<meta property="og:description" content="Motivation This study is about optimization especially focused on the matrix multiplication. Matrix mutliplication is a topic that has been studied for decades. This study was done from paper reviews about matrix mutliplication. Naive matrix multiplication Naive matrix multiplication is an operation on matrices. Matrix is a set of numbers like follows. $$\begin{pmatrix} a_{1,1} &amp; a_{1,2} &amp; a_{1,3}\\ a_{2,1} &amp; a_{2,2} &amp; a_{2,3} \end{pmatrix} \times \begin{pmatrix} b_{1,1} &amp; b_{1,2}\\ b_{2,1} &amp; b_{2,2}\\ b_{3,1} &amp; b_{3,2}\\ \end{pmatrix} \\= \begin{pmatrix} a_{1,1} \cdot b_{1,1} + a_{1,2} \cdot b_{2,1} + a_{1,3} \cdot b_{3,1} &amp; a_{1,1} \cdot b_{1,2} + a_{1,2} \cdot b_{2,2} + a_{1,3} \cdot b_{3,2}\\ a_{2,1} \cdot b_{1,1} + a_{2,2} \cdot b_{2,1} + a_{2,3} \cdot b_{3,1} &amp; a_{2,1} \cdot b_{1,2} + a_{2,2} \cdot b_{2,2} + a_{2,3} \cdot b_{3,2}\\ \end{pmatrix}$$ In other words, $$c_{i,j} = \sum\limits_{k=1}^{K}a_{i,k}b_{k,j}$$ For $(i,j) \leftarrow (1,1), \cdots, (n,m)$ in parellel For $k \leftarrow 1, \cdots, K$ $c_{i,j} = c_{i,j} + a_{i,k}b_{k,j}$ Algorithm 1. Naive matrix multiplication This algorithm take $O(n^3)$.out matrix mutliplication. Here are implementations for it. CPU based GPU based Dense matrix Cache optimization The basic optimization will be use cache to reduce memory usage. For $(i,j) \leftarrow (1,1), \cdots, (n,m)$ in parellel $\text{local_cache} \leftarrow 0$ For $k \leftarrow 1, \cdots, K$ $\text{local_cache} = \text{local_cache} + a_{i,k}b_{k,j}$ $c_{i,j} = \text{local_cache}$ Algorithm 2. Naive matrix multiplication This algorithm take $O(n^3)$.out matrix mutliplication. Here is a GPU implementation for it. Blocked matrix multiplication multiplication Matrix can be subdivided into blocks. In other words, $$A = \begin{pmatrix} a_{1,1} &amp; a_{1,2} &amp; a_{1,3} &amp; a_{1,4}\\ a_{2,1} &amp; a_{2,2} &amp; a_{2,3} &amp; a_{2,4}\\ a_{3,1} &amp; a_{3,2} &amp; a_{3,3} &amp; a_{3,4}\\ a_{4,1} &amp; a_{4,2} &amp; a_{4,3} &amp; a_{4,4} \end{pmatrix} = \begin{pmatrix} A_{1,1} &amp; A_{1,2}\\ A_{2,1} &amp; A_{2,2} \end{pmatrix} $$ For $$A_{1,1} = \begin{pmatrix} a_{1,1} &amp; a_{1,2}\\ a_{2,1} &amp; a_{2,2} \end{pmatrix} \hspace{1cm} A_{1,2} = \begin{pmatrix} a_{1,3} &amp; a_{1,4}\\ a_{2,3} &amp; a_{2,4} \end{pmatrix} $$ $$A_{2,1} = \begin{pmatrix} a_{3,1} &amp; a_{3,2}\\ a_{4,1} &amp; a_{4,2} \end{pmatrix} \hspace{1cm} A_{2,2} = \begin{pmatrix} a_{3,3} &amp; a_{3,4}\\ a_{4,3} &amp; a_{4,4} \end{pmatrix} $$ Matrix multiplication can be computed by the blocked way either. Notice that it assumed that blockSize divides well for matrix. However, it can be easily generalized. $$C_{i,j} = \sum\limits_{k=1}^{K/BlockSize}A_{i,k}B_{k,j}$$ For $(i,j) \leftarrow (1,1), \cdots, (n/BlockSize, m/BlockSize)$ in parellel $x_{base} \leftarrow (i - 1) \times BlockSize$ $y_{base} \leftarrow (j - 1) \times BlockSize$ For $(x,y) \leftarrow (1,1), \cdots, (BlockSize, BlockSize)$ in parellel For $k \leftarrow 1, \cdots, K$ $C_{x + x_{base},y + y_{base}} += a_{x + x_{base},k} \times b_{k,y + y_{base}}$ Algorithm 3. Blocked matrix multiplication Here is a GPU implementation for it. Shared memory There is an optimization that can be applied when algorithm divided to blocks and there is a shared memory either. It assumed that there is a shared memory size of $BlockSize \times BlockSize$. For $(i,j) \leftarrow (1,1), \cdots, (n/BlockSize, m/BlockSize)$ in parellel $\text{local_cache} \leftarrow 0$ $x_{base} \leftarrow (i - 1) \times BlockSize$ $y_{base} \leftarrow (j - 1) \times BlockSize$ $A \leftarrow \text{matrix that size of }BlockSize \times BlockSize$ $B \leftarrow \text{matrix that size of }BlockSize \times BlockSize$ For $k \leftarrow 1, \cdots, K/BlockSize$ $k_{base} \leftarrow (k - 1) \times BlockSize$ For $(x,y) \leftarrow (1,1), \cdots, (BlockSize, BlockSize)$ in parellel $A_{x,y} = a_{x + x_{base},y + k_{base}}$ $B_{x,y} = b_{x + k_{base},y + y_{base}}$ For $(x,y) \leftarrow (1,1), \cdots, (BlockSize, BlockSize)$ in parellel For $k \leftarrow 1, \cdots, BlockSize$ $\text{local_cache} = \text{local_cache} + A_{x, k} \times B_{k, y}$ For $(x,y) \leftarrow (1,1), \cdots, (BlockSize, BlockSize)$ in parellel $C_{x + x_{base},y + y_{base}} = \text{local_cache}$ Algorithm 4. Naive matrix multiplication This algorithm take $O(n^3)$. There are some GPU implementations here. Notice that there are possible 4 combinations for shared memory matrices by transposing matrix. Shared memory 1 Shared memory 2 Shared memory 3 Shared memory 4 Strassen&apos;s algorithm Strassen&apos;s algorithm is an algorithm that can compute matrix multiplication fast. In general, strassen&apos;s algorithm takes $O(n^{\log_2 7})$. Notice that strassen&apos;s algorithm works with even number of rows and colums. However, it can be fixed by padding numbers at the end of row and column. $ \begin{array}{lcl} M_1 &amp; = &amp; (A_{1,1} + A_{2,2})(B_{1,1} + B_{2,2}) \\ M_2 &amp; = &amp; (A_{2,1} + A_{2,2})B_{1,1} \\ M_3 &amp; = &amp; A_{1,1}(B_{1,2} - B_{2,2}) \\ M_4 &amp; = &amp; A_{2,2}(B_{2,1} - B_{1,1}) \\ M_5 &amp; = &amp; (A_{1,1} + A_{1,2})B_{2,2} \\ M_6 &amp; = &amp; (A_{2,1} - A_{1,1})(B_{1,1} + B_{1,2}) \\ M_7 &amp; = &amp; (A_{1,2} - A_{2,2})(B_{2,1} + B_{2,2}) \\ \end{array} $ $$ \begin{pmatrix} C_{1,1} &amp; C_{1,2}\\ C_{2,1} &amp; C_{2,2} \end{pmatrix} = \begin{pmatrix} M_1 + M_4 - M_5 + M_7 &amp; M_3 + M_5\\ M_2 + M_4 &amp; M_1 - M_2 + M_3 + M_6 \end{pmatrix} $$ This algorithm uses 7 mutiplications and 18 additions. From the master theorem, it takes $O(n^{\log_2 7})$. Notice that naive matrix multiplication have 8 multiplications and 4 additions in the blocked way. To implement this on GPU, there are some problems need to be solved. First problem is a memory allocation problem. Since it need some extra values, it needs more memory spaces. Noticet that these memory spaces are needed because it will be called recursively. In general, this algorithm requires $\frac{21}{4}n^2$ to compute $n$ by $n$ matrix at depth 1. If algorithm wants to go deeper, it needs more memory spaces like $\frac{21}{4}n^2 = 5.25n^2, (\frac{21}{4} + \frac{147}{16})n^2 = 14.4375n^2$. Which may too big to be used at some point. Therefore, this algorithm decided to compute $M_1$ to $M_7$ one by one to reduce memory usage. It will result in smaller extra memory space like $\frac{3}{4}n^2 = 0.75n^2, (\frac{3}{4} + \frac{15}{16})n^2 = 0.9375n^2$ to compute $n$ by $n$ matrix. Notice that it will less than $n^2$ with any depth. At the same time, to remove memory allocation overhead, it precomputes memory usage and allocate it first then computes with pre-allocated memory spaces. Here is a GPU implementation for it. Now, here is the point to think about second problem which is a computation order. Thinking about the memory usage, if it computes $M2$, there is no need to load $B_{1,1}$ on actual memory to compute $M2$. Therefore, it can be ignored. Like this, if both algorithm uses the same values or simillar values, it can reduce memory usage. Each $M_i$ can be thought as $S_i \times T_i$ then edit distance can be constructed as follow. $\text{Distance}(\text{State},\text{Target}) = \sum\limits_{i=1}^{2}\sum\limits_{j=1}^{2} (\unicode{x1D7D9}(\text{Target} \in A_{i,j}, \text{State} ot\in A_{i,j}) + \unicode{x1D7D9}(\text{State} \in A_{i,j}, \text{Target} ot\in A_{i,j}))$ $\text{EditDistance(S)}_{i,j} = \operatorname{min}(\text{Distance}(S_i, S_j), 1 + \text{Distance}(-S_i, S_j), 1 + \text{Distance}(\emptyset, S_j))$ $\text{EditDistance(T)}_{i,j} = \operatorname{min}(\text{Distance}(T_i, T_j), 1 + \text{Distance}(-T_i, T_j), 1 + \text{Distance}(\emptyset, T_j))$ Notice that there are three cases to update values. Just update values on need($\text{Distance}(S_i, S_j)$) Flip values and update($1 + \text{Distance}(-S_i, S_j)$) Empty values and update($1 + \text{Distance}(\emptyset, S_j)$) Then, it changed to traversal problem. This problem is an ordering problem that order $M_1$ to $M_7$ to minimize sum of edit distance. To select distance, there are three ways. Update matrix and input to new one and add a cost of $(EditDistance(S)(k, k+1) + EditDistance(T)(k, k+1))$. In this case, $S_k$ and $T_k$ will be updated. If $T_k$ has only one element from possible values. Update matrix to new one and add a cost of $(EditDistance(S)(k, k+1))$. In this case, $S_k$ will be updated. If $S_k$ has only one element from possible values. Update input to new one and add a cost of $(EditDistance(T)(k, k+1))$. In this case, $T_k$ will be updated. In this case, it results in optimized implementation at here. Notice that there were 18 setMat and 18 addMat operations but now it has 14 setMat and 18 addMat operations. Interesting thing is it never flip or update values but it just empty out and fill it from the ground. Winograd&apos;s algorithm $ \begin{array}{lcl} S_1 &amp; = &amp; A_{2,1} + A_{2,2} &amp; \hspace{1cm} &amp; S_2 &amp; = &amp; S_1 - A_{1,1} \\ S_3 &amp; = &amp; A_{1,1} - A_{2,1} &amp; \hspace{1cm} &amp; S_4 &amp; = &amp; A_{1,2} - S_2 \\ T_1 &amp; = &amp; B_{1,2} - B_{1,1} &amp; \hspace{1cm} &amp; T_2 &amp; = &amp; B_{2,2} - T_1 \\ T_3 &amp; = &amp; B_{2,2} - B_{1,2} &amp; \hspace{1cm} &amp; T_4 &amp; = &amp; T_2 - B_{2,1} \\ M_1 &amp; = &amp; A_{1,1}B_{1,1} &amp; \hspace{1cm} &amp; M_2 &amp; = &amp; A_{1,2}B_{2,1} \\ M_3 &amp; = &amp; S_4B_{2,2} &amp; \hspace{1cm} &amp; M_4 &amp; = &amp; A_{2,2}T_4 \\ M_5 &amp; = &amp; S_1T_1 &amp; \hspace{1cm} &amp; M_6 &amp; = &amp; S_2T_2 \\ M_7 &amp; = &amp; S_3T_3 &amp; \hspace{1cm} &amp; U_1 &amp; = &amp; M_1 + M_2 \\ U_2 &amp; = &amp; M_1 + M_6 &amp; \hspace{1cm} &amp; U_3 &amp; = &amp; U_2 + M_7 &amp; \hspace{1cm} \\ U_4 &amp; = &amp; U_2 + M_5 &amp; \hspace{1cm} &amp; U_5 &amp; = &amp; U_4 + M_3 &amp; \hspace{1cm} \\ U_6 &amp; = &amp; U_3 - M_4 &amp; \hspace{1cm} &amp; U_7 &amp; = &amp; U_3 + M_5 \\ \end{array} $ $$ \begin{pmatrix} C_{1,1} &amp; C_{1,2}\\ C_{2,1} &amp; C_{2,2} \end{pmatrix} = \begin{pmatrix} U_1 &amp; U_5 \\ U_6 &amp; U_7 \end{pmatrix} $$ This algorithm uses 7 multiplications and 15 additions. Which does less computations. To implement this, there is another problem unlike Strassen&apos;s algorithm. To compute the final value, it needs a strict order. For example, $S4$ needs $S2$ and $S2$ needs $S1$. Which result in $S1 \rightarrow S2 \rightarrow S4$ order. Therefore, there are few ways to use auxiliary memory spaces. In this project, I used one of them. As a result, it uses the same amount of memory with strassen&apos;s algorithm but it uses much less operations. There wer 14 setMat and 18 addMat operations but now it has 8 setMat and 15 addMat operations. Here is the implementation of it.">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Matrix Multiplication Optimization">
<meta name="twitter:description" content="Motivation This study is about optimization especially focused on the matrix multiplication. Matrix mutliplication is a topic that has been studied for decades. This study was done from paper reviews about matrix mutliplication. Naive matrix multiplication Naive matrix multiplication is an operation on matrices. Matrix is a set of numbers like follows. $$\begin{pmatrix} a_{1,1} &amp; a_{1,2} &amp; a_{1,3}\\ a_{2,1} &amp; a_{2,2} &amp; a_{2,3} \end{pmatrix} \times \begin{pmatrix} b_{1,1} &amp; b_{1,2}\\ b_{2,1} &amp; b_{2,2}\\ b_{3,1} &amp; b_{3,2}\\ \end{pmatrix} \\= \begin{pmatrix} a_{1,1} \cdot b_{1,1} + a_{1,2} \cdot b_{2,1} + a_{1,3} \cdot b_{3,1} &amp; a_{1,1} \cdot b_{1,2} + a_{1,2} \cdot b_{2,2} + a_{1,3} \cdot b_{3,2}\\ a_{2,1} \cdot b_{1,1} + a_{2,2} \cdot b_{2,1} + a_{2,3} \cdot b_{3,1} &amp; a_{2,1} \cdot b_{1,2} + a_{2,2} \cdot b_{2,2} + a_{2,3} \cdot b_{3,2}\\ \end{pmatrix}$$ In other words, $$c_{i,j} = \sum\limits_{k=1}^{K}a_{i,k}b_{k,j}$$ For $(i,j) \leftarrow (1,1), \cdots, (n,m)$ in parellel For $k \leftarrow 1, \cdots, K$ $c_{i,j} = c_{i,j} + a_{i,k}b_{k,j}$ Algorithm 1. Naive matrix multiplication This algorithm take $O(n^3)$.out matrix mutliplication. Here are implementations for it. CPU based GPU based Dense matrix Cache optimization The basic optimization will be use cache to reduce memory usage. For $(i,j) \leftarrow (1,1), \cdots, (n,m)$ in parellel $\text{local_cache} \leftarrow 0$ For $k \leftarrow 1, \cdots, K$ $\text{local_cache} = \text{local_cache} + a_{i,k}b_{k,j}$ $c_{i,j} = \text{local_cache}$ Algorithm 2. Naive matrix multiplication This algorithm take $O(n^3)$.out matrix mutliplication. Here is a GPU implementation for it. Blocked matrix multiplication multiplication Matrix can be subdivided into blocks. In other words, $$A = \begin{pmatrix} a_{1,1} &amp; a_{1,2} &amp; a_{1,3} &amp; a_{1,4}\\ a_{2,1} &amp; a_{2,2} &amp; a_{2,3} &amp; a_{2,4}\\ a_{3,1} &amp; a_{3,2} &amp; a_{3,3} &amp; a_{3,4}\\ a_{4,1} &amp; a_{4,2} &amp; a_{4,3} &amp; a_{4,4} \end{pmatrix} = \begin{pmatrix} A_{1,1} &amp; A_{1,2}\\ A_{2,1} &amp; A_{2,2} \end{pmatrix} $$ For $$A_{1,1} = \begin{pmatrix} a_{1,1} &amp; a_{1,2}\\ a_{2,1} &amp; a_{2,2} \end{pmatrix} \hspace{1cm} A_{1,2} = \begin{pmatrix} a_{1,3} &amp; a_{1,4}\\ a_{2,3} &amp; a_{2,4} \end{pmatrix} $$ $$A_{2,1} = \begin{pmatrix} a_{3,1} &amp; a_{3,2}\\ a_{4,1} &amp; a_{4,2} \end{pmatrix} \hspace{1cm} A_{2,2} = \begin{pmatrix} a_{3,3} &amp; a_{3,4}\\ a_{4,3} &amp; a_{4,4} \end{pmatrix} $$ Matrix multiplication can be computed by the blocked way either. Notice that it assumed that blockSize divides well for matrix. However, it can be easily generalized. $$C_{i,j} = \sum\limits_{k=1}^{K/BlockSize}A_{i,k}B_{k,j}$$ For $(i,j) \leftarrow (1,1), \cdots, (n/BlockSize, m/BlockSize)$ in parellel $x_{base} \leftarrow (i - 1) \times BlockSize$ $y_{base} \leftarrow (j - 1) \times BlockSize$ For $(x,y) \leftarrow (1,1), \cdots, (BlockSize, BlockSize)$ in parellel For $k \leftarrow 1, \cdots, K$ $C_{x + x_{base},y + y_{base}} += a_{x + x_{base},k} \times b_{k,y + y_{base}}$ Algorithm 3. Blocked matrix multiplication Here is a GPU implementation for it. Shared memory There is an optimization that can be applied when algorithm divided to blocks and there is a shared memory either. It assumed that there is a shared memory size of $BlockSize \times BlockSize$. For $(i,j) \leftarrow (1,1), \cdots, (n/BlockSize, m/BlockSize)$ in parellel $\text{local_cache} \leftarrow 0$ $x_{base} \leftarrow (i - 1) \times BlockSize$ $y_{base} \leftarrow (j - 1) \times BlockSize$ $A \leftarrow \text{matrix that size of }BlockSize \times BlockSize$ $B \leftarrow \text{matrix that size of }BlockSize \times BlockSize$ For $k \leftarrow 1, \cdots, K/BlockSize$ $k_{base} \leftarrow (k - 1) \times BlockSize$ For $(x,y) \leftarrow (1,1), \cdots, (BlockSize, BlockSize)$ in parellel $A_{x,y} = a_{x + x_{base},y + k_{base}}$ $B_{x,y} = b_{x + k_{base},y + y_{base}}$ For $(x,y) \leftarrow (1,1), \cdots, (BlockSize, BlockSize)$ in parellel For $k \leftarrow 1, \cdots, BlockSize$ $\text{local_cache} = \text{local_cache} + A_{x, k} \times B_{k, y}$ For $(x,y) \leftarrow (1,1), \cdots, (BlockSize, BlockSize)$ in parellel $C_{x + x_{base},y + y_{base}} = \text{local_cache}$ Algorithm 4. Naive matrix multiplication This algorithm take $O(n^3)$. There are some GPU implementations here. Notice that there are possible 4 combinations for shared memory matrices by transposing matrix. Shared memory 1 Shared memory 2 Shared memory 3 Shared memory 4 Strassen&apos;s algorithm Strassen&apos;s algorithm is an algorithm that can compute matrix multiplication fast. In general, strassen&apos;s algorithm takes $O(n^{\log_2 7})$. Notice that strassen&apos;s algorithm works with even number of rows and colums. However, it can be fixed by padding numbers at the end of row and column. $ \begin{array}{lcl} M_1 &amp; = &amp; (A_{1,1} + A_{2,2})(B_{1,1} + B_{2,2}) \\ M_2 &amp; = &amp; (A_{2,1} + A_{2,2})B_{1,1} \\ M_3 &amp; = &amp; A_{1,1}(B_{1,2} - B_{2,2}) \\ M_4 &amp; = &amp; A_{2,2}(B_{2,1} - B_{1,1}) \\ M_5 &amp; = &amp; (A_{1,1} + A_{1,2})B_{2,2} \\ M_6 &amp; = &amp; (A_{2,1} - A_{1,1})(B_{1,1} + B_{1,2}) \\ M_7 &amp; = &amp; (A_{1,2} - A_{2,2})(B_{2,1} + B_{2,2}) \\ \end{array} $ $$ \begin{pmatrix} C_{1,1} &amp; C_{1,2}\\ C_{2,1} &amp; C_{2,2} \end{pmatrix} = \begin{pmatrix} M_1 + M_4 - M_5 + M_7 &amp; M_3 + M_5\\ M_2 + M_4 &amp; M_1 - M_2 + M_3 + M_6 \end{pmatrix} $$ This algorithm uses 7 mutiplications and 18 additions. From the master theorem, it takes $O(n^{\log_2 7})$. Notice that naive matrix multiplication have 8 multiplications and 4 additions in the blocked way. To implement this on GPU, there are some problems need to be solved. First problem is a memory allocation problem. Since it need some extra values, it needs more memory spaces. Noticet that these memory spaces are needed because it will be called recursively. In general, this algorithm requires $\frac{21}{4}n^2$ to compute $n$ by $n$ matrix at depth 1. If algorithm wants to go deeper, it needs more memory spaces like $\frac{21}{4}n^2 = 5.25n^2, (\frac{21}{4} + \frac{147}{16})n^2 = 14.4375n^2$. Which may too big to be used at some point. Therefore, this algorithm decided to compute $M_1$ to $M_7$ one by one to reduce memory usage. It will result in smaller extra memory space like $\frac{3}{4}n^2 = 0.75n^2, (\frac{3}{4} + \frac{15}{16})n^2 = 0.9375n^2$ to compute $n$ by $n$ matrix. Notice that it will less than $n^2$ with any depth. At the same time, to remove memory allocation overhead, it precomputes memory usage and allocate it first then computes with pre-allocated memory spaces. Here is a GPU implementation for it. Now, here is the point to think about second problem which is a computation order. Thinking about the memory usage, if it computes $M2$, there is no need to load $B_{1,1}$ on actual memory to compute $M2$. Therefore, it can be ignored. Like this, if both algorithm uses the same values or simillar values, it can reduce memory usage. Each $M_i$ can be thought as $S_i \times T_i$ then edit distance can be constructed as follow. $\text{Distance}(\text{State},\text{Target}) = \sum\limits_{i=1}^{2}\sum\limits_{j=1}^{2} (\unicode{x1D7D9}(\text{Target} \in A_{i,j}, \text{State} ot\in A_{i,j}) + \unicode{x1D7D9}(\text{State} \in A_{i,j}, \text{Target} ot\in A_{i,j}))$ $\text{EditDistance(S)}_{i,j} = \operatorname{min}(\text{Distance}(S_i, S_j), 1 + \text{Distance}(-S_i, S_j), 1 + \text{Distance}(\emptyset, S_j))$ $\text{EditDistance(T)}_{i,j} = \operatorname{min}(\text{Distance}(T_i, T_j), 1 + \text{Distance}(-T_i, T_j), 1 + \text{Distance}(\emptyset, T_j))$ Notice that there are three cases to update values. Just update values on need($\text{Distance}(S_i, S_j)$) Flip values and update($1 + \text{Distance}(-S_i, S_j)$) Empty values and update($1 + \text{Distance}(\emptyset, S_j)$) Then, it changed to traversal problem. This problem is an ordering problem that order $M_1$ to $M_7$ to minimize sum of edit distance. To select distance, there are three ways. Update matrix and input to new one and add a cost of $(EditDistance(S)(k, k+1) + EditDistance(T)(k, k+1))$. In this case, $S_k$ and $T_k$ will be updated. If $T_k$ has only one element from possible values. Update matrix to new one and add a cost of $(EditDistance(S)(k, k+1))$. In this case, $S_k$ will be updated. If $S_k$ has only one element from possible values. Update input to new one and add a cost of $(EditDistance(T)(k, k+1))$. In this case, $T_k$ will be updated. In this case, it results in optimized implementation at here. Notice that there were 18 setMat and 18 addMat operations but now it has 14 setMat and 18 addMat operations. Interesting thing is it never flip or update values but it just empty out and fill it from the ground. Winograd&apos;s algorithm $ \begin{array}{lcl} S_1 &amp; = &amp; A_{2,1} + A_{2,2} &amp; \hspace{1cm} &amp; S_2 &amp; = &amp; S_1 - A_{1,1} \\ S_3 &amp; = &amp; A_{1,1} - A_{2,1} &amp; \hspace{1cm} &amp; S_4 &amp; = &amp; A_{1,2} - S_2 \\ T_1 &amp; = &amp; B_{1,2} - B_{1,1} &amp; \hspace{1cm} &amp; T_2 &amp; = &amp; B_{2,2} - T_1 \\ T_3 &amp; = &amp; B_{2,2} - B_{1,2} &amp; \hspace{1cm} &amp; T_4 &amp; = &amp; T_2 - B_{2,1} \\ M_1 &amp; = &amp; A_{1,1}B_{1,1} &amp; \hspace{1cm} &amp; M_2 &amp; = &amp; A_{1,2}B_{2,1} \\ M_3 &amp; = &amp; S_4B_{2,2} &amp; \hspace{1cm} &amp; M_4 &amp; = &amp; A_{2,2}T_4 \\ M_5 &amp; = &amp; S_1T_1 &amp; \hspace{1cm} &amp; M_6 &amp; = &amp; S_2T_2 \\ M_7 &amp; = &amp; S_3T_3 &amp; \hspace{1cm} &amp; U_1 &amp; = &amp; M_1 + M_2 \\ U_2 &amp; = &amp; M_1 + M_6 &amp; \hspace{1cm} &amp; U_3 &amp; = &amp; U_2 + M_7 &amp; \hspace{1cm} \\ U_4 &amp; = &amp; U_2 + M_5 &amp; \hspace{1cm} &amp; U_5 &amp; = &amp; U_4 + M_3 &amp; \hspace{1cm} \\ U_6 &amp; = &amp; U_3 - M_4 &amp; \hspace{1cm} &amp; U_7 &amp; = &amp; U_3 + M_5 \\ \end{array} $ $$ \begin{pmatrix} C_{1,1} &amp; C_{1,2}\\ C_{2,1} &amp; C_{2,2} \end{pmatrix} = \begin{pmatrix} U_1 &amp; U_5 \\ U_6 &amp; U_7 \end{pmatrix} $$ This algorithm uses 7 multiplications and 15 additions. Which does less computations. To implement this, there is another problem unlike Strassen&apos;s algorithm. To compute the final value, it needs a strict order. For example, $S4$ needs $S2$ and $S2$ needs $S1$. Which result in $S1 \rightarrow S2 \rightarrow S4$ order. Therefore, there are few ways to use auxiliary memory spaces. In this project, I used one of them. As a result, it uses the same amount of memory with strassen&apos;s algorithm but it uses much less operations. There wer 14 setMat and 18 addMat operations but now it has 8 setMat and 15 addMat operations. Here is the implementation of it.">


<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://programelot.github.io/"/>





  <title></title>
  
















</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  

  <div class="container sidebar-position-left  ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">REAL</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-project">
          <a href="/projects/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Project
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>


 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

<div id="posts" class="posts-expand">
  
  

  

  
  
  

  <article class="post post-type- " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://programelot.github.io/projects/MMO/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Programelot">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="assets/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="REAL">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
          
          
            Matrix Multiplication Optimization
          
        </h1>
        


        <div class="post-meta">
          
            
          

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/projects/MMO/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="/projects/MMO/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

          
            
                <div class="post-description">
                    
                </div>
            
          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
  
  












  <p id="Motivation"></p><h2>Motivation</h2>
This study is about optimization especially focused on the matrix multiplication.<br>
Matrix mutliplication is a topic that has been studied for decades.<br>
This study was done from paper reviews about matrix mutliplication.
<hr>
<p id="Naive matrix multiplication"></p><h3>Naive matrix multiplication</h3>
Naive matrix multiplication is an operation on <a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)">matrices</a>.<br>
Matrix is a set of numbers like follows.<br>
$$\begin{pmatrix}
a_{1,1} & a_{1,2} & a_{1,3}\\
a_{2,1} & a_{2,2} & a_{2,3}
\end{pmatrix} \times \begin{pmatrix}
b_{1,1} & b_{1,2}\\
b_{2,1} & b_{2,2}\\
b_{3,1} & b_{3,2}\\
\end{pmatrix} \\= \begin{pmatrix}
a_{1,1} \cdot b_{1,1} + a_{1,2} \cdot b_{2,1} + a_{1,3} \cdot b_{3,1} & a_{1,1} \cdot b_{1,2} + a_{1,2} \cdot b_{2,2} + a_{1,3} \cdot b_{3,2}\\
a_{2,1} \cdot b_{1,1} + a_{2,2} \cdot b_{2,1} + a_{2,3} \cdot b_{3,1} & a_{2,1} \cdot b_{1,2} + a_{2,2} \cdot b_{2,2} + a_{2,3} \cdot b_{3,2}\\
\end{pmatrix}$$
In other words,
$$c_{i,j} = \sum\limits_{k=1}^{K}a_{i,k}b_{k,j}$$<br>
<div class="alg">
    <b>For</b>  $(i,j) \leftarrow (1,1), \cdots, (n,m)$ in parellel
    <div class="algTab">
        <b>For</b>  $k \leftarrow 1, \cdots, K$
        <div class="algTab">
            $c_{i,j} = c_{i,j} + a_{i,k}b_{k,j}$<br>
        </div>
    </div>
</div>
<p class="algCap">Algorithm 1. Naive matrix multiplication</p>
This algorithm take $O(n^3)$.out matrix mutliplication.<br>
Here are implementations for it.<br>
<ol>
    <li><a href="https://github.com/programelot/MatrixMultiplication/blob/master/DMM/src/MM1.cu">CPU based</a></li>
    <li><a href="https://github.com/programelot/MatrixMultiplication/blob/master/DMM/src/MM2.cu">GPU based</a></li>
</ol>
<hr>
<p id="Dense matrix"></p><h2>Dense matrix</h2>
<hr>
<p id="Cache optimization"></p><h3>Cache optimization</h3>
The basic optimization will be use cache to reduce memory usage.<br>
<div class="alg">
    <b>For</b>  $(i,j) \leftarrow (1,1), \cdots, (n,m)$ in parellel
    <div class="algTab">
        $\text{local_cache} \leftarrow 0$<br>
        <b>For</b>  $k \leftarrow 1, \cdots, K$
        <div class="algTab">
            $\text{local_cache} = \text{local_cache} + a_{i,k}b_{k,j}$<br>
        </div>
        $c_{i,j} = \text{local_cache}$<br>
    </div>
</div>
<p class="algCap">Algorithm 2. Naive matrix multiplication</p>
This algorithm take $O(n^3)$.out matrix mutliplication.<br>
Here is a GPU <a href="https://github.com/programelot/MatrixMultiplication/blob/master/DMM/src/MM3.cu">implementation</a> for it.<br>
<hr>
<p id="Blocked matrix multiplication multiplication"></p><h3>Blocked matrix multiplication multiplication</h3>
Matrix can be subdivided into blocks.<br>
In other words,
    $$A = \begin{pmatrix}
    a_{1,1} & a_{1,2} & a_{1,3} & a_{1,4}\\
    a_{2,1} & a_{2,2} & a_{2,3} & a_{2,4}\\
    a_{3,1} & a_{3,2} & a_{3,3} & a_{3,4}\\
    a_{4,1} & a_{4,2} & a_{4,3} & a_{4,4}
    \end{pmatrix}
    =
    \begin{pmatrix}
    A_{1,1} & A_{1,2}\\
    A_{2,1} & A_{2,2}
    \end{pmatrix}
    $$<br>
    For
    $$A_{1,1} = \begin{pmatrix}
    a_{1,1} & a_{1,2}\\
    a_{2,1} & a_{2,2}
    \end{pmatrix} \hspace{1cm} 
    A_{1,2} = \begin{pmatrix}
    a_{1,3} & a_{1,4}\\
    a_{2,3} & a_{2,4}
    \end{pmatrix}
    $$<br>
    $$A_{2,1} = \begin{pmatrix}
    a_{3,1} & a_{3,2}\\
    a_{4,1} & a_{4,2}
    \end{pmatrix} \hspace{1cm} 
    A_{2,2} = \begin{pmatrix}
    a_{3,3} & a_{3,4}\\
    a_{4,3} & a_{4,4}
    \end{pmatrix}
    $$
Matrix multiplication can be computed by the blocked way either.<br>
Notice that it assumed that blockSize divides well for matrix.<br>
However, it can be easily generalized.<br>
$$C_{i,j} = \sum\limits_{k=1}^{K/BlockSize}A_{i,k}B_{k,j}$$<br>
<div class="alg">
    <b>For</b>  $(i,j) \leftarrow (1,1), \cdots, (n/BlockSize, m/BlockSize)$ in parellel
    <div class="algTab">
        $x_{base} \leftarrow (i - 1) \times BlockSize$<br> 
        $y_{base} \leftarrow (j - 1) \times BlockSize$<br> 
        <b>For</b>  $(x,y) \leftarrow (1,1), \cdots, (BlockSize, BlockSize)$ in parellel
            <div class="algTab">
                <b>For</b>  $k \leftarrow 1, \cdots, K$
            <div class="algTab">
                $C_{x + x_{base},y + y_{base}} +=  a_{x + x_{base},k} \times b_{k,y + y_{base}}$<br>
            </div>
        </div>
    </div>
</div>
<p class="algCap">Algorithm 3. Blocked matrix multiplication</p>
Here is a GPU <a href="https://github.com/programelot/MatrixMultiplication/blob/master/DMM/src/MM4.cu">implementation</a> for it.<br>
<hr>
<p id="Shared memory"></p><h3>Shared memory</h3>
There is an optimization that can be applied when algorithm divided to blocks and there is a shared memory either.
It assumed that there is a shared memory size of $BlockSize \times BlockSize$.
<div class="alg">
    <b>For</b>  $(i,j) \leftarrow (1,1), \cdots, (n/BlockSize, m/BlockSize)$ in parellel
    <div class="algTab">
        $\text{local_cache} \leftarrow 0$<br> 
        $x_{base} \leftarrow (i - 1) \times BlockSize$<br> 
        $y_{base} \leftarrow (j - 1) \times BlockSize$<br> 
        $A \leftarrow \text{matrix that size of }BlockSize \times BlockSize$<br> 
        $B \leftarrow \text{matrix that size of }BlockSize \times BlockSize$<br> 
        <b>For</b>  $k \leftarrow 1, \cdots, K/BlockSize$
        <div class="algTab">
            $k_{base} \leftarrow (k - 1) \times BlockSize$<br> 
            <b>For</b>  $(x,y) \leftarrow (1,1), \cdots, (BlockSize, BlockSize)$ in parellel
            <div class="algTab">
                $A_{x,y} = a_{x + x_{base},y + k_{base}}$<br>
                $B_{x,y} = b_{x + k_{base},y + y_{base}}$<br>
            </div>
            <b>For</b>  $(x,y) \leftarrow (1,1), \cdots, (BlockSize, BlockSize)$ in parellel
            <div class="algTab">
                <b>For</b>  $k \leftarrow 1, \cdots, BlockSize$
                <div class="algTab">
                    $\text{local_cache} = \text{local_cache} + A_{x, k} \times B_{k, y}$<br>
                </div>
            </div>
        </div>
        <b>For</b>  $(x,y) \leftarrow (1,1), \cdots, (BlockSize, BlockSize)$ in parellel
        <div class="algTab">
            $C_{x + x_{base},y + y_{base}} = \text{local_cache}$<br>
        </div>
    </div>
</div>
<p class="algCap">Algorithm 4. Naive matrix multiplication</p>
This algorithm take $O(n^3)$.<br>
There are some GPU implementations here.<br>
Notice that there are possible 4 combinations for shared memory matrices by transposing matrix.<br>
<ol>
    <li><a href="https://github.com/programelot/MatrixMultiplication/blob/master/DMM/src/MM5.cu">Shared memory 1</a></li>
    <li><a href="https://github.com/programelot/MatrixMultiplication/blob/master/DMM/src/MM6.cu">Shared memory 2</a></li>
    <li><a href="https://github.com/programelot/MatrixMultiplication/blob/master/DMM/src/MM7.cu">Shared memory 3</a></li>
    <li><a href="https://github.com/programelot/MatrixMultiplication/blob/master/DMM/src/MM8.cu">Shared memory 4</a></li>
</ol>
<hr>
<p id="Strassen's algorithm"></p><h3>Strassen's algorithm</h3>
Strassen's algorithm is an algorithm that can compute matrix multiplication fast.<br>
In general, strassen's algorithm takes $O(n^{\log_2 7})$.<br>
Notice that strassen's algorithm works with even number of rows and colums.<br>
However, it can be fixed by padding numbers at the end of row and column.<br>
<br>
<div align="center">
    $
    \begin{array}{lcl}
    M_1 & = & (A_{1,1} + A_{2,2})(B_{1,1} + B_{2,2}) \\
    M_2 & = & (A_{2,1} + A_{2,2})B_{1,1} \\
    M_3 & = & A_{1,1}(B_{1,2} - B_{2,2}) \\
    M_4 & = & A_{2,2}(B_{2,1} - B_{1,1}) \\
    M_5 & = & (A_{1,1} + A_{1,2})B_{2,2} \\
    M_6 & = & (A_{2,1} - A_{1,1})(B_{1,1} + B_{1,2}) \\
    M_7 & = & (A_{1,2} - A_{2,2})(B_{2,1} + B_{2,2}) \\
    \end{array}
    $
    $$
    \begin{pmatrix}
    C_{1,1} & C_{1,2}\\
    C_{2,1} & C_{2,2}
    \end{pmatrix}
    =
    \begin{pmatrix}
    M_1 + M_4 - M_5 + M_7 & M_3 + M_5\\
    M_2 + M_4 & M_1 - M_2 + M_3 + M_6
    \end{pmatrix}
    $$
</div>
This algorithm uses 7 mutiplications and 18 additions.<br>
From the <a href="https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms)">master theorem</a>, it takes $O(n^{\log_2 7})$.<br>
Notice that naive matrix multiplication have 8 multiplications and 4 additions in the blocked way.<br>
To implement this on GPU, there are some problems need to be solved.<br>
First problem is a memory allocation problem.<br>
Since it need some extra values, it needs more memory spaces.<br>
Noticet that these memory spaces are needed because it will be called recursively.<br>
In general, this algorithm requires $\frac{21}{4}n^2$ to compute $n$ by $n$ matrix at depth 1.<br> 
If algorithm wants to go deeper, it needs more memory spaces like $\frac{21}{4}n^2 = 5.25n^2, (\frac{21}{4} + \frac{147}{16})n^2 = 14.4375n^2$.<br>
Which may too big to be used at some point.<br>
Therefore, this algorithm decided to compute $M_1$ to $M_7$ one by one to reduce memory usage.<br>
It will result in smaller extra memory space like $\frac{3}{4}n^2 = 0.75n^2, (\frac{3}{4} + \frac{15}{16})n^2 = 0.9375n^2$ to compute $n$ by $n$ matrix.<br>
Notice that it will less than $n^2$ with any depth.<br>
At the same time, to remove memory allocation overhead, it precomputes memory usage and allocate it first then computes with pre-allocated memory spaces.<br>
Here is a GPU <a href="https://github.com/programelot/MatrixMultiplication/blob/master/DMM/src/MM9.cu">implementation</a> for it.<br>
Now, here is the point to think about second problem which is a computation order.<br>
Thinking about the memory usage, if it computes $M2$, there is no need to load $B_{1,1}$ on actual memory to compute $M2$.<br>
Therefore, it can be ignored.<br>
Like this, if both algorithm uses the same values or simillar values, it can reduce memory usage.<br>
Each $M_i$ can be thought as $S_i \times T_i$ then edit distance can be constructed as follow.<br>
<div align="center">
    $\text{Distance}(\text{State},\text{Target}) = \sum\limits_{i=1}^{2}\sum\limits_{j=1}^{2}
    (\unicode{x1D7D9}(\text{Target} \in A_{i,j}, \text{State} \not\in A_{i,j})
    + \unicode{x1D7D9}(\text{State} \in A_{i,j}, \text{Target} \not\in A_{i,j}))$<br>
    $\text{EditDistance(S)}_{i,j} = \operatorname{min}(\text{Distance}(S_i, S_j), 1 + \text{Distance}(-S_i, S_j), 1 + \text{Distance}(\emptyset, S_j))$<br>
    $\text{EditDistance(T)}_{i,j} = \operatorname{min}(\text{Distance}(T_i, T_j), 1 + \text{Distance}(-T_i, T_j), 1 + \text{Distance}(\emptyset, T_j))$<br>
</div>
Notice that there are three cases to update values.
<ol>
    <li>Just update values on need($\text{Distance}(S_i, S_j)$)</li>
    <li>Flip values and update($1 + \text{Distance}(-S_i, S_j)$)</li>
    <li>Empty values and update($1 + \text{Distance}(\emptyset, S_j)$)</li>
</ol>
Then, it changed to traversal problem.<br>
This problem is an ordering problem that order $M_1$ to $M_7$ to minimize sum of edit distance.<br>
To select distance, there are three ways.<br>
<ol>
    <li>Update matrix and input to new one and add a cost of $(EditDistance(S)(k, k+1) + EditDistance(T)(k, k+1))$.<br>
        In this case, $S_k$ and $T_k$ will be updated.
    </li>
    <li>If $T_k$ has only one element from possible values.<br>
        Update matrix to new one and add a cost of $(EditDistance(S)(k, k+1))$.<br>
        In this case, $S_k$ will be updated.
    </li>
    <li>If $S_k$ has only one element from possible values.<br>
        Update input to new one and add a cost of $(EditDistance(T)(k, k+1))$.<br>
        In this case, $T_k$ will be updated.
    </li>
</ol>
In this case, it results in optimized implementation at <a href="https://github.com/programelot/MatrixMultiplication/blob/master/DMM/src/MM10.cu">here</a>.<br>
Notice that there were 18 setMat and 18 addMat operations but now it has 14 setMat and 18 addMat operations.<br>
Interesting thing is it never flip or update values but it just empty out and fill it from the ground.
<hr>
<p id="Winograd's algorithm"></p><h3>Winograd's algorithm</h3>
<div align="center">
    $
    \begin{array}{lcl}
    S_1 & = & A_{2,1} + A_{2,2} & \hspace{1cm} & S_2 & = & S_1 - A_{1,1} \\
    S_3 & = & A_{1,1} - A_{2,1} & \hspace{1cm} & S_4 & = & A_{1,2} - S_2 \\
    T_1 & = & B_{1,2} - B_{1,1} & \hspace{1cm} & T_2 & = & B_{2,2} - T_1 \\
    T_3 & = & B_{2,2} - B_{1,2} & \hspace{1cm} & T_4 & = & T_2 - B_{2,1} \\
    M_1 & = & A_{1,1}B_{1,1} & \hspace{1cm} & M_2 & = & A_{1,2}B_{2,1} \\
    M_3 & = & S_4B_{2,2} & \hspace{1cm} & M_4 & = & A_{2,2}T_4 \\
    M_5 & = & S_1T_1 & \hspace{1cm} & M_6 & = & S_2T_2 \\
    M_7 & = & S_3T_3 & \hspace{1cm} & U_1 & = & M_1 + M_2 \\
    U_2 & = & M_1 + M_6 & \hspace{1cm} & U_3 & = & U_2 + M_7 & \hspace{1cm} \\
    U_4 & = & U_2 + M_5 & \hspace{1cm} & U_5 & = & U_4 + M_3 & \hspace{1cm} \\
    U_6 & = & U_3 - M_4 & \hspace{1cm} & U_7 & = & U_3 + M_5 \\
    \end{array}
    $
    $$
    \begin{pmatrix}
    C_{1,1} & C_{1,2}\\
    C_{2,1} & C_{2,2}
    \end{pmatrix}
    =
    \begin{pmatrix}
    U_1 & U_5 \\
    U_6 & U_7
    \end{pmatrix}
    $$
</div>
This algorithm uses 7 multiplications and 15 additions.<br>
Which does less computations.<br>
To implement this, there is another problem unlike Strassen's algorithm.<br>
To compute the final value, it needs a strict order.<br>
For example, $S4$ needs $S2$ and $S2$ needs $S1$.<br>
Which result in $S1 \rightarrow S2 \rightarrow S4$ order.<br>
Therefore, there are few ways to use auxiliary memory spaces.<br>
In this project, I used one of them.<br>
As a result, it uses the same amount of memory with strassen's algorithm but it uses much less operations.<br>
There wer 14 setMat and 18 addMat operations but now it has 8 setMat and 15 addMat operations.<br>
<a href="https://github.com/programelot/MatrixMultiplication/blob/master/DMM/src/MM11.cu">Here</a> is the implementation of it.<br>

      
    </div>
    
    <div class="references">
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>Post author:</strong>
      Programelot
    </li>
    <li class="post-copyright-link">
      <strong>Post link:</strong>
      <a href="https://programelot.github.io/projects/MMO/" title="Matrix Multiplication Optimization">https://programelot.github.io/projects/MMO/</a>
    </li>
    <li class="post-copyright-license">
      <strong>Copyright Notice: </strong>
      All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
    </li>
  </ul>


      
    </div>
    
    <footer class="post-footer">
      

      
      
      
      
      

      
      
      

      
    </footer>
  </article>
  <div class="post-spread">
    
  </div>
</div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Programelot</span>
  <br>
  
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://jekyllrb.com">Jekyll</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/simpleyyt/jekyll-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>





















  
   
  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/jquery/index.js?v=2.1.3"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/assets/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/assets/js/src/motion.js?v=5.1.1"></script>



  
  

  <script type="text/javascript" src="/assets/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/assets/js/src/post-details.js?v=5.1.1"></script>


  


  <script type="text/javascript" src="/assets/js/src/bootstrap.js?v=5.1.1"></script>



  


  

    
      <script id="dsq-count-scr" src="https://programelot.disqus.com/count.js" async></script>
    

    
      
      <script type="text/javascript">
          var disqus_config = function () {
              this.page.url = 'https://programelot.github.io/projects/MMO/';
              this.page.identifier = '/projects/MMO/';
              this.page.title = 'Matrix Multiplication Optimization';
          };
          var d = document, s = d.createElement('script');
          s.src = 'https://programelot.disqus.com/embed.js';
          s.setAttribute('data-timestamp', '' + +new Date());
          (d.head || d.body).appendChild(s);
      </script>
      
    

  




	





  











  




  







  


  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  


  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
  </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
       "HTML-CSS": { linebreaks: { automatic: true } },
               SVG: { linebreaks: { automatic: true } }
      });
      </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" async src="https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  

  

</body>
</html>

