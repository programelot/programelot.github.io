<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[Google foobar]]></title>
      <url>/algorithm/2023/04/13/Google-foobar/</url>
      <content type="text"><![CDATA[What is a Google foobarGoogle foobar is a secret hiring process of the Google.When programmer searches about the programming, google randomly sends an invitation for google foobar.Unfortunately, I lost the picture of the invitation moment.However, I decided to upload all of the solution and explanation about it in here.Please feel free to read about it if you needs.Short problems are not linked to other posts.However, some problems requires a deep analysis so I left it to the links.List of problemsBomb babyBraille TranslationBringing a Gun to a Trainer FightBunny Worker LocationsDisorderly EscapeDistract the TrainersDodge the LasersDont get volunteeredDoomsday FuelElevator MaintenanceEn Route SaluteEscape PodsExpanding NebulaFind the Access CodesFree the bunny workersFuel injection perfectionGearing Up for DestructionHey, I Already Did That!I Love Lance &amp; JaniceIon Flux RelabelingMinion Work AssignmentsNumbers Station Coded MessagesPlease Pass the Coded MessagesPower HungryPrepare the Bunnies’ EscapeQueue To DoRe-IDRunning with BunniesSkipping WorkSolar DoomsdayThe cake is not a lie!The Grandest Staircase Of Them All]]></content>
      <categories>
        
          <category> algorithm </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Sum of beatty sequence]]></title>
      <url>/algorithm/2023/01/30/Sum-of-beatty-sequence/</url>
      <content type="text"><![CDATA[ProblemLet’s think about the sequence of $[\alpha k]$ for $k \in \mathcal{Z}^+$ and $\alpha \in \mathcal{R}^+/\mathcal{Q}^+$ and $\alpha &gt; 1$.For example, $[\sqrt{2}] \approxeq 1.414 = 1, [2 \sqrt{2}] \approxeq 2.828 = 2, [3 \sqrt{2}] \approxeq 4.242 = 4, [4 \sqrt{2}] \approxeq 5.656 = 5, [5 \sqrt{2}] \approxeq 7.071 = 7$ for $\alpha = \sqrt{2}$.Let’s define $S(\alpha, n) = \sum\limits_{k = 1}^{n}[\alpha k]$.Then, problem is that compute $S(\alpha, n)$.In general, it can be computed easily by computing one by one which takes $O(n)$.However, there is another algorithm that takes much less time than this.AlgorithmThen, $S(\alpha, n) + S(\beta, [m/\beta]) = \sum\limits_{k = 1}^{m} k$ with following conditions.  $\alpha^{-1} + \beta^{-1} = 1$  $m = [\alpha n]$Proof is as follows.First of all, due to the Rayleigh’s theorem(Beatty’s theorem), it is true that sum of two sequence can cover the interger sequence.Secondly, it is true that $S(\beta, [m/\beta])$ covers all integers from $1$ to $m$ that exists in the $[\beta k]$ since it ends with $[m/\beta]$.At the same time, $S(\alpha, n)$ ends with $[\alpha n] = m$. Therefore, it covers all integers from $1$ to $m$.From the proof above, $S(\alpha, n) + S(\beta, [m/\beta]) = \sum\limits_{k = 1}^{m} k = m(m + 1)/2$As a result, $S(\alpha, n) = m(m + 1)/2 - S(\beta, [m/\beta])$.Now, onlything that matter is whether computing $S(\beta, [m/\beta])$ is simpler than computing $S(\alpha, n)$.To achived this, we need to check whether $\alpha &gt; 2$ or not.If $\alpha &gt; 2$, use the follwoing equations to reduce $\alpha$.$S(\alpha, n) = \sum\limits_{k = 1}^{n}[\alpha k] $$ = \sum\limits_{k = 1}^{n}[((\alpha - 1) + 1) k] $$ = \sum\limits_{k = 1}^{n}[(\alpha - 1)k + k] $$ = \sum\limits_{k = 1}^{n}[(\alpha - 1)k] + \sum\limits_{k = 1}^{n}k $$ = S(\alpha - 1, n) + n(n+1)/2$Notice that it means $S(\alpha, n) = S(\alpha - k, n) + kn(n+1)/2$ for $k \in \mathcal{Z}^{+}$ such that $k &lt; \alpha - 1$.Now, let’s assume that $1 &lt; \alpha &lt; 2$.It is ture that $\beta = \frac{\alpha}{\alpha - 1}$ from $\alpha^{-1} + \beta^{-1} = 1$.If $\beta$ is bigger than $2$, we can use the same process above to change it exists between $1$ and $2$.Therefore, only consideration is whether $[m/\beta]$ is smaller than $n$ or not.And it is true from $[m/\beta] = [[\alpha n]/\beta] = [[\alpha n]/(\alpha/(\alpha - 1))] \le ((\alpha n)/\alpha)(\alpha - 1) = (\alpha - 1) n &lt; n$.Notice that $ 0 &lt; \alpha - 1 &lt; 1$.As a result, it makes problem simpler.Notice that this algorithm is faster since it reduces number to be computed in some ratio.Therefore, it usually faster than computing it one by one.ExampleFor example, $S(\sqrt{2}, 100)$ can be computed as follows.Notice that $\alpha = \sqrt{2}$ then $\beta = \frac{\sqrt{2}}{\sqrt{2} - 1} = 2 + \sqrt{2}$  $S(\sqrt{2}, 100) = 141 * 142 / 2 - S(2 + \sqrt{2}, 41)$ by $m = [\sqrt{2} *  100] = 141, [m/\beta] = 41$.  $S(2 + \sqrt{2}, 41) = S(\sqrt{2}, 41) + 2 * 41 * 42/2$  $S(\sqrt{2}, 41) = 57 * 58 / 2 - S(2 + \sqrt{2}, 16)$ by $m = [\sqrt{2} *  100] = 57, [m/\beta] = 16$.  $S(2 + \sqrt{2}, 16) = S(\sqrt{2}, 16) + 2 * 16 * 17/2$  $S(\sqrt{2}, 16) = 22 * 23 / 2 - S(2 + \sqrt{2}, 6)$ by $m = [\sqrt{2} *  16] = 22, [m/\beta] = 6$.  $S(2 + \sqrt{2}, 6) = S(\sqrt{2}, 6) + 2 * 6 * 7/2$  $S(\sqrt{2}, 6) = 8 * 9 / 2 - S(2 + \sqrt{2}, 2)$ by $m = [\sqrt{2} *  6] = 8, [m/\beta] = 2$.  $S(2 + \sqrt{2}, 2) = S(\sqrt{2}, 2) + 2 * 2 * 3/2$  $S(\sqrt{2}, 2) = 2 * 3 / 2 - S(2 + \sqrt{2}, 0)$ by $m = [\sqrt{2} *  2] = 2, [m/\beta] = 0$.  $S(2 + \sqrt{2}, 0) = 0$  $S(\sqrt{2}, 2) = 2 * 3 / 2 - S(2 + \sqrt{2}, 0) = 3 - 0 = 3$  $S(2 + \sqrt{2}, 2) = S(\sqrt{2}, 2) + 2 * 2 * 3/2 = 3 + 6 = 9$  $S(\sqrt{2}, 6) = 8 * 9 / 2 - S(2 + \sqrt{2}, 2) = 36 - 9 = 27$  $S(2 + \sqrt{2}, 6) = S(\sqrt{2}, 6) + 2 * 6 * 7/2 = 27 + 42 = 69$  $S(\sqrt{2}, 16) = 22 * 23 / 2 - S(2 + \sqrt{2}, 6) = 253 - 69 = 184$  $S(2 + \sqrt{2}, 16) = S(\sqrt{2}, 16) + 2 * 16 * 17/2 = 184 + 272 = 456$  $S(\sqrt{2}, 41) = 57 * 58 / 2 - S(2 + \sqrt{2}, 16) = 1653 - 456 = 1197$  $S(2 + \sqrt{2}, 41) = S(\sqrt{2}, 41) + 2 * 41 * 42/2 = 1197 + 1722 = 2919$  $S(\sqrt{2}, 100) = 141 * 142 / 2 - S(2 + \sqrt{2}, 41) = 10011 - 2919 = 7092$]]></content>
      <categories>
        
          <category> algorithm </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Rayleigh's theorem(Beatty's theorem)]]></title>
      <url>/algorithm/2023/01/30/Rayleighs-theorem/</url>
      <content type="text"><![CDATA[TheoremLet’s think about the sequence of $[\alpha k]$ for $k \in \mathcal{Z}^+$ and $\alpha \in \mathcal{R}^+/\mathcal{Q}^+$ and $\alpha &gt; 1$.For example, $[\sqrt{2}] \approxeq 1.414 = 1, [2 \sqrt{2}] \approxeq 2.828 = 2, [3 \sqrt{2}] \approxeq 4.242 = 4, [4 \sqrt{2}] \approxeq 5.656 = 5, [5 \sqrt{2}] \approxeq 7.071 = 7$ for $\alpha = \sqrt{2}$.Then, $[\alpha k]$ and $[\beta k]$ partitions an integer sequence if $\alpha^{-1} + \beta^{-1} = 1$.For example, $\frac{1}{\sqrt{2}} + \frac{1}{\sqrt{2}/(\sqrt{2} - 1)} = \frac{1}{\sqrt{2}} + \frac{\sqrt{2} - 1}{\sqrt{2}} = 1$.Which means $[\sqrt{2}/(\sqrt{2} - 1)] \approxeq 3.414 = 3, [2\sqrt{2}/(\sqrt{2} - 1)] \approxeq 6.828 = 6$.ProofProof will be done with contradictions for two parts.No collisionThere is something that exists in both $[\alpha k]$ and $[\beta k]$.Then, there is positive integer $v, r, k \in \mathcal{Z}^+$ such that $v = [\alpha r] = [\beta k]$.Which means $v \le \alpha r &lt; v + 1$ and $ v \le \beta k &lt; v + 1$.By dividing each by $\alpha, \beta$, $\frac{v}{\alpha} \le r &lt; \frac{v + 1}{\alpha}$ and $\frac{v}{\beta} \le k &lt; \frac{v + 1}{\beta}$.However, equality can’t be happen because $\alpha, \beta$ is irrational.Now adding two equation results in $\frac{v}{\alpha} + \frac{v}{\beta} = v(\frac{1}{\alpha} + \frac{1}{\beta}) = v &lt; r + k &lt; \frac{v + 1}{\alpha} + \frac{v + 1}{\beta} &lt; (v + 1)(\frac{1}{\alpha} + \frac{1}{\beta}) = v + 1$.Which result in $v &lt; r + k &lt; v + 1$ and it’s a contradiction to $r + k$ is an integer.CoverageThere is something that doesn’t exists in both $[\alpha k]$ and $[\beta k]$.Then, there is positive integer $v, r, k \in \mathcal{Z}^+$ such that $[\alpha r] = v - 1, [\alpha (r + 1)] = v + 1, [\beta k] = v - 1, [\beta (k + 1)] = v + 1$.Which means, $\alpha r &lt; v, \alpha (r + 1) \ge v + 1, \beta k &lt; v , \beta (k + 1) \ge v + 1$.Notice that equality can’t happen in here either because $\alpha, \beta$ is irrational.By dividing $\alpha, \beta$, it results in  $r &lt; \frac{v}{\alpha}, r + 1 &gt; \frac{v + 1}{\alpha}, k &lt; \frac{v}{\beta} , k + 1 &gt; \frac{v + 1}{\beta}$.Adding 1st and 3rd inequality, $r + k &lt; \frac{v}{\alpha} + \frac{v}{\beta} = v(\frac{1}{\alpha} + \frac{1}{\beta}) = v$.Similarly adding 2nd and 4th inequality, $r + 1 + k + 1 &gt; \frac{v + 1}{\alpha} + \frac{v + 1}{\beta} = (v + 1)(\frac{1}{\alpha} + \frac{1}{\beta}) = v + 1$.Which means $r + k &gt; v + 1 - 2 = v - 1$.By using both inequality, $v - 1 &lt; r + k &lt; v$ and it’s a contradiction to $r + k$ is an integer.As a result, claim holds.]]></content>
      <categories>
        
          <category> algorithm </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Stack and queue]]></title>
      <url>/algorithm/2021/08/13/Stack-Queue/</url>
      <content type="text"><![CDATA[There is another data strucutre that used to store tons of data.Therefore, it has two special operations known as the insert and the pop.However, there are two types of ways to make policies for inserting and poping a data from a structure.Depending on this two method, it is so-called as a stack or a queue.Stack has a property known as LIFO(Last-In-First-Out).Queue has a property known as FIFO(First-In-First-Out).StackIf we insert a data into a stack first, it should be popped from the stack first.Therefore, it reserves the order we inserted even when it is popped.QueueIf we insert a data into a queue first, it should be popped from the stack later.Therefore, it reverses the order we inserted even when it is popped.ComplexityIf we use a list for implementing a stack and a queue, it is easy to get a good performance because we already has $O(1)$ complexity for inserting and deleting at the front and the end of the list.            Time complexity      Stack      Queue                  Insert      $O(1)$      $O(1)$              Pop      $O(1)$      $O(1)$              Space complexity      $O(n)$      $O(n)$      ]]></content>
      <categories>
        
          <category> algorithm </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Array and list]]></title>
      <url>/algorithm/2021/08/12/Array-List/</url>
      <content type="text"><![CDATA[Before talking about algorithm itself, we need to talk about data structures that typically uses for algorithms.Some of algorithms even works based on a specific data structure that optimizes the algorithm.In this chapter, I’ll explain the most common data structures only.ArrayTo store a number of data, there should be some data structure that can give a specific data you want anytime and store a data in to the storage either.To acheive this property, there is the easiest data structure known as an array.Like the name itself, it stores data in an array of storage.Pros of this algorithm is that you can get a data anytime from an array in a constant time because all you need is an index.In mathmatical format, it usually written as $a[0]$ or $a_0$.However, there is a big disadvantange for this.If you want to use an array, you need to know exact size of data you need.Otherwise, you may can access to the data where you didn’t meant to.Therefore, it has a big disadvantage known as the fixed-size.However, it can extend the array by making a new array and copy every element in side of the array.Therefore, complexity of an array is like follow.            Time complexity      Array                  Search/Change      $O(1)$              Add (Front)      $O(n)$              Add (Random)      $O(n)$              Add (Back)      $O(n)$              Delete (Front)      $O(n)$              Delete (Random)      $O(n)$              Delete (Back)      $O(n)$              Merge      $O(n)$              Space complexity      $O(n)$      Notice that adding and delete will change the size of the array.Merge means that merging two array into a single array.It will be assumed to have the same size of two arrays.List 1To avoid this fixed-size problem, there is an alternative structure known as a list.A list consists of nodes.Each node has a data and a pointer to the next node.Therefore, it can access to next node from any node.However, it has a slow search algorithm because it can access only the next node.Therefore, it takes a linear time to read an array.            Time complexity      List 1                  Search/Change      $O(n)$              Add (Front)      $O(1)$              Add (Random)      $O(n)$              Add (Back)      $O(n)$              Delete (Front)      $O(1)$              Delete (Random)      $O(n)$              Delete (Back)      $O(n)$              Merge      $O(n)$              Space complexity      $O(n)$      One other problem is that it can only add the new data without overhead at the front of the list.Therefore, there is another ways to make a list.List 2What if we make a pointer to denotes the last point of the list at the same time?It will gives an advantages that makes accessable at the end of the list.Therefore, it will give better performance when it works for the end of the list.At the same time, it has an advantage to merge two lists because it can connect the end point of a list to another list.            Time complexity      List 2                  Search/Change      $O(n)$              Add (Front)      $O(1)$              Add (Random)      $O(n)$              Add (Back)      $O(1)$              Delete (Front)      $O(1)$              Delete (Random)      $O(n)$              Delete (Back)      $O(1)$              Merge      $O(1)$              Space complexity      $O(n)$      However, it still has $O(n)$ complexity for read/change operation.Therefore, it usually doesn’t be used in actual implementation however the notation of the list is typically used for many other data structures.VectorLast implementation is which standard c++ language uses.It works like an ordinary array but it increases its size by double the size.It gives a nice performance because it gives a constant complexity for adding and deleting data at the back of the array.Notice that this is amortized analysis.Therefore, it sometimes cause long term process.            Time complexity      Vector                  Search/Change      $O(1)$              Add (Front)      $O(n)$              Add (Random)      $O(n)$              Add (Back)      Amortized $O(1)$              Delete (Front)      $O(n)$              Delete (Random)      $O(n)$              Delete (Back)      Amortized $O(1)$              Merge      $O(n)$              Space complexity      $O(n)$      Comparison            Time complexity      Array      List 1      List 2      Vector                  Search/Change      $O(1)$      $O(n)$      $O(n)$      $O(1)$              Add (Front)      $O(n)$      $O(1)$      $O(1)$      $O(n)$              Add (Random)      $O(n)$      $O(n)$      $O(n)$      $O(n)$              Add (Back)      $O(n)$      $O(n)$      $O(1)$      Amortized $O(1)$              Delete (Front)      $O(n)$      $O(1)$      $O(1)$      $O(n)$              Delete (Random)      $O(n)$      $O(n)$      $O(n)$      $O(n)$              Delete (Back)      $O(n)$      $O(n)$      $O(1)$      Amortized $O(1)$              Merge      $O(n)$      $O(n)$      $O(1)$      $O(n)$              Space complexity      $O(n)$      $O(n)$      $O(n)$      $O(n)$      ]]></content>
      <categories>
        
          <category> algorithm </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Complexity]]></title>
      <url>/algorithm/2021/08/10/Complexity/</url>
      <content type="text"><![CDATA[To analyze performances of algorithms, we need to define some jargons.Time complexityThe time complexity of an algorithm, $T(n)$ is a function that denotes running time of the algorithm depending on the size of input data.For example, let’s think about an algorithm that adds data with other data.You may smart enough to add any arbitrary two number in 1 second.Then, how many time will it took for 100 data?It will be 100 seconds.In this case, $T(n) = n$.Big O notationBig O notation is the method to guarantee the upper bound of algorithms’ perforamce.For any time complexity $T(n)$, we say $T(n)$ $=$ $O(g(n))$ if there is some positive constant $M$,$x_0$ such that $T(n)$ $\le$ $M g(n)$ for all $x \ge x_0$.For example, if $T(n)$ $=$ $n^3$ $+$ $10n^2$ $+$ $n$ $+$ $27$, $T(n)$ $=$ $O(n^3)$ for $M$ $=$ $39$, $x_0$ $=$ $1$.Notice that $T(n)$ $=$ $n^3$ $+$ $10n^2$ $+$ $n$ $+$ $27$ $\le$ $n^3$ $+$ $10n^3$ $+$ $n^3$ $+$ $27n^3$ $=$ $39n^3$ for $n$ $\ge$ $1$.In fact, it is enough to find the most steepest part in $T(n)$.Asymptotically approximateIn fact, it can be choosed to be worse than expected in big O notation.For example, $T(n)$ $=$ $O(n^2)$ if $T(n)$ $=$ $O(n)$ in all cases.Therefore, we say that “An algorithm’s time complexity asymptotically approximates to $g(n)$” when $\lim\limits_{n \leftarrow \infty}\frac{T(n)}{Mg(n)} = 1$ for some positive constant $M$.Big $\Omega$ notationBig $\Omega$ notation is the opposite with the big O notation.It guarantess the lower bound of algorithms’ performance.For any time complexity $T(n)$, we say $T(n)$ $=$ $\Omega(g(n))$ if there is some positive constant $M$,$x_0$ such that $T(n)$ $\ge$ $M g(n)$ for all $x \ge x_0$.Big $\Theta$ notationBig theta notation is used when an algorithm has the same complexity for both upper and lower bounds.In other word, we say $T(n)$ $=$ $\Theta(g(n))$ if there is some positive constant $M1$,$M_2$,$x_0$ such that $M_2 g(n)$ $\le$ $T(n)$ $\le$ $M_1 g(n)$ for all $x \ge x_0$.However, it is hard to expect that algorithm has such a complexity.Some algorithms’ performance vary between the data itself.Therefore, some algorithm can’t have big $\Theta$ notation for their time complexity.Amortized complexityAmortized complexity is another measurement to analysis an algorithm.Many algorithms doesn’t have a nice $\Theta$ notation to explain the performance’s performance.It really depends on the situation.However, it can be pessimistic to use only big O notation.Therefore, amortized complexity measures a performance of an algorithm by dividing its complexity between executions.If some algorithm works $O(1)$ for $n$ times and it works $O(n^2)$ for $1$ time.Then, the amortized complexity of this algorithm is $O(\frac{1 \times n + n^2}{n + 1})$ $=$ $O(n)$.Space complexitySpace complexity is another measurable tool for algorithms.It denotes how many memory spaces it uses.We use all of notations above to represent space complexity either.]]></content>
      <categories>
        
          <category> algorithm </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Algorithm]]></title>
      <url>/algorithm/2021/08/02/Algorithm/</url>
      <content type="text"><![CDATA[Algorithm is a mathmatical tool to solve some problem with propal methods.For example, let’s think about a situation follows.Oneday, you’ve got a job from a city library.However, there was a tournado yesterday so all books dropped out of the shelf.Your employer calls you and asks you to clean it up in order of books’ ID.Despite of your low payment, you have no way but clean it up.Therefore, you started to think about the way to clean it up.Insertion sortYou may give up to think about better way to clean it up.Instead of it, you just started to clean it up by reading the ID of each book and put the book which has the smallest ID into the shelf.Quick sortWhile you are doing such a thing, your friend decided to do the follow.  If there is only one book, just put it in with the revered direction.  If there is more than a book, select a book in random.  Put all books that have smaller ID than the book that you selected at 1 into the shelf.  Then add the book that you selected at 1 but in the reversed direction to identify this is the book which you used.  If all books are in the self with inverse direction, flip all books.  Otherwise, pick books between two reversed book and take books out of the shelf and do the same process from 1.Notice that the starting point, end point of the shelf will be considered as the reversed book in 6.This looks more complex but it usually takes much less time than you did.See the example below.ExampleFor example, there are books that have IDs of 52,33,25,19,28,38,37,45,73,68,61,69,87,78,90.Now, let’s follow the each process.You will clean the book like bellow.Notice that I marked [] as books that shelf has, {} as what you are memorizing and () as what you are looking.In this algorithm, you can see only one book at once because numbers are too complex to see it on glance.At the same time, you can memorize a number at most because of its long digits.  (52), 33, 25, 19, 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  {52}, (33), 25, 19, 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  52, {33}, (25), 19, 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  52, 33, {25}, (19), 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  52, 33, 25, {19}, (28), 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  52, 33, 25, {19}, 28, (38), 37, 45, 73, 68, 61, 69, 87, 78, 90  52, 33, 25, {19}, 28, 38, (37), 45, 73, 68, 61, 69, 87, 78, 90  52, 33, 25, {19}, 28, 38, 37, (45), 73, 68, 61, 69, 87, 78, 90  52, 33, 25, {19}, 28, 38, 37, 45, (73), 68, 61, 69, 87, 78, 90  52, 33, 25, {19}, 28, 38, 37, 45, 73, (68), 61, 69, 87, 78, 90  52, 33, 25, {19}, 28, 38, 37, 45, 73, 68, (61), 69, 87, 78, 90  52, 33, 25, {19}, 28, 38, 37, 45, 73, 68, 61, (69), 87, 78, 90  52, 33, 25, {19}, 28, 38, 37, 45, 73, 68, 61, 69, (87), 78, 90  52, 33, 25, {19}, 28, 38, 37, 45, 73, 68, 61, 69, 87, (78), 90  52, 33, 25, {19}, 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, (90)  [19], 52, 33, 25, 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [19], (52), 33, 25, 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [19], {52}, (33), 25, 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [19], 52, {33}, (25), 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [19], 52, 33, {25}, (28), 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [19], 52, 33, {25}, 28, (38), 37, 45, 73, 68, 61, 69, 87, 78, 90  [19], 52, 33, {25}, 28, 38, (37), 45, 73, 68, 61, 69, 87, 78, 90  [19], 52, 33, {25}, 28, 38, 37, (45), 73, 68, 61, 69, 87, 78, 90  [19], 52, 33, {25}, 28, 38, 37, 45, (73), 68, 61, 69, 87, 78, 90  [19], 52, 33, {25}, 28, 38, 37, 45, 73, (68), 61, 69, 87, 78, 90  [19], 52, 33, {25}, 28, 38, 37, 45, 73, 68, (61), 69, 87, 78, 90  [19], 52, 33, {25}, 28, 38, 37, 45, 73, 68, 61, (69), 87, 78, 90  [19], 52, 33, {25}, 28, 38, 37, 45, 73, 68, 61, 69, (87), 78, 90  [19], 52, 33, {25}, 28, 38, 37, 45, 73, 68, 61, 69, 87, (78), 90  [19], 52, 33, {25}, 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, (90)  [19, 25], 52, 33, 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [19, 25], (52), 33, 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [19, 25], {52}, (33), 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [19, 25], 52, {33}, (28), 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [19, 25], 52, 33, {28}, (38), 37, 45, 73, 68, 61, 69, 87, 78, 90  [19, 25], 52, 33, {28}, 38, (37), 45, 73, 68, 61, 69, 87, 78, 90  [19, 25], 52, 33, {28}, 38, 37, (45), 73, 68, 61, 69, 87, 78, 90  [19, 25], 52, 33, {28}, 38, 37, 45, (73), 68, 61, 69, 87, 78, 90  [19, 25], 52, 33, {28}, 38, 37, 45, 73, (68), 61, 69, 87, 78, 90  [19, 25], 52, 33, {28}, 38, 37, 45, 73, 68, (61), 69, 87, 78, 90  [19, 25], 52, 33, {28}, 38, 37, 45, 73, 68, 61, (69), 87, 78, 90  [19, 25], 52, 33, {28}, 38, 37, 45, 73, 68, 61, 69, (87), 78, 90  [19, 25], 52, 33, {28}, 38, 37, 45, 73, 68, 61, 69, 87, (78), 90  [19, 25], 52, 33, {28}, 38, 37, 45, 73, 68, 61, 69, 87, 78, (90)  [19, 25, 28], 52, 33, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [19, 25, 28], (52), 33, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [19, 25, 28], {52}, (33), 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [19, 25, 28], 52, {33}, (38), 37, 45, 73, 68, 61, 69, 87, 78, 90  [19, 25, 28], 52, {33}, 38, (37), 45, 73, 68, 61, 69, 87, 78, 90  [19, 25, 28], 52, {33}, 38, 37, (45), 73, 68, 61, 69, 87, 78, 90  [19, 25, 28], 52, {33}, 38, 37, 45, (73), 68, 61, 69, 87, 78, 90  [19, 25, 28], 52, {33}, 38, 37, 45, 73, (68), 61, 69, 87, 78, 90  [19, 25, 28], 52, {33}, 38, 37, 45, 73, 68, (61), 69, 87, 78, 90  [19, 25, 28], 52, {33}, 38, 37, 45, 73, 68, 61, (69), 87, 78, 90  [19, 25, 28], 52, {33}, 38, 37, 45, 73, 68, 61, 69, (87), 78, 90  [19, 25, 28], 52, {33}, 38, 37, 45, 73, 68, 61, 69, 87, (78), 90  [19, 25, 28], 52, {33}, 38, 37, 45, 73, 68, 61, 69, 87, 78, (90)  [19, 25, 28, 33], 52, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [19, 25, 28, 33], (52), 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [19, 25, 28, 33], {52}, (38), 37, 45, 73, 68, 61, 69, 87, 78, 90  [19, 25, 28, 33], 52, {38}, (37), 45, 73, 68, 61, 69, 87, 78, 90  [19, 25, 28, 33], 52, 38, {37}, (45), 73, 68, 61, 69, 87, 78, 90  [19, 25, 28, 33], 52, 38, {37}, 45, (73), 68, 61, 69, 87, 78, 90  [19, 25, 28, 33], 52, 38, {37}, 45, 73, (68), 61, 69, 87, 78, 90  [19, 25, 28, 33], 52, 38, {37}, 45, 73, 68, (61), 69, 87, 78, 90  [19, 25, 28, 33], 52, 38, {37}, 45, 73, 68, 61, (69), 87, 78, 90  [19, 25, 28, 33], 52, 38, {37}, 45, 73, 68, 61, 69, (87), 78, 90  [19, 25, 28, 33], 52, 38, {37}, 45, 73, 68, 61, 69, 87, (78), 90  [19, 25, 28, 33], 52, 38, {37}, 45, 73, 68, 61, 69, 87, 78, (90)  [19, 25, 28, 33, 37], 52, 38, 45, 73, 68, 61, 69, 87, 78, 90  [19, 25, 28, 33, 37], (52), 38, 45, 73, 68, 61, 69, 87, 78, 90  [19, 25, 28, 33, 37], {52}, (38), 45, 73, 68, 61, 69, 87, 78, 90  [19, 25, 28, 33, 37], 52, {38}, (45), 73, 68, 61, 69, 87, 78, 90  [19, 25, 28, 33, 37], 52, {38}, 45, (73), 68, 61, 69, 87, 78, 90  [19, 25, 28, 33, 37], 52, {38}, 45, 73, (68), 61, 69, 87, 78, 90  [19, 25, 28, 33, 37], 52, {38}, 45, 73, 68, (61), 69, 87, 78, 90  [19, 25, 28, 33, 37], 52, {38}, 45, 73, 68, 61, (69), 87, 78, 90  [19, 25, 28, 33, 37], 52, {38}, 45, 73, 68, 61, 69, (87), 78, 90  [19, 25, 28, 33, 37], 52, {38}, 45, 73, 68, 61, 69, 87, (78), 90  [19, 25, 28, 33, 37], 52, {38}, 45, 73, 68, 61, 69, 87, 78, (90)  [19, 25, 28, 33, 37, 38], 52, 45, 73, 68, 61, 69, 87, 78, 90  [19, 25, 28, 33, 37, 38], (52), 45, 73, 68, 61, 69, 87, 78, 90  [19, 25, 28, 33, 37, 38], {52}, (45), 73, 68, 61, 69, 87, 78, 90  [19, 25, 28, 33, 37, 38], 52, {45}, (73), 68, 61, 69, 87, 78, 90  [19, 25, 28, 33, 37, 38], 52, {45}, 73, (68), 61, 69, 87, 78, 90  [19, 25, 28, 33, 37, 38], 52, {45}, 73, 68, (61), 69, 87, 78, 90  [19, 25, 28, 33, 37, 38], 52, {45}, 73, 68, 61, (69), 87, 78, 90  [19, 25, 28, 33, 37, 38], 52, {45}, 73, 68, 61, 69, (87), 78, 90  [19, 25, 28, 33, 37, 38], 52, {45}, 73, 68, 61, 69, 87, (78), 90  [19, 25, 28, 33, 37, 38], 52, {45}, 73, 68, 61, 69, 87, 78, (90)  [19, 25, 28, 33, 37, 38, 45], 52, 73, 68, 61, 69, 87, 78, 90  [19, 25, 28, 33, 37, 38, 45], (52), 73, 68, 61, 69, 87, 78, 90  [19, 25, 28, 33, 37, 38, 45], {52}, (73), 68, 61, 69, 87, 78, 90  [19, 25, 28, 33, 37, 38, 45], {52}, 73, (68), 61, 69, 87, 78, 90  [19, 25, 28, 33, 37, 38, 45], {52}, 73, 68, (61), 69, 87, 78, 90  [19, 25, 28, 33, 37, 38, 45], {52}, 73, 68, 61, (69), 87, 78, 90  [19, 25, 28, 33, 37, 38, 45], {52}, 73, 68, 61, 69, (87), 78, 90  [19, 25, 28, 33, 37, 38, 45], {52}, 73, 68, 61, 69, 87, (78), 90  [19, 25, 28, 33, 37, 38, 45], {52}, 73, 68, 61, 69, 87, 78, (90)  [19, 25, 28, 33, 37, 38, 45, 52], 73, 68, 61, 69, 87, 78, 90  [19, 25, 28, 33, 37, 38, 45, 52], (73), 68, 61, 69, 87, 78, 90  [19, 25, 28, 33, 37, 38, 45, 52], {73}, (68), 61, 69, 87, 78, 90  [19, 25, 28, 33, 37, 38, 45, 52], 73, {68}, (61), 69, 87, 78, 90  [19, 25, 28, 33, 37, 38, 45, 52], 73, 68, {61}, (69), 87, 78, 90  [19, 25, 28, 33, 37, 38, 45, 52], 73, 68, {61}, 69, (87), 78, 90  [19, 25, 28, 33, 37, 38, 45, 52], 73, 68, {61}, 69, 87, (78), 90  [19, 25, 28, 33, 37, 38, 45, 52], 73, 68, {61}, 69, 87, 78, (90)  [19, 25, 28, 33, 37, 38, 45, 52, 61], 73, 68, 69, 87, 78, 90  [19, 25, 28, 33, 37, 38, 45, 52, 61], (73), 68, 69, 87, 78, 90  [19, 25, 28, 33, 37, 38, 45, 52, 61], {73}, (68), 69, 87, 78, 90  [19, 25, 28, 33, 37, 38, 45, 52, 61], 73, {68}, (69), 87, 78, 90  [19, 25, 28, 33, 37, 38, 45, 52, 61], 73, {68}, 69, (87), 78, 90  [19, 25, 28, 33, 37, 38, 45, 52, 61], 73, {68}, 69, 87, (78), 90  [19, 25, 28, 33, 37, 38, 45, 52, 61], 73, {68}, 69, 87, 78, (90)  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68], 73, 69, 87, 78, 90  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68], (73), 69, 87, 78, 90  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68], {73}, (69), 87, 78, 90  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68], 73, {69}, (87), 78, 90  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68], 73, {69}, 87, (78), 90  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68], 73, {69}, 87, 78, (90)  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69], 73, 87, 78, 90  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69], (73), 87, 78, 90  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69], {73}, (87), 78, 90  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69], {73}, 87, (78), 90  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69], {73}, 87, 78, (90)  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69, 73], 87, 78, 90  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69, 73], (87), 78, 90  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69, 73], {87}, (78), 90  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69, 73], 87, {78}, (90)  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69, 73, 78], 87, 90  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69, 73, 78], (87), 90  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69, 73, 78], {87}, (90)  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69, 73, 78, 87], 90  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69, 73, 78, 87], (90)  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69, 73, 78, 87, 90]As a result, it takes 135 steps.However, your friend can get a result like below.Notice that I marked revered book by !.  (52), 33, 25, 19, 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  {52}, (33), 25, 19, 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [33], {52}, 25, 19, 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [33], {52}, (25), 19, 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [33, 25], {52}, 19, 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [33, 25], {52}, (19), 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [33, 25, 19], {52}, 28, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [33, 25, 19], {52}, (28), 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [33, 25, 19, 28], {52}, 38, 37, 45, 73, 68, 61, 69, 87, 78, 90  [33, 25, 19, 28], {52}, (38), 37, 45, 73, 68, 61, 69, 87, 78, 90  [33, 25, 19, 28, 38], {52}, 37, 45, 73, 68, 61, 69, 87, 78, 90  [33, 25, 19, 28, 38], {52}, (37), 45, 73, 68, 61, 69, 87, 78, 90  [33, 25, 19, 28, 38, 37], {52}, 45, 73, 68, 61, 69, 87, 78, 90  [33, 25, 19, 28, 38, 37], {52}, (45), 73, 68, 61, 69, 87, 78, 90  [33, 25, 19, 28, 38, 37, 45], {52}, 73, 68, 61, 69, 87, 78, 90  [33, 25, 19, 28, 38, 37, 45], {52}, (73), 68, 61, 69, 87, 78, 90  [33, 25, 19, 28, 38, 37, 45], {52}, 73, (68), 61, 69, 87, 78, 90  [33, 25, 19, 28, 38, 37, 45], {52}, 73, 68, (61), 69, 87, 78, 90  [33, 25, 19, 28, 38, 37, 45], {52}, 73, 68, 61, (69), 87, 78, 90  [33, 25, 19, 28, 38, 37, 45], {52}, 73, 68, 61, 69, (87), 78, 90  [33, 25, 19, 28, 38, 37, 45], {52}, 73, 68, 61, 69, 87, (78), 90  [33, 25, 19, 28, 38, 37, 45], {52}, 73, 68, 61, 69, 87, 78, (90)  [33, 25, 19, 28, 38, 37, 45, !52!], 73, 68, 61, 69, 87, 78, 90  [33, 25, 19, 28, 38, 37, 45, !52!], (73), 68, 61, 69, 87, 78, 90  [33, 25, 19, 28, 38, 37, 45, !52!], {73}, (68), 61, 69, 87, 78, 90  [33, 25, 19, 28, 38, 37, 45, !52!, 68], {73}, 61, 69, 87, 78, 90  [33, 25, 19, 28, 38, 37, 45, !52!, 68], {73}, (61), 69, 87, 78, 90  [33, 25, 19, 28, 38, 37, 45, !52!, 68, 61], {73}, 69, 87, 78, 90  [33, 25, 19, 28, 38, 37, 45, !52!, 68, 61], {73}, (69), 87, 78, 90  [33, 25, 19, 28, 38, 37, 45, !52!, 68, 61, 69], {73}, 87, 78, 90  [33, 25, 19, 28, 38, 37, 45, !52!, 68, 61, 69], {73}, (87), 78, 90  [33, 25, 19, 28, 38, 37, 45, !52!, 68, 61, 69], {73}, 87, (78), 90  [33, 25, 19, 28, 38, 37, 45, !52!, 68, 61, 69], {73}, 87, 78, (90)  [33, 25, 19, 28, 38, 37, 45, !52!, 68, 61, 69, !73!], 87, 78, 90  [33, 25, 19, 28, 38, 37, 45, !52!, 68, 61, 69, !73!], (87), 78, 90  [33, 25, 19, 28, 38, 37, 45, !52!, 68, 61, 69, !73!], {87}, (78), 90  [33, 25, 19, 28, 38, 37, 45, !52!, 68, 61, 69, !73!, 78], {87}, 90  [33, 25, 19, 28, 38, 37, 45, !52!, 68, 61, 69, !73!, 78], {87}, (90)  [33, 25, 19, 28, 38, 37, 45, !52!, 68, 61, 69, !73!, 78, !87!], 90  [33, 25, 19, 28, 38, 37, 45, !52!, 68, 61, 69, !73!, 78, !87!], (90)  [33, 25, 19, 28, 38, 37, 45, !52!, 68, 61, 69, !73!, 78, !87!, !90!]  [33, 25, 19, 28, 38, 37, 45, !52!, 68, 61], 69, [!73!, 78, !87!, !90!]  [33, 25, 19, 28, 38, 37, 45, !52!, 68], 61, 69, [!73!, 78, !87!, !90!]  [33, 25, 19, 28, 38, 37, 45, !52!], 68, 61, 69, [!73!, 78, !87!, !90!]  [33, 25, 19, 28, 38, 37, 45, !52!], (68), 61, 69, [!73!, 78, !87!, !90!]  [33, 25, 19, 28, 38, 37, 45, !52!], {68}, (61), 69, [!73!, 78, !87!, !90!]  [33, 25, 19, 28, 38, 37, 45, !52!, 61], {68}, 69, [!73!, 78, !87!, !90!]  [33, 25, 19, 28, 38, 37, 45, !52!, 61], {68}, (69), [!73!, 78, !87!, !90!]  [33, 25, 19, 28, 38, 37, 45, !52!, 61, !68!], 69, [!73!, 78, !87!, !90!]  [33, 25, 19, 28, 38, 37, 45, !52!, 61, !68!], (69), [!73!, 78, !87!, !90!]  [33, 25, 19, 28, 38, 37, 45, !52!, 61, !68!, !69!, !73!, 78, !87!, !90!]  [33, 25, 19, 28, 38, 37, 45, !52!], 61, [!68!, !69!, !73!, 78, !87!, !90!]  [33, 25, 19, 28, 38, 37, 45, !52!], (61), [!68!, !69!, !73!, 78, !87!, !90!]  [33, 25, 19, 28, 38, 37, 45, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  33, [25, 19, 28, 38, 37, 45, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  33, 25, [19, 28, 38, 37, 45, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  33, 25, 19, [28, 38, 37, 45, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  33, 25, 19, 28, [38, 37, 45, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  33, 25, 19, 28, 38, [37, 45, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  33, 25, 19, 28, 38, 37, [45, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  33, 25, 19, 28, 38, 37, 45, [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  (33), 25, 19, 28, 38, 37, 45, [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  {33}, (25), 19, 28, 38, 37, 45, [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [25], {33}, 19, 28, 38, 37, 45, [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [25], {33}, (19), 28, 38, 37, 45, [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [25, 19], {33}, 28, 38, 37, 45, [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [25, 19], {33}, (28), 38, 37, 45, [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [25, 19, 28], {33}, 38, 37, 45, [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [25, 19, 28], {33}, (38), 37, 45, [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [25, 19, 28], {33}, 38, (37), 45, [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [25, 19, 28], {33}, 38, 37, (45), [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [25, 19, 28, !33!], 38, 37, 45, [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [25, 19, 28, !33!], (38), 37, 45, [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [25, 19, 28, !33!], {38}, (37), 45, [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [25, 19, 28, !33!, 37], {38}, 45, [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [25, 19, 28, !33!, 37], {38}, (45), [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [25, 19, 28, !33!, 37, !38!], 45, [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [25, 19, 28, !33!, 37, !38!], (45), [!52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [25, 19, 28, !33!, 37, !38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  25, [19, 28, !33!, 37, !38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  25, 19, [28, !33!, 37, !38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  25, 19, 28, [!33!, 37, !38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  (25), 19, 28, [!33!, 37, !38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  {25}, (19), 28, [!33!, 37, !38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [19], {25}, 28, [!33!, 37, !38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [19], {25}, (28), [!33!, 37, !38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [19, !25!], 28, [!33!, 37, !38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [19, !25!], (28), [!33!, 37, !38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [19, !25!, !28!, !33!, 37, !38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  19, [!25!, !28!, !33!, 37, !38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  (19), [!25!, !28!, !33!, 37, !38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [!19!, !25!, !28!, !33!, 37, !38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [!19!, !25!, !28!, !33!], 37, [!38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [!19!, !25!, !28!, !33!], (37), [!38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [!19!, !25!, !28!, !33!, !37!, !38!, !45!, !52!, !61!, !68!, !69!, !73!, 78, !87!, !90!]  [!19!, !25!, !28!, !33!, !37!, !38!, !45!, !52!, !61!, !68!, !69!, !73!], 78, [!87!, !90!]  [!19!, !25!, !28!, !33!, !37!, !38!, !45!, !52!, !61!, !68!, !69!, !73!], (78), [!87!, !90!]  [!19!, !25!, !28!, !33!, !37!, !38!, !45!, !52!, !61!, !68!, !69!, !73!, !78!, !87!, !90!]  [19, !25!, !28!, !33!, !37!, !38!, !45!, !52!, !61!, !68!, !69!, !73!, !78!, !87!, !90!]  [19, 25, !28!, !33!, !37!, !38!, !45!, !52!, !61!, !68!, !69!, !73!, !78!, !87!, !90!]  [19, 25, 28, !33!, !37!, !38!, !45!, !52!, !61!, !68!, !69!, !73!, !78!, !87!, !90!]  [19, 25, 28, 33, !37!, !38!, !45!, !52!, !61!, !68!, !69!, !73!, !78!, !87!, !90!]  [19, 25, 28, 33, 37, !38!, !45!, !52!, !61!, !68!, !69!, !73!, !78!, !87!, !90!]  [19, 25, 28, 33, 37, 38, !45!, !52!, !61!, !68!, !69!, !73!, !78!, !87!, !90!]  [19, 25, 28, 33, 37, 38, 45, !52!, !61!, !68!, !69!, !73!, !78!, !87!, !90!]  [19, 25, 28, 33, 37, 38, 45, 52, !61!, !68!, !69!, !73!, !78!, !87!, !90!]  [19, 25, 28, 33, 37, 38, 45, 52, 61, !68!, !69!, !73!, !78!, !87!, !90!]  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, !69!, !73!, !78!, !87!, !90!]  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69, !73!, !78!, !87!, !90!]  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69, 73, !78!, !87!, !90!]  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69, 73, 78, !87!, !90!]  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69, 73, 78, 87, !90!]  [19, 25, 28, 33, 37, 38, 45, 52, 61, 68, 69, 73, 78, 87, 90]As a result, your friend can get a result in 113 step.Which means 19% faster than you are.To solve this problem, we call both methods as sorting algorithms.However, it shows different perforamce as you can see.From the next time, it will focused on the performance of the algorithm and comparison between them.]]></content>
      <categories>
        
          <category> algorithm </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Hardness of approximation]]></title>
      <url>/algorithm/approximation/2021/05/28/Hardness-of-approximation/</url>
      <content type="text"><![CDATA[MAX-E3SAT is a problem that finds a truth value assignment that satisfies the maximum number of clauses for a given a set of clause which contains exactly three literals.MAX-2SAT is a problem that finds a truth value assignment that satisfies the maximum number of clauses for a given a set of clauses which contains at most two literals.Then there exists a $\frac{7}{8}$-approximation algorithm for MAX-E3SAT problem.Proof is like follow.If you see all of the clauses and count the existance of each literals and complementary of literals.Then, we can pick at least half of them to be satisfied by pick $x_i$ or $\overline{x_i}$.Then, we can cover at least $\frac{7}{8}$ of them because we have three literals for one clauses.Then, P=NP if there exists a $(\frac{7}{8} + \epsilon)$-approximation algorithm for MAX-E3SAT problem for all constant $\epsilon &gt; 0$.Proof will be updated later.Then there exists no $\alpha$-approximation for MAX 2SAT for any constant $\alpha &gt; \frac{433}{440}$ unless P=NP.Proof is like follow.First of all, let’s think about follow.For three literals $l1$, $l2$ and $l3$, consider the follwing set of ten clauses in terms of $l1, l2, l3$ and auxiliary variable $y$.  $l1$  $l2$  $l3$  $\overline{l1}\lor\overline{l2}$  $\overline{l2}\lor\overline{l3}$  $\overline{l1}\lor\overline{l3}$  $y$  $l1\lor\overline{y}$  $l2\lor\overline{y}$  $l3\lor\overline{y}$If $l1 \lor l2 \lor l3$ is satisfied, we can choose the value of $y$ so that exactly seven of the ten clauses are satisfied and it is impossible to satisfy more than that.If $l1 \lor l2 \lor l3$ is not satisfied, we can choose the value of $y$ so that exactly six of the ten clauses are satisfied and it is impossible to satisfy more than that.Notice that we can have following table with number of satisfied clauses if $y$ is true.            # true literals      # true clauses in 1~3      # true clauses in 4~6      # true clauses in 7~10      # true clauses in Total                  3      3      0      4      7              2      2      2      3      7              1      1      3      2      6              0      0      3      1      4      If $y$ is false then number of satisfied clauses is like below.            # true literals      # true clauses in 1~3      # true clauses in 4~6      # true clauses in 7~10      # true clauses in Total                  3      3      0      3      6              2      2      2      3      7              1      1      3      3      7              0      0      3      3      6      As a result, maximum is follow.            # true literals      # true clauses in 1~3      # true clauses in 4~6      # true clauses in 7~10 (best)      # true clauses in Total(best)                  3      3      0      4 ($y$ as true)      7              2      2      2      3 ($y$ as true/false)      7              1      1      3      3 ($y$ as false)      7              0      0      3      3 ($y$ as false)      6      Now. think about $m$ clauses that consistes an instance of the MAX E3SAT problem with $n$ varaibles.We construct a MAX 2SAT instance by follow.For each $j$th clause in MAX E3SAT problem, make 10 clauses with distinct auxiliary varaible in the algorithm above.Then, set $l1$, $l2$, $l3$ as each literals used in $j$th clause.For example, following MAX E3SAT was given.  $x_1 \lor x_2 \lor x_3$  $\overline{x_2} \lor x_4 \lor x_5$Then, following is corresponding MAX 2SAT problem.  $x_1$  $x_2$  $x_3$  $\overline{x_1}\lor\overline{x_2}$  $\overline{x_2}\lor\overline{x_3}$  $\overline{x_1}\lor\overline{x_3}$  $y_1$  $x_1\lor\overline{y_1}$  $x_2\lor\overline{y_1}$  $x_3\lor\overline{y_1}$  $\overline{x_2}$  $x_4$  $x_5$  $x_2\lor\overline{x_4}$  $\overline{x_4}\lor\overline{x_5}$  $x_2\lor\overline{x_5}$  $y_2$  $\overline{x_2}\lor\overline{y_2}$  $x_4\lor\overline{y_2}$  $x_5\lor\overline{y_2}$Notice that if we make it so then, it shares the optimal solution because MAX 2SAT problem can satisfies 7 of clauses iff corresponding MAX E3SAT problem’s clause satisfies and MAX 2SAT problem can satisfies 6 of clauses otherwise which is less than 7. For example, if $x_1$ is true and $x_4$ is true then, we can set some $y_1$ and $y_2$ to 14 of 20 becomes true.Now, run the $\alpha$-approximation algorithm for MAX 2SAT on this instance.Let’s defnine some terminologies.  $k^{\star}$ be the number of satisfing clauses of MAX E3SAT instance from the optimal solution.  $\overline{k}$ be the number of satisfing clauses of MAX E3SAT instance from the $\alpha$-approximation algorithm’s output of MAX 2SAT problem.Notice that we may have some auxiliary variables $y$ but we will just ignore it for $\overline{k}$.Then, MAX 2SAT instance’s optimal soltuion satisfies $7k^{\star}$ $+$ $6(m - k^{\star})$ clauses.Which means, $\alpha(7k^{\star}$ $+$ $6(m - k^{\star}))$ $\le$ $7\overline{k}$ $+$ $6(m - \overline{k})$.Nocie that $0$ $&lt;$ $\alpha$ $\le$ $1$ because this is maximization problem.As a result, following inequality is true.$\alpha(7k^{\star}$ $+$ $6(m - k^{\star}))$ $\le$ $7\overline{k}$ $+$ $6(m - \overline{k})$ $\leftrightarrow$$\alpha(k^{\star}$ $+$ $6m)$ $\le$ $\overline{k}$ $+$ $6m$ $\leftrightarrow$$\alpha k^{\star}$ $+$ $\alpha 6m$ $\le$ $\overline{k}$ $+$ $6m$ $\leftrightarrow$$\alpha k^{\star}$ $+$ $\alpha 6m$ $-$ $6m$ $\le$ $\overline{k}$ $\leftrightarrow$$\alpha k^{\star}$ $+$ $6(\alpha  - 1)m$ $\le$ $\overline{k}$ $\leftrightarrow$$\alpha k^{\star}$ $-$ $6(1 - \alpha)m$ $\le$ $\overline{k}$.Now, we already have $\frac{7}{8}$-approximation algorithm.Therefore, $k^{\star}$ $\ge$ $\frac{7}{8}m$ and $\frac{8}{7}k^{\star}$ $\ge$ $m$.As a result, $\overline{k}$ $\ge$$\alpha k^{\star}$ $-$ $6(1 - \alpha)m$ $\ge$$\alpha k^{\star}$ $-$ $6(1 - \alpha)\frac{8}{7}k^{\star}$ $=$$(\alpha - 6(1 - \alpha)\frac{8}{7})k^{\star}$ $=$$(\frac{55}{7}\alpha - \frac{48}{7})k^{\star}$.Notice that $m$ $\le$ $\frac{8}{7}k^{\star}$ $\leftrightarrow$$-$ $\frac{8}{7}k^{\star}$ $\le$ $-$ $m$ $\leftrightarrow$$-$ $6(1 - \alpha)\frac{8}{7}k^{\star}$ $\le$ $-$ $6(1 - \alpha)m$.If there is $\alpha &gt; \frac{433}{440}$,$\overline{k}$ $\ge$$(\frac{55}{7}\alpha - \frac{48}{7})k^{\star}$ $&gt;$$(\frac{55}{7}\frac{433}{440} - \frac{48}{7})k^{\star}$ $=$$(\frac{7}{8})k^{\star}$.Now, we show that such $\alpha$-approximation for MAX 2SAT can be used to give $(\frac{7}{8} + \epsilon)$-approximation algorithm for MAX-E3SAT problem for some constant $\epsilon &gt; 0$ and $\alpha &gt; \frac{433}{440}$.Which is a contradiction.Therefore, there is no such an approximation algorithm.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Approximation algorithm(13) - Buy-at-bulk network design]]></title>
      <url>/algorithm/approximation/2021/05/26/Approximation-algorithm(13)/</url>
      <content type="text"><![CDATA[In the real world, it is usually cheaper when you buy some thing in a bulk because of the delivery costs.Now think about a undirected graph $G$ $=$ $(V,E)$ with length $l_e$ $\ge$ $0$ for all $e$ $\in$ $E$.This problem requires $k$ pairs of vertices $(s_i,t_i)$ and demand $d_i$.One thing that makes this problem interesting is that there is a cost function $f(u)$ such that satisfies following.  $f(0)$ $=$ $0$  $f$ is non-decreasing.  $f$ is subadditive which means $f(x + y)$ $\le$ $f(x)$ $+$ $f(y)$ for all $x, y$ $\in$ $\mathbb{N}$Now problem asks to find $k$ paths from $s_i$ to $t_i$ with capacity $c:E \rightarrow \mathbb{N}$to minimize $\sum\limits_{e \in E}f(c_e)l_e$such that for any edge we can send $d_i$ units of commodity from $s_i$ to $t_i$ at the same time without violating $c$.Notice that this algorithm can be solved in polynomial time if $G$ is a tree.The reason is like follow.If you think about any possible path on a tree, there is a unique path from $u$ to $v$ where $u,v$ $\in$ $V$.Therefore, solution is just find a unique path and set the capacity and that’s all.Notice that this means it’s not just in polynomial time but it gives a trivial unique solution.Now, let’s consider the following algorithm.    $\operatorname{for}$ each pair of vertices $u,v$ in $V$        $P_{uv}$ be the shortest path from $u$ to $v$ in $E$        $d_{uv}$ be the length of shortest path from $u$ to $v$.    Find a tree metric $(V',T)$ that approximates $d$    $\operatorname{for}$ each pair of vertices $u,v$ in $V$        $P'_{uv}$ be the shortest path from $u$ to $v$ in $T$        $c_e \leftarrow 0$ for all $e$ $\in$ $E$    $\operatorname{for}$ each pair $s_i, t_i$        $\operatorname{for}$ each edge $(u,v)$ in $P'_{s_i t_i}$            $\operatorname{for}$ each edge $e$ in $P_{u v}$                Increase $c_e$ $\leftarrow$ $c_e$ $+$ $d_i$                             return $c$First of all, we need to show that there is a tree metric.Therefore we need to show that $d$ is a pseudometric.Notice that most of properties are trivial but only triangular inequality need to be more detial.Now, let’s think about any path between $(x,y)$ and $(y,z)$.Then, $d_{xy}$ $+$ $d_{yz}$ $=$$\sum\limits_{e \in P_{xy}}l_e$ $+$ $\sum\limits_{e \in P_{yz}}l_e$ $=$$\sum\limits_{e \in P_{xy} \cup P_{yz}}l_e$ $\ge$ $\sum\limits_{e \in P_{xz}}l_e$ $=$ $d_{xz}$.Notice that concatinating path from $x$ to $y$ and path from $y$ to $z$ is a path from $x$ to $z$.Which means at least longer or equal than “shortest” path from $x$ to $z$ in other world $P_{xy} \cup P_{yz}$ $\supseteq$ $P_{xz}$.Now, problem is that we need to go through some $x$ that was not in $V$ but is in $V’$.Therefore, we need to remove every such vertices.Here is another algorithm that gives a tree metric from a tree metric.    $\operatorname{fit}$(V, V', T)        $T' \leftarrow T$        $\operatorname{while}$ $\exists v \in V$ and $v$'s parent $w$ such that $w$ was not a left node of $T$            Merge $v$ and $w$ to $v$                Multiply the length of every edge of $T'$ by 4        return $(V, T')$    If given a tree metric $T$ can be like follow.    Your browser does not support the HTML canvas tag.Then, other tree metric $T’$ can be like follow.    Your browser does not support the HTML canvas tag.Then, this algorithm returns a tree metric on $V$ such that $T_{uv}$ $\le$ $T’_{uv}$ $\le$ $4T_{uv}$ for all $u,v$ $\in$ $V$.Notice that $T$ above is a result of original tree metric approximation algorithm.Proof is like follow.First, $T’_{uv}$ $\le$ $T_{uv}$ untill we multiply 4 because we only merge the vertices.Therefore, $T’_{uv}$ $\le$ $4T_{uv}$ is true at the end of the algorithm.Now, let’s recap some facts from the tree metric $T$.  $\mathcal{L}_n$ is the level of the $n$. Notice that level of root node is $\log_2 \Delta$ and level of leaf node is $0$.  $\mathcal{A}_{uv}$ is the least common ancestor of $u$ and $v$.Then, $T_{uv}$ $=$ $2\sum_{k=1}^{\mathcal{L}_{\mathcal{A}_{uv}}}2^k$ $=$ $2^{\mathcal{L}_{\mathcal{A}_{uv}} + 2} - 4$ is true.Now, let’s think about the smallest possible length for $T’_{uv}$.Then, $u$ and $v$ will go only to the parent and the possible most go is right below $\mathcal{A}_{uv}$.One of $u$ and $v$ may be can be merged to $\mathcal{A}_{uv}$ but not other one of $u$ and $v$ can be $\mathcal{A}_{uv}$.As a result, one of edge from $\mathcal{A}_{uv}$ to child will still left to exist.Therefore, $T’_{uv}$ $\ge$$4 \cdot 2^{\mathcal{L}_{\mathcal{A}_{uv}}}$ $=$ $2^{\mathcal{L}_{\mathcal{A}_{uv}} + 2}$ $\ge$$2^{\mathcal{L}_{\mathcal{A}_{uv}} + 2} - 4$ $=$$T_{uv}$Therefore, claim holds.Notice that this means $d_{uv}$ $\le$ $T_{uv}$ $\le$ $T’_{uv}$ and $E[T’_{uv}]$ $\le$ $E[4T_{uv}]$ $=$ $4E[T_{uv}]$ $\le$ $O(\ln \left\vert V \right\vert)d_{uv}$.Therefore, $d_{uv}$ $\le$ $T’_{uv}$ and $E[T’_{uv}]$ $\le$ $O(\ln \left\vert V \right\vert)d_{uv}$.Now, think about the algorithm follow.    $\operatorname{for}$ each pair of vertices $u,v$ in $V$        $P_{uv}$ be the shortest path from $u$ to $v$ in $E$        $d_{uv}$ be the length of shortest path from $u$ to $v$.    Find a tree metric $(V',T)$ that approximates $d$    $(V,T')$ $\leftarrow$ $\operatorname{fit}(V, V', T)$    $\operatorname{for}$ each pair of vertices $u,v$ in $V$        $P'_{uv}$ be the shortest path from $u$ to $v$ in $T'$        $c_e \leftarrow 0$ for all $e$ $\in$ $E$    $\operatorname{for}$ each pair $s_i, t_i$        $\operatorname{for}$ each edge $(u,v)$ in $P'_{s_i t_i}$            $\operatorname{for}$ each edge $e$ in $P_{u v}$                Increase $c_e$ $\leftarrow$ $c_e$ $+$ $d_i$                             return $c$Then this algorithm is a $O(\log n)$-approximation algorithm.Proof is like follow.Let’s denote some terminologies.  $P^{\star}_{u v}$ is the shortest path between $u$ and $v$ from the optimal solution.  $c^{\star}_e$ for $e$ $\in$ $E$ is the capacity of $e$ from the optimal solution.  $\operatorname{OPT}$ is the optimal solution; $\operatorname{OPT}$ $=$ $\sum\limits_{e \in E}f(\sum\limits_{i = 1 : e \in P^{\star}_{s_i t_i}}^{k} d_i)l_e$ $=$ $\sum\limits_{(u,v) \in E} f(\sum\limits_{i = 1 : (u,v) \in P^{\star}_{s_i t_i}}^{k} d_i)d_{uv}$ $=$$\sum\limits_{(u,v) \in E} f(\sum\limits_{i = 1}^{k} d_i \mathbb{1}((u,v) \in P^{\star}_{s_i t_i})) d_{uv}$.  $P’_{u v}$ is the unique shortest path between $u$ and $b$ from the $T’$.  $P^{S}_{u v}$ is a path that changes each edge $(x,y)$ in $P^{\star}_{u v}$ to $P’_{x, y}$.  $\operatorname{OPT’}$ is the solution from $P^{S}_{s_i t_i}$; $\operatorname{OPT’}$ $=$$\sum\limits_{e \in T’}f(\sum\limits_{i = 1 : e \in P^{S}_{s_i t_i}}^{k} d_i)l_e$ $=$$\sum\limits_{(x,y) \in T’} f(\sum\limits_{i = 1}^{k} d_i \sum\limits_{(u,v) \in E} \mathbb{1}((u,v) \in P^{\star}_{s_i t_i} \text{ and } (x,y) \in P’_{u v}))T’_{xy}$.Notice that $P^{S}_{s_i t_i}$ may not be simple.Then, $E[\operatorname{OPT’}]$ $=$$E[\sum\limits_{(x,y) \in T’} f(\sum\limits_{i = 1}^{k} d_i \sum\limits_{(u,v) \in E} \mathbb{1}((u,v) \in P^{\star}_{s_i t_i} \text{ and } (x,y) \in P’_{u v}))T’_{xy}]$ $\le$$E[\sum\limits_{(x,y) \in T’} \sum\limits_{(u,v) \in E} f(\sum\limits_{i = 1}^{k}d_i \mathbb{1}((u,v) \in P^{\star}_{s_i t_i} \text{ and } (x,y) \in P’_{u v}))T’_{xy}]$ $=$$E[\sum\limits_{(u,v) \in E} \sum\limits_{(x,y) \in T’} f(\sum\limits_{i = 1}^{k}d_i \mathbb{1}((u,v) \in P^{\star}_{s_i t_i} \text{ and } (x,y) \in P’_{u v}))T’_{xy}]$ $=$$E[\sum\limits_{(u,v) \in E} \sum\limits_{(x,y) \in P’_{u v}} f(\sum\limits_{i = 1}^{k}d_i \mathbb{1}((u,v) \in P^{\star}_{s_i t_i}))T’_{xy}]$ $=$$\sum\limits_{(u,v) \in E} E[ \sum\limits_{(x,y) \in P’_{u v}} f(\sum\limits_{i = 1}^{k}d_i \mathbb{1}((u,v) \in P^{\star}_{s_i t_i}))T’_{xy}]$ $=$$\sum\limits_{(u,v) \in E} E[ f(\sum\limits_{i = 1}^{k}d_i \mathbb{1}((u,v) \in P^{\star}_{s_i t_i})) \sum\limits_{(x,y) \in P’_{u v}}T’_{xy}]$ $=$$\sum\limits_{(u,v) \in E} f(\sum\limits_{i = 1}^{k} d_i \mathbb{1}((u,v) \in P^{\star}_{s_i t_i})) E[ \sum\limits_{(x,y) \in P’_{u v}}T’_{xy}]$ $=$$\sum\limits_{(u,v) \in E} f(\sum\limits_{i = 1}^{k} d_i \mathbb{1}((u,v) \in P^{\star}_{s_i t_i})) E[T’_{uv}]$ $\le$$\sum\limits_{(u,v) \in E} f(\sum\limits_{i = 1}^{k} d_i \mathbb{1}((u,v) \in P^{\star}_{s_i t_i})) O(\ln \left\vert V \right\vert)d_{uv}$ $=$$O(\ln \left\vert V \right\vert) \sum\limits_{(u,v) \in E} f(\sum\limits_{i = 1}^{k} d_i \mathbb{1}((u,v) \in P^{\star}_{s_i t_i})) d_{uv}$ $=$$O(\ln \left\vert V \right\vert) \operatorname{OPT}$.Notice that following facts.First inequality holds because $f$ is subadditive.Third equality holds because $P’_{uv}$ $\in$ $T’$.Sixth equality holds because $f(\sum\limits_{i = 1}^{k} d_i \mathbb{1}((u,v) \in P^{\star}_{s_i t_i}))$ is independent from $u,v$.Seventh equality holds because $\sum\limits_{(x,y) \in P’_{u v}}T’_{xy}$ is the distance from $u$ to $v$.Therefore claim holds.Simiallary, following is true.Now let’s denote $\operatorname{ALG}$ as the value of the output solution.Then, $\operatorname{ALG}$ $=$$\sum\limits_{(u,v) \in E} f(\sum\limits_{i = 1}^{k} d_i \sum\limits_{(x,y) \in T’} \mathbb{1}((x,y) \in P’_{s_i t_i} \text{ and } (u,v) \in P_{xy}))d_{uv}$ $\le$$\sum\limits_{(u,v) \in E} \sum\limits_{(x,y) \in T’} f(\sum\limits_{i = 1}^{k} d_i \mathbb{1}((x,y) \in P’_{s_i t_i} \text{ and } (u,v) \in P_{xy}))d_{uv}$ $=$$\sum\limits_{(x,y) \in T’} \sum\limits_{(u,v) \in E} f(\sum\limits_{i = 1}^{k} d_i \mathbb{1}((x,y) \in P’_{s_i t_i} \text{ and } (u,v) \in P_{xy}))d_{uv}$ $=$$\sum\limits_{(x,y) \in T’} \sum\limits_{(u,v) \in P_{xy}} f(\sum\limits_{i = 1}^{k} d_i \mathbb{1}((x,y) \in P’_{s_i t_i}))d_{uv}$ $=$$\sum\limits_{(x,y) \in T’} f(\sum\limits_{i = 1}^{k} d_i \mathbb{1}((x,y) \in P’_{s_i t_i})) \sum\limits_{(u,v) \in P_{xy}}d_{uv}$ $=$$\sum\limits_{(x,y) \in T’} f(\sum\limits_{i = 1}^{k} d_i \mathbb{1}((x,y) \in P’_{s_i t_i})) d_{xy}$ $\le$$\sum\limits_{(x,y) \in T’} f(\sum\limits_{i = 1}^{k} d_i \mathbb{1}((x,y) \in P’_{s_i t_i})) T’_{xy}$ $\le$$\sum\limits_{(x,y) \in T’} f(\sum\limits_{i = 1}^{k} d_i \sum\limits_{(u,v) \in E} \mathbb{1}((u,v) \in P^{\star}_{s_i t_i} \text{ and } (x,y) \in P’_{u v}))T’_{xy}$ $=$$\operatorname{OPT’}$.All eqaulities and inequalities except last ineuqality can be proven in the same way with above.Therefore, we need to show only last inequality holds.Proof for “$\sum\limits_{(x,y) \in T’} f(\sum\limits_{i = 1}^{k} d_i \mathbb{1}((x,y) \in P’_{s_i t_i})) T’_{xy}$ $\le$$\sum\limits_{(x,y) \in T’} f(\sum\limits_{i = 1}^{k} d_i \sum\limits_{(u,v) \in E} \mathbb{1}((u,v) \in P^{\star}_{s_i t_i} \text{ and } (x,y) \in P’_{u v}))T’_{xy}$” is like follow.Let’s think about “$\mathbb{1}((x,y) \in P’_{s_i t_i})$” and “$\sum\limits_{(u,v) \in E} \mathbb{1}((u,v) \in P^{\star}_{s_i t_i} \text{ and } (x,y) \in P’_{u v})$” for some $x, y, i$.Then first one is $1$ if $(x,y) \in P’_{s_i t_i}$ and $0$ otherwise.Therefore, there is nothing to show if $(x,y) \not\in P’_{s_i t_i}$.Now, let’s assume that $(x,y) \in P’_{s_i t_i}$.Then, $P^{\star}_{s_i t_i}$ should include at least $(s_i,t_i)$ or longer path from $s_i$ to $t_i$ for any $i$.Which means the cardinarity of $\{(u,v) \in P^{\star}_{s_i t_i} \text{ and } (x,y) \in P’_{u v}\}$ should be bigger or equal than $1$.Therefore, claim holds.Notice that concatinating $(u,v) \in P^{\star}_{s_i t_i}$ in $T’$ will be a valid path from $u$ to $v$.Therefore, $(x,y)$ should be in some where in there.As a result, $\operatorname{ALG}$ $\le$ $\operatorname{OPT’}$ $\le$ $O(\ln \left\vert V \right\vert) \operatorname{OPT}$.Therefore claim holds.Notice that algorithm runs in a polynomial time.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Approximation of metrics by tree metrics]]></title>
      <url>/algorithm/approximation/2021/05/19/tree-metrics/</url>
      <content type="text"><![CDATA[For a given vertices $V$ and distance $V \times V \rightarrow \mathcal{R} : d$.$(V,d)$ is so called a metric if following properties are hold.  $d_{uv}$ $\ge$ $0$ for all $u,v$ $\in$ $V$  $d_{uv}$ $=$ $0$ iff $u = v$  $d_{uv}$ $=$ $d_{vu}$ for all $u,v$ $\in$ $V$  (Triangular inequality) $d_{uv}$ $\le$ $d_{uw}$ $+$ $d_{wv}$ for all $u,v,w$ $\in$ $V$Notice that we say it is a pseudometric if property 2 changes to “$d_{uv}$ $=$ $0$ if $u = v$”.A tree metic $(V’, T)$ for a set of vertices $V$ is a tree $T$ defined on a set of nodes $V’ \supseteq V$, whoese edges are given non negative lengths.For $u,v$ $\in$ $V’$, let $T_{uv}$ denote the length of the unique path between $u$ and $v$ in $T$.Notice that tree matrix is a matric.It is trivial to be hold for 1 ~ 3.For 4, if there is a path $u$ to $w$ and $w$ to $v$, then concatinating them is a path from $u$ to $v$.It will have the length equal or less than sum of each path because there could be a cycle and it can be removed.Given a metric $d$ on $V$, we say a tree metric $(V’, T)$ is a $\operatorname{tree metic embedding}$ of distortion $\alpha$ if $d_{uv}$ $\le$ $T_{uv}$ $\le$ $\alpha d_{uv}$ for all $u,v$ $\in$ $V$.However, do we even have any approximation for it in every time?Yes, if we have gigantic distortion.However, it’s no with some distortion.In fact it is known that there is no tree metric has distortion less than $\frac{n - 1}{8}$ for some metric $d$ on $n$ vertices.However there is a good theorem.Given a metric $d$ on $V$ such that $d_{uv}$ $\ge$ $1$ for all $u$ $\neq$ $v$, there exists a randomized algorithm that finds a tree metric $(V’, T)$ such that for all $u, v$ $\in$ $V$, $d_{uv}$ $\le$ $T_{uv}$ and $E[T_{uv}]$ $\le$ $O(\ln \left\vert V \right\vert)d_{uv}$.Notice that this randomized algorithm picks a tree metric from a given graph.Proof is like follow.Consider the following algorithm where $B(x, r)$ is a hypersphere with the center at $x$ and radius of $r$.    Pick $r_0$ $\in$ $[\frac{1}{2}, 1)$ uniformly at random    Choose $\Delta$ as the smallest power of two greater or equal than $2\max_{u,v \in V}d_{uv}$    Let $r_i$ $=$ $2^ir_0$ for $1$ $\le$ $i$ $\le$ $\log_2 \Delta$    Pick a permutation $\pi$ of $v$ uniformly at random    $\mathcal{C}(\log_2 \Delta) \leftarrow \{V\}$    Create a node corresponding to $V$ and make it the root node    $\operatorname{for}$ $i \leftarrow \log_2 \Delta, \log_2 \Delta - 1, \cdots, 1$        $\mathcal{C}(i - 1) \leftarrow \emptyset$        $\operatorname{for}$ $C \in \mathcal{C}(i)$            $S \leftarrow C$            $\operatorname{for}$ $j \leftarrow 1, 2, \cdots, \left\vert V \right\vert$                $\operatorname{if}$ $B(\pi(j), r_{i-1})$ $\cap$ $S$ $\neq$ $\emptyset$                    Add $\{B(\pi(j), r_{i-1}) \cap S\}$ to $\mathcal{C}(i -1)$                    $S$ $\leftarrow$ $S$ $-$ $(B(\pi(j), r_{i-1}) \cap S)$                                        Create nodes corresponding to each set in $\mathcal{C}(i -1)$ and attach each node to the node in $\mathcal{C}(i)$ corresponding to its superset by an edge of length $2^i$                $V' \leftarrow$ all nodes in $\bigcup\limits_{k = 0}^{\log_2 \Delta}\mathcal{C}(k)$    $T \leftarrow$ all edges between $\mathcal{C}(k)$ and $\mathcal{C}(k - 1)$ for all $1$ $\le$ $k$ $\le$ $\log_2 \Delta$    return $(V', T)$ For an example, following graph’s result will be like follow.If given metric is like follow.    Your browser does not support the HTML canvas tag.Returned tree metric can be like follow.    Your browser does not support the HTML canvas tag.Now, following is true if we call the deepest nodes as level 0 and level $i$’s parent as level $i + 1$.Note that each node at level 0 corresponding to a singleton and every vertex in $V$ appears exactly once.The reason is that there can’t be more than center itself because $r_0$ $&lt;$ $1$ $\le$ $d_{uv}$ for all $u,v$ $\in$ $V$.Notice that every vertex at level $i$ is belongs to a hyper sphere of radius $r_i$ and centered by one of vertex in side of the set.Which also means that level $\log_2 \Delta$ will contain entire $V$ because $r_{\log_2 \Delta}$ $=$ $2^{\log_2 \Delta} r_0$ $\ge$ $2^{\log_2 \Delta} \frac{1}{2}$ $=$ $\Delta \frac{1}{2}$ $\ge$ $2\max_{u,v \in V}d_{uv}\frac{1}{2}$ $=$ $\max_{u,v \in V}d_{uv}$.Now, let’s denote some terminologies for a node $n$ in the $(V’, T)$.  $\mathcal{L}_n$ is the level of the $n$. Notice that level of root node is $\log_2 \Delta$ and level of leaf node is $0$.  $\mathcal{S}_n$ is the set of vertices which the corresponding hyper sphere includes.Then, there are some facts.  $d_{yz}$ $\le$ $2r_{\mathcal{L}_n}$ for any $y,z$ $\in$ $\mathcal{S}_n$ becuase it should be in the same hyper sphere.  For any $u,v$ $\in$ $V$, they can’t belongs to the same node at level $[\log_2 d_{uv}] - 1$ because otherwise $d_{uv}$ $\le$ $2r_{[\log_2 d_{uv}] - 1}$ $=$ $2 \cdot 2^{[\log_2 d_{uv}] - 1}r_0$ $=$ $2^{[\log_2 d_{uv}]}r_0$ $\le$ $2^{\log_2 d_{uv}}r_0$ $=$ $d_{uv}r_0$ $&lt;$ $d_{uv}$ which is a contradiction.Then, $T_{uv}$ $\ge$ $d_{uv}$ is true.Proof is like follow.If $d_{uv}$ $&lt;$ $4$ then, $T_{uv}$ $\ge$ $d_{uv}$ because $T_{uv}$ $\ge$ $4$.Notice that $T_{uv}$ $\ge$ $4$ is true because all edges in the $T$ is bigger or equal than $2$ and there should be at least one parent to go $v$ from $u$.Now, other cases are $d_{uv}$ $\ge$ $4$.Frist, $d_{uv}$ $\le$ $\sum\limits_{k = 0}^{[\log_2 d_{uv}]} 2^k$ because RHS is bigger than a binary representation of $d_{uv}$.Then, $d_{uv}$ $\le$ $\sum\limits_{k = 0}^{[\log_2 d_{uv}]} 2^k$ $=$ $\sum\limits_{k = 1}^{[\log_2 d_{uv}]} 2^k$ $+$ $1$ $\le$ $\sum\limits_{k = 1}^{[\log_2 d_{uv}]} 2^k$ $+$ $\sum\limits_{k = 1}^{[\log_2 d_{uv}]} 2^k$ $=$ $2\sum\limits_{k = 1}^{[\log_2 d_{uv}]} 2^k$ $\le$ $T_{uv}$.Notice that last inequality comes from the fact 2 above.If we think about path from $u$ to $v$, it should go to at least level $[\log_2 d_{uv}]$ because it can’t belongs to the same node untill level $[\log_2 d_{uv}] - 1$.Also, $2\sum\limits_{k = 1}^{[\log_2 d_{uv}]} 2^k$ is a sum of length from $u$ to $v$ via level $[\log_2 d_{uv}]$.Therefore, claim holds.Now there are only one thing to show that $E[T_{uv}]$ $\le$ $O(\ln \left\vert V \right\vert)d_{uv}$.To show this, there some terminologies to define.  $\mathcal{A}_{uv}$ is the least common ancestor of $u$ and $v$.  We say $w$ settles the pair of $u$ and $v$ on $i$ if $w$ is the first vertex in permutation $\pi$ such that at least one of $u$ and $v$ is in the hypersphere $B(w, r_i)$.  We say $w$ cut $u$ and $v$ on level $i$ if exactly one of $u$ and $v$ is in the hypersphere $B(w, r_i)$.  $X_{iw}(u,v)$ be the event that $w$ cuts $(u, v)$ on level $i$.  $S_{iW}(u,v)$ be the event that $w$ settles $(u, v)$ on level $i$.  $\mathbb{1}(x)$ is an indicator fuction such that $1$ if $x$ is true $0$ otherwise.First, $T_{uv}$ $=$ $2\sum_{k=1}^{\mathcal{L}_{\mathcal{A}_{uv}}}2^k$ $=$ $2(2^{\mathcal{L}_{\mathcal{A}_{uv}} + 1} - 2)$ $=$ $2^{\mathcal{L}_{\mathcal{A}_{uv}} + 2} - 4$ $\le$ $2^{\mathcal{L}_{\mathcal{A}_{uv}} + 2}$.Then, $T_{uv}$ $\le$ $\max\limits_{i = 0}^{\log_2 \Delta - 1} \mathbb{1}(\exists X_{iw}(u,v) \cap S_{iw}(u,v) \text{ for } w \in V) 2^{i + 3}$.Notice that $\operatorname{argmax}\limits_{i = 0}^{\log_2 \Delta - 1} \mathbb{1}(\exists X_{iw}(u,v) \cap S_{iw}(u,v) \text{ for } w \in V)2^{i + 3}$ is the $\mathcal{A}_{uv}$.Therefore, $T_{uv}$ $\le$ $\max\limits_{i = 0}^{\log_2 \Delta - 1} \mathbb{1}(\exists X_{iw}(u,v) \cap S_{iw}(u,v) \text{ for } w \in V) 2^{i + 3}$$\le$ $\sum\limits_{w \in V}\sum\limits_{i = 0}^{\log_2 \Delta - 1} \mathbb{1}(\exists X_{iw}(u,v) \cap S_{iw}(u,v)) 2^{i + 3}$.Now if we go back to the algorithm, there are two random events at the algorithm which are “picking $r_0$” and “picking a random permutation $\pi$”.Therefore, we can average on certain path to select $T_{uv}$.Then, $E[T_{uv}]$ $\le$ $\sum\limits_{w \in V}\sum\limits_{i = 0}^{\log_2 \Delta - 1} Pr[X_{iw}(u,v) \cap S_{iw}(u,v)] 2^{i + 3}$.Notice that existance of $X_{iw}(u,v) \cap S_{iw}(u,v)$ for $w$ $\in$ $V$ on level $i$ will be depend on random varaibles and other things are indepedent with the random variable.Now, let’s consider followings.With out loosing generality, let’s assume that $d_{uw}$ $\le$ $d_{vw}$ for $w$ that cuts $u$ and $v$ on level $i$.Then, $d_{uw}$ $\le$ $r_i$ $&lt;$ $d_{vw}$ because $u$ should be in $B(w, r_i)$ and $v$ shouldn’t.With above, we need to select $r_i$ uniformly at random with $2^ir_0$.Then we will pick $r_i$ in $[2^{i-1},2^i)$ because we will pick $r_0$ in $[\frac{1}{2}, 1)$. Now, let’s think about the $Pr[X_{iw}(u,v)]$.Then, $Pr[X_{iw}(u,v)]$ $=$ $\frac{\left\vert [2^{i-1}, 2^i) \cap [d_{uw}, d_{vw}) \right\vert}{\left\vert [2^{i-1}, 2^i \right\vert}$ $=$ $\frac{\left\vert [2^{i-1}, 2^i) \cap [d_{uw}, d_{vw}) \right\vert}{2^{i-1}}$.As a result, $\sum\limits_{i=0}^{\log_2 \Delta - 1}Pr[X_{iw}(u,v)]2^{i + 3}$ $=$ $\sum\limits_{i=0}^{\log_2 \Delta - 1}\frac{\left\vert [2^{i-1}, 2^i) \cap [d_{uw}, d_{vw}) \right\vert}{2^{i-1}}2^{i + 3}$ $=$$\sum\limits_{i=0}^{\log_2 \Delta - 1}\frac{2^{i + 3}}{2^{i-1}}\left\vert [2^{i-1}, 2^i) \cap [d_{uw}, d_{vw}) \right\vert$ $=$$2^4\sum\limits_{i=0}^{\log_2 \Delta - 1}\left\vert [2^{i-1}, 2^i) \cap [d_{uw}, d_{vw}) \right\vert$ $=$$16\sum\limits_{i=0}^{\log_2 \Delta - 1}\left\vert [2^{i-1}, 2^i) \cap [d_{uw}, d_{vw}) \right\vert$ $=$$16\left\vert [d_{uw}, d_{vw}) \right\vert$ for any $w$.Notice that $\bigcup\limits_{i=0}^{\log_2 \Delta - 1}[2^{i-1}, 2^i)$ $=$ $[2^{-1}, 2^{\log_2 \Delta - 1})$ $=$ $[2^{-1}, 2^{-1}\Delta)$ $=$ $[2^{-1}, 2^{-1}2\max_{u,v \in V}d_{uv})$ $=$ $[2^{-1}, \max_{u,v \in V}d_{uv})$ $\supseteq$ set of possible $d_{uv}$ for all $u,v$ $\in$ $V$.Which means it will see all possible area.Notice that $d_{uv}$ $\ge$ $1$ and $Pr[X_{iw}(u,v)]$ means probability that $w$ on level $i$ will cut $u$ and $v$.As a result, $Pr[X_{iw}(u,v)]$ $=$ $16\left\vert [d_{uw}, d_{vw}) \right\vert$ $=$ $16(d_{vw} - d{uw})$ $\le$ $16(d_{vu} + d_{uw} - d_{uw})$ $=$ $16d_{vu}$ $=$ $16d_{uv}$ from the triangular inequality.Now, let’s think about list $\mathbb{L}$ from $V$ such that sorted in the order $\min(d_{ux}, d_{vx})$ for $x$ $\in$ $\mathbb{L}$.Now, think about 2 things.  Some $w$ $\in$ $V$ such that $w$ cuts $u$ and $v$.  Let’s denote $w$’s level as $i$.  Let’s denote $w$’s index in $\mathbb{L}$ is $\mathcal{I}_w$Then, one of $u$ or $v$ should be in $B(z, r_i)$ for any $z$ that has less index than $\mathcal{I}_w$.Notice that $\min(d_{uz}, d_{vz})$ $\le$ $\min(d_{uw}, d_{vw})$ $\le$ $r_i$ because it is sorted by $\min(d_{ux}, d_{vx})$ and $w$ cuts $u$ and $v$.Then, $Pr[S_{iw}(u,v) \vert X_{iw}(u,v)]$ $\le$ $\frac{1}{\mathcal{I}_w}$ because $w$ need to be placed in $\pi$ more previous than all such $z$s.Notice that there are at least $\mathcal{I}_w$ candiadates that can cut $u$ and $v$ and $w$ need to be the first on $\pi$ to settle $u$ and $v$.Moreover, $\sum\limits_{w \in V}Pr[S_{iw}(u,v) \vert X_{iw}(u,v)]$ $\le$ $\sum\limits_{w \in V}\frac{1}{\mathcal{I}_w}$ for a fixed $i$.Notice that if we fix $i$, $\mathcal{I}_w$ will be some arbitrary order of vertices.However that will be still fixed in some way after set-up random variables.Which means $\sum\limits_{w \in V}Pr[S_{iw}(u,v) \vert X_{iw}(u,v)]$ $=$ $\sum\limits_{w \in V : \mathcal{I}_w = k, w \text{ cuts } u \text{ and } v}\frac{1}{k}$ $=$ $\sum\limits_{k = 1 : \mathcal{I}_w = k, w \text{ cuts } u \text{ and } v}^{\left\vert V \right\vert}\frac{1}{k}$ $\le$ $\sum\limits_{k = 1}^{\left\vert V \right\vert}\frac{1}{k}$ $\le$ $\ln \left\vert V \right\vert$ $+$ $1$.Finally, notice that $\mathcal{I}_w$ doesn’t depend on $i$ because it only depend on $\min(d_{ux}, d_{vx})$.Infact, it doesn’t even depend on any random varaible.Now, following 4 things are true for summary.  $E[T_{uv}]$ $\le$ $\sum\limits_{w \in V}\sum\limits_{i = 0}^{\log_2 \Delta - 1} Pr[X_{iw}(u,v) \cap S_{iw}(u,v)] 2^{i + 3}$  $\sum\limits_{i = 0}^{\log_2 \Delta - 1} Pr[X_{iw}(u,v)]2^{i + 3}$ $\le$ $16d_{uv}$  $Pr[S_{iw}(u,v) \vert X_{iw}(u,v)]$ $\le$ $\frac{1}{\mathcal{I}_w}$  $\sum\limits_{w \in V} \frac{1}{\mathcal{I}_w}$ $\le$ $\ln \left\vert V \right\vert$ $+$ $1$As a result, $E[T_{uv}]$ $\le$$\sum\limits_{w \in V}\sum\limits_{i = 0}^{\log_2 \Delta - 1} Pr[X_{iw}(u,v) \cap S_{iw}(u,v)] 2^{i + 3}$ $=$$\sum\limits_{w \in V}\sum\limits_{i = 0}^{\log_2 \Delta - 1} Pr[S_{iw}(u,v) \vert X_{iw}(u,v)]Pr[X_{iw}(u,v)] 2^{i + 3}$ $=$$\sum\limits_{w \in V}\sum\limits_{i = 0}^{\log_2 \Delta - 1} Pr[X_{iw}(u,v)]2^{i + 3} Pr[S_{iw}(u,v) \vert X_{iw}(u,v)]$ $\le$$\sum\limits_{w \in V}\sum\limits_{i = 0}^{\log_2 \Delta - 1} Pr[X_{iw}(u,v)]2^{i + 3}\frac{1}{\mathcal{I}_w}$ $=$$\sum\limits_{w \in V}\frac{1}{\mathcal{I}_w}\sum\limits_{i = 0}^{\log_2 \Delta - 1} Pr[X_{iw}(u,v)]2^{i + 3}$ $\le$$\sum\limits_{w \in V}\frac{1}{\mathcal{I}_w}16d_{uv}$ $=$$16d_{uv}\sum\limits_{w \in V} \frac{1}{\mathcal{I}_w}$ $\le$$16d{uv}(\ln \left\vert V \right\vert + 1)$ $=$ $O(\ln \left\vert V \right\vert)d_{uv}$.Therefore, claim holds.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Useful mathematical tools]]></title>
      <url>/algorithm/approximation/2021/05/19/Useful-mathematical-tools/</url>
      <content type="text"><![CDATA[  $\ln n$ $\le$ $\sum\limits_{k = 1}^{n} \frac{1}{k}$ $\le$ $\ln n$ $+$ $1$ for $n$ $\in$ $\mathbb{Z}^{+}$  $\sum\limits_{k = 1}^{n}ar^{k-1}$ $=$ $a\frac{r^{n} - 1}{r - 1}$ if $r$ $\neq$ $1$, $n$ $\in$ $\mathbb{Z}$]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Graph coloring]]></title>
      <url>/algorithm/approximation/2021/05/15/Graph-coloring/</url>
      <content type="text"><![CDATA[We need some definitions and theorems to discuss about the coloring.Definitions  A coloring of graph $G$ is an assignment of colors to the vertices of $G$.In this case, it can be considered as the assignemnts of sets like vertex 1 to set 1 and vetex 2 to set 3 and so-on.  A graph is is $k$-coloring if each vertex is assigned to exactly one of $k$ colors.  A coloring is proper if no two adjacent vertices are assigned to the same color.  A graph is so-called $k$-colorable if it has a proper $k$-coloring.  We say $g(n)$ $=$ $\tilde{O}(f(n))$ if there exists some constant $c$ $\ge$ $0$ such that $g(n)$ $=$ $O(f(n))\ln^c$.This is some extention that ignoring logarithmic terms.  We say coloring is a semicoloring if at most $\frac{\left\vert V \right\vert}{4}$ edges have endpoints with the same color.Notice that semicoloring don’t need to be proper.It allows $\frac{\left\vert V \right\vert}{4}$ edges to be collide.Theorems  There is a polynomial time algorithm to check 2-colorable.Notice that we can do it by DFS and checking there is some pair of vertices that colors collide.  It is known to be NP-hard to decide if a graph is 3-colorable.  It is known to be NP-hard to decide if a graph is 3-colorable or any of its proper coloring requires at least five colors.This means input will give like 3-colorable, 5-colorable, 6-colorable, etc..There will no 4-colorable inputs.  It is known to be NP-hard to find a 3-coloring from a 3-colorable graph.Notice that if there is such an algorithm in polynomial time then running that algorithm and checking the result are enough to solve 3-colorable problem.  A graph with maximum degree $\delta$ can be $(\delta + 1)$-colored in polynomial time.Notice that all of vertices never run out of colors in this case because even all the neighbors use seperate colors, there are still one color to use.  3-colorable graph can be $O(\sqrt{n})$-colored in polynomial time for $n$ as the number of vertices.  There exists a polynomial time random algorithm that finds a semicoloring with $4\Delta^{\log_3 2}$ colors at least $\frac{1}{2}$ probability where $\Delta$ is the maximum degree of a graph with.  There is a polynomial algorithm that finds $\tilde{O}(n^{\log_6 2})$-coloring of a given 3-colorable graph at least $\frac{1}{2}$ probability.Proof of theoremsTheorem 6 can be done by following algorithm.    $\operatorname{while} \exists$ a vertex $v$ with at least degree $\sqrt{n}$        Choose three new colors $c_1, c_2, c_3$        Color $v$ with $c_1$        Color neighbors of $v$ with $c_2, c_3$        Remove colored vertices        Color the rest of graph with $\sqrt{n}$ new colorsNotice that the neighbors of $v$ can be colored with 2 colors in each iteration because given graph is a 3-colorable graph.The reason is that all of neighbor of $v$ can’t be $c_1$ if we select $v$ as $c_1$.Then, it’s a 2-coloring problem which can be solve in polynomial time.With that, $v$ and its neighbors are using distinct from other vertices because it producess new colors.Notice that we can remove it because it uses independent color sets.Now, it uses just $\sqrt{n}$ color at the last.Therefore it’s enough to show there are at most $O(\sqrt{n})$ iteration for “ $\operatorname{while} \exists$ a vertex $v$ with at least degree $\sqrt{n}$”.With that, it removes at least $\sqrt{n}I$ vetices from graph if we denote $I$ as the number of such an iteration because each vertices has $\sqrt{n}$ degree.However, we can’t remove more vetices than number of vertices at the beginning.Therefore, $\sqrt{n}I$ $\le$ $n$.As a result, $I$ $\le$ $\sqrt{n}$.Notice that algorithm uses at most $4\sqrt{n}$ colors and it runs in a polynomial time.Theorem 7 can be done by following algorithm.Consider the following vector program for given graph $G$ $=$ $(V,E)$.Let $n$ $=$ $\left\vert V \right\vert$Minimize $\lambda$such that    $v_i \cdot v_j \le \lambda$ for all $(i, j) \in E$    $v_i \cdot v_i = 1$ for all $i \in V$    $v_i \in \mathcal{R}^n$ for all $i \in V$Then, there is a feasible solution such that $v_i \cdot v_j$ $=$ $-\frac{1}{2}$ for all $(i,j) \in E$ if given graph is a 3-colorable graph.Proof is like follow.Consider an equilateral triangle inscribed in the intersection of the unit hypersphere and a hyperplane containing the origin.Fix a 3-coloring and assign one of the three vertices of the triangle as the vector of each vetex in $V$ so that vertices are assigned the same vector iff they are assigned the same color.Notice that all $v_i$ will be on this hypersphere because “$v_i \cdot v_i = 1$ for all $i \in V$”.Now, we have $v_i \cdot v_j$ $=$ $\left\vert v_i \right\vert \left\vert v_j \right\vert \cos (\frac{2\pi}{3})$ $=$ $-\frac{1}{2}$.Notice that angle is $\frac{2\pi}{3}$ because it’s an equilateral triangle.Then, we can design an algorithm like follow.    Solve algorithm above to obtain optimal solution $v_1, \cdots, v_n$    Choose vector $r_1, \cdots, r_t$ independently and uniformly at random from the unit hypersphere where $t = 2 + \log_3 \Delta$.    Divides the hyper sphere into $2^t$ different regions by half space $H_i = \{x | x \cdot r_i \ge 0\}$ for $0$ $\le$ $i$ $\le$ $t$    Assign a distinct color for all vectices in each resign.Polynomial running time of semidefinitive program/random picking will be updated later.Then, all other process will be in the polynomial time.Notice that this algorithm uses $2^t$ $=$ $2^{2 + \log_3 \Delta}$ $=$ $4 \times 2^{\log_3 \Delta}$ $=$ $4 {\Delta}^{\log_3 2}$ colors.Therefore, it’s enough to show that this makes a semicoloring with $\frac{1}{2}$ probability.Now, let’s think about edge $e$ $=$ $(v_i, v_j)$.With this, consider some $H_k$ and $r^{\star}_k$.Which $r^{\star}_k$ is the projection of $r_k$ on to the plane defined by $v_i, v_j, 0$.Then, $v_i \cdot r_k$ $=$ $v_i \cdot r^{\star}_k$ and $v_i \cdot r_k$ $=$ $v_i \cdot r^{\star}_k$.Notice that $r_k$ can be decomposed to $r^{\star}_k$ and $r^{\circ}_k$.Then, $v_i \cdot r_k$ $=$ $v_i \cdot (r^{\star}_k + r^{\circ}_k)$ $=$ $v_i \cdot r^{\star}_k$ $+$ $v_i \cdot r^{\circ}_k$ $=$$v_i \cdot r^{\star}_k$.This works in the same way for $v_j$ either.Now, $Pr[\text{Both } v_i \text{ and } v_j \text{ are in the } H_k]$ $=$$Pr[\text{Both } v_i \cdot r_k \text{ and } v_j \cdot r_k \text{ has the same sign}]$ $=$ $Pr[\text{Both } v_i \cdot r^{\star}_k \text{ and } v_j \cdot r^{\star}_k \text{ has the same sign}]$Then, $Pr[\text{Both } v_i \cdot r^{\star}_k \text{ and } v_j \cdot r^{\star}_k \text{ has the same sign}]$ $=$ $\frac{\pi - \theta}{\pi}$ which $\theta$ is the angle between $v_i$ and $v_j$.Proof is like follow.Let’s denote $\theta_i, \theta_j, \theta_k$ as the angle of $v_i, v_j, r^{\star}_k$.Then, there are two cases for $\theta_k$ which both $v_i \cdot r^{\star}_k$ and $v_j \cdot r^{\star}_k$ are positive or negative.With this two cases, there are 2 cases per each case for each of them from criteria “$\theta_i = \theta_j + \pi$” to choose the range of possible angle.Therefore, there are 4 cases in total.  $\theta_j - \frac{\pi}{2}$ $\le$ $\theta_k$ $\le$ $\theta_i + \frac{\pi}{2}$ if $\theta_i$ $\le$ $\theta_j$ $\le$ $\theta_i$ $+$ $\pi$ and both $v_i \cdot r^{\star}_k$ and $v_j \cdot r^{\star}_k$ are positive.  $\theta_j + \frac{\pi}{2}$ $\le$ $\theta_k$ $\le$ $\theta_i + \frac{3\pi}{2}$ if $\theta_i$ $\le$ $\theta_j$ $\le$ $\theta_i$ $+$ $\pi$ and both $v_i \cdot r^{\star}_k$ and $v_j \cdot r^{\star}_k$ are negative.  $\theta_i - \frac{\pi}{2}$ $\le$ $\theta_k$ $\le$ $\theta_j + \frac{\pi}{2}$ if $\theta_i$ $-$ $\pi$ $\le$ $\theta_j$ $\le$ $\theta_i$ and both $v_i \cdot r^{\star}_k$ and $v_j \cdot r^{\star}_k$ are positive.  $\theta_i + \frac{\pi}{2}$ $\le$ $\theta_k$ $\le$ $\theta_j + \frac{3\pi}{2}$ if $\theta_i$ $-$ $\pi$ $\le$ $\theta_j$ $\le$ $\theta_i$ and both $v_i \cdot r^{\star}_k$ and $v_j \cdot r^{\star}_k$ are negative.Then, we can know claim holds in each cases.  $2\pi Pr[\text{Both } v_i \cdot r^{\star}_k \text{ and } v_j \cdot r^{\star}_k \text{ has the same sign}]$ $=$$(\theta_i + \frac{\pi}{2})$ $-$ $(\theta_j - \frac{\pi}{2})$ $+$ $(\theta_i + \frac{3\pi}{2})$ - $(\theta_j + \frac{\pi}{2})$  $=$$2(\theta_i - \theta_j)$ $+$ $2\pi$ $=$$2(\pi + \theta_i - \theta_j)$ $=$$2(\pi - (\theta_j - \theta_i))$if $\theta_i$ $\le$ $\theta_j$ $\le$ $\theta_i$ $+$ $\pi$.As a result, $Pr[\text{Both } v_i \cdot r^{\star}_k \text{ and } v_j \cdot r^{\star}_k \text{ has the same sign}]$ $=$ $\frac{\pi - (\theta_j - \theta_i)}{\pi}$.Therefore claim holds in this case.  $2\pi Pr[\text{Both } v_i \cdot r^{\star}_k \text{ and } v_j \cdot r^{\star}_k \text{ has the same sign}]$ $=$$(\theta_j + \frac{\pi}{2})$ $-$ $(\theta_i - \frac{\pi}{2})$ $+$ $(\theta_j + \frac{3\pi}{2})$ $-$ $(\theta_i + \frac{\pi}{2})$ $=$$2(\theta_j - \theta_i)$ $+$ $2\pi$ $=$$2(\pi + \theta_j - \theta_i)$ $=$$2(\pi - (\theta_i - \theta_j))$if $\theta_i$ $-$ $\pi$ $\le$ $\theta_j$ $\le$ $\theta_i$.As a result, $Pr[\text{Both } v_i \cdot r^{\star}_k \text{ and } v_j \cdot r^{\star}_k \text{ has the same sign}]$ $=$ $\frac{\pi - (\theta_i - \theta_j)}{\pi}$.Therefore claim holds in this case.Notice that $Pr[\text{Both } v_i \text{ and } v_j \text{ are in the } H_k]$ $=$$\frac{\pi - \theta}{\pi}$ $=$$1$ $-$ $\frac{\theta}{\pi}$ $=$$1$ $-$ $\frac{\arccos(v_i \cdot v_j)}{\pi}$ $=$$1$ $-$ $\frac{1}{\pi}\arccos(v_i \cdot v_j)$.Therefore, $Pr[\text{Both } i \text{ and } j \text{ assigned to the same color}]$ $=$ $Pr[\text{Both } v_i \text{ and } v_j \text{ are in the same reigon for all half space}]$ $=$ $(1 - \frac{1}{\pi}\arccos(v_i \cdot v_j))^t$.With above, there are two facts from semidefinitive program and proof above.  $v_i \cdot v_j \le \lambda$ for all $(i, j) \in E$  There is a feasible solution such that $v_i \cdot v_j$ $=$ $-\frac{1}{2}$ for all $(i,j) \in E$Now let’s denote semidefinitive program optimum’s $\lambda$ as ${\lambda}^{\star}$.Then, ${\lambda}^{\star}$ $\le$ $-\frac{1}{2}$ because semidefinitive program was minimization problem.Therefore, $Pr[\text{Both } i \text{ and } j \text{ assigned to the same color}]$ $=$$(1 - \frac{1}{\pi}\arccos(v_i \cdot v_j))^t$ $\le$$(1 - \frac{1}{\pi}\arccos({\lambda}^{\star}))^t$ $\le$$(1 - \frac{1}{\pi}\arccos(-\frac{1}{2}))^t$ $=$$(1 - \frac{1}{\pi}\frac{2\pi}{3})^t$ $=$$(1 - \frac{2}{3})^t$ $=$$(\frac{1}{3})^{2 + \log_3 \Delta}$ $=$$\frac{1}{9\Delta}$.Notice that $\arccos x$ is a monotonic decreasing function for $-1$ $\le$ $x$ $\le$ $1$.Therefore, $1$ $-$ $\frac{1}{\pi}\arccos x$ is a monotonic increasing function.Now, sum of degree is $2\left\vert E \right\vert$ and possible maximum sum of degree is $n\Delta$.Therefore, $2\left\vert E \right\vert$ $\le$ $n\Delta$ which means $\frac{\left\vert E \right\vert}{\Delta}$ $\le$ $\frac{n}{2}$.As a result, expected number of edges whose endpoints are colred the same is at most $\left\vert E \right\vert \frac{1}{9\Delta}$ $=$ $\frac{\left\vert E \right\vert}{9\Delta}$ $\le$ $\frac{n}{18}$.Then, $Pr[X \ge \frac{n}{4}]$ $\le$ $\frac{E[X]}{n/4}$ $=$ $\frac{4}{n}E[X]$ $\le$ $\frac{4}{n}\frac{n}{18}$ $=$ $\frac{2}{9}$ $\le$ $\frac{1}{2}$ with markov’s inequality where $X$ denotes number of edges where endpoiunts are colored the same.Therefore claim holds.Theorem 8 can be done by following algorithm.Let $\sigma = n^{\log_6 3}$.    Let's call the below part as "part 1"        $\operatorname{while} \exists$ a vertex $v$ with degree $\ge$ $\sigma$            Color $v$ and it's neighbors using three new colors            Remove the colored vertices                Let's call the below part as "part 2"        $\operatorname{while}$ the graph is not empty, repeat at most $\log_2 n$ times           Try running Algorithm at theorem 7 with $[\log_2(\log_2 n)] + 1$ times to find a semicoloring           $R \leftarrow$ endpoints of edges whose endpoints are colored the same           Color $V - R$ according to the semicoloring, using new colors           Remove the colored vetices                Color the remaining vertices with distinct new colorsNow, it is trivial that this algorithm runs in a polynomial time.Therefore, it is enough to show it returns $\tilde{O}(n^{\log_6 2})$-coloring at least $\frac{1}{2}$ probability.Now, let’s think about part 1.Then, maximum number of iteration is at most $[\frac{n}{\sigma}] + 1$ because it remvoes at least $\sigma$ vertices at once.Therefore, color used at part 1 is at most $3([\frac{n}{\sigma}] + 1)$.Now, let’s think about part 2.Left part of graph has at most $\sigma$ degree because we removed every possible vertices with more than degree $\sigma$.Now, let’s denote the maximum of degree after part 1 as $\Delta$ and the number of vertices as $n’$.If we see each iteration, it fails to find a semicoloring with $(\frac{1}{2})^{[\log_2(\log_2 n)] + 1}$ probability.Which is $(\frac{1}{2})^{[\log_2(\log_2 n)] + 1}$ $=$$(2)^{-[\log_2(\log_2 n)] - 1}$ $\le$$(2)^{-\log_2(\log_2 n) - 1}$ $=$$(2)^{-\log_2(\log_2 n)}\frac{1}{2}$ $=$$(2)^{\log_2(\log_2 n)^{-1}}\frac{1}{2}$ $=$$(\log_2 n)^{-1}\frac{1}{2}$ $=$$\frac{1}{2\log_2 n}$.Therefore, it fails at most $\frac{1}{2\log_2 n}$ probability.As a result, this algorithm success to find a semicoloring in every iteration at least $(1 - \frac{1}{2\log_2 n})^{\log_2 n}$ $\ge$ $\frac{1}{2}$ probability if $n$ $\ge$ $2$.Proof is like follow.Notice that if we change $log_2 x$ to $t$ then $(1 - \frac{1}{2\log_2 n})^{\log_2 n}$ $=$ $(1 - \frac{1}{2t})^{t}$.Now it can be derivated easily about $t$ and result will be $(1 - \frac{1}{2t})(\ln(1 - \frac{1}{2t}) + \frac{1}{2t - 1})$.Notice that $t \ge 1$.Then, $1 - \frac{1}{2t} \ge 0$ because $t \ge 1$.For the other part, $\ln(1 - \frac{1}{2t})$ $+$ $\frac{1}{2t - 1}$ $=$ $\ln(\frac{2t - 1}{2t})$ $+$ $\frac{1}{2t - 1}$ $=$ $\ln(\frac{2t - 1}{2t})$ $+$ $\frac{2t}{2t - 1}$ $-$ $\frac{2t - 1}{2t - 1}$ $=$$\ln(\frac{2t - 1}{2t})$ $+$ $\frac{2t}{2t - 1}$ $-$ $1$ $=$$\ln X$ $+$ $\frac{1}{X}$ $-$ $1$.for $X$ $=$ $\frac{2t -1}{2t}$.Notice that $0$ $&lt;$ $X$ $&lt;$ $1$.Then, $\ln X$ $+$ $\frac{1}{X}$ $-$ $1$ $\ge$ $0$.Proof is like follow.If we derivated $y$ $=$ $\ln X$ $+$ $\frac{1}{X}$ $-$ $1$, $y’$ $=$ $\frac{1}{X}$ $-$ $\frac{1}{X^2}$ $=$ $\frac{X - 1}{X^2}$.As a result, $y$ decreases in $0$ $&lt;$ $X$ $&lt;$ $1$.However, it’s zero when $X$ $=$ $1$.Therefore, $\ln X$ $+$ $\frac{1}{X}$ $-$ $1$ $\ge$ $0$.As a result, this algorithm success to find a semicoloring in every iteration at least $\frac{1}{2}$ probability if $n$ $\ge$ $2$.Now, it doesn’t have too much meaning if you think about $n$ $=$ $1$.Therefore, assumption isn’t too tight.Now, let’s see how many vertices are removed at each iteration.If we start with $n’$ vertices, $\left\vert R \right\vert$ $\le$ $2 \frac{n’}{4}$ $=$ $\frac{n’}{2}$.As a result, it will reduce to be half at least.Therefore, there will be at most $O(1)$ vertices after part 2.Notice that we will do part 2 at most $log_2 n$ time and it will be enough to be so.As a result color used in each part is like follow.  Part 1 uses at most $3([\frac{n}{\sigma}] + 1)$ $=$$3([\frac{n}{n^{\log_6 3}}] + 1)$ $\le$$3(\frac{n}{n^{\log_6 3}} + 1)$ $=$$3(\frac{n^{\log_6 6}}{n^{\log_6 3}} + 1)$ $=$$3(n^{\log_6 2} + 1)$ colors.  Part 2 uses at most $4\Delta^{\log_3 2}(\log_2 n)$ $\le$$4\sigma^{\log_3 2}(\log_2 n)$ $=$$4(n^{\log_6 3})^{\log_3 2}(\log_2 n)$ $=$$4n^{\log_6 3 \cdot \log_3 2}(\log_2 n)$ $=$$4n^{\log_6 2}(\log_2 n)$ colors.  Left part will use at most $O(1)$ colors.As a result, algorithm uses at most $3n^{\log_6 2}$ $+$ $3$ $+$ $4n^{\log_6 2}(\log_2 n)$ $+$ $O(1)$.Therefore, claim holds.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Markov's inequality]]></title>
      <url>/algorithm/approximation/2021/05/14/Markovs-inequality/</url>
      <content type="text"><![CDATA[If $X$ is a nonnegative random variable and $a \gt 0$ then,$Pr[X \ge a]$ $\le$ $\frac{E(X)}{a}$.Proof is like follow.First of all, $E(X | X \ge a)$ $\ge$ $a$ because condition means that varaible is greater or equal than $a$.Simillary, $E(X | X \lt a)$ $\ge$ $0$ because $X$ is a nonnegative random variable.Then, $E(X)$ $=$ $Pr[X \lt a] \cdot E(X | X \lt a)$ $+$ $Pr[X \ge a] \cdot E(X | X \ge a)$ $\ge$ $Pr[X \ge a] \cdot E(X | X \ge a)$ $\ge$ $Pr[X \ge a] \cdot a$.Therefore, claim holds]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Semidefinite programming]]></title>
      <url>/algorithm/approximation/2021/05/12/Semidefinite-programming/</url>
      <content type="text"><![CDATA[To show what is semidefinite programming, we need to know what is semidefinite matrix.Notice that $x$ $=$ $\mathcal{R}^{n}$ $=$ $\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}$.  $\mathcal{S}_n$ is the set of $n$ by $n$ symmetric matrix.  We call symmetric matrix $X$ $\in$ $\mathcal{S}_n$ is positive semidefinite if for all $x \in \mathcal{R}^{n}$, $x^TXx \ge 0$.  For $A,B$ $\in$ $\mathcal{S}_n$, inner product of $A,B$ is defined as $&lt;A,B&gt; = tr(A^T B)$Followings are equivalent for $X$ $\in$ $\mathcal{S}_n$.  $X$ is positive semidefinitive  All eigenvalues of $X$ are non negative  $X$ $=$ $V^TV$ for some $V$ $\in$ $R^{n \times n}$  $X$ $=$ $\sum\limits_{i = 1}^{n} \lambda_i w_i w_i^T$ for some $\lambda_i \ge 0$ and vector $w_i$ $\in$ $R^n$ such that $w_i^T w_i = 1$ and $w_i^T w_j = 0$ for all $i \neq j$A semifinite program is a mathematical program where the varaibles form a symmetric matrix and the objective function and constraints are all linear.Notice that this program is just a linear programming but variables and constraints are semidefinitive matrices.In other word, it’s like follow.Semidefinite programmingMinimize or maximize $\sum\limits_{i,j} c_{ij} x_{ij}$such that $\sum\limits_{i,j} a_{ijk_1} x_{ij}$ $=$ $b_{k_1}$ $\forall k_1$$\sum\limits_{i,j} a_{ijk_2} x_{ij}$ $\ge$ $b_{k_2}$ $\forall k_2$$\sum\limits_{i,j} a_{ijk_3} x_{ij}$ $\le$ $b_{k_3}$ $\forall k_3$$X$ $=$ $\begin{pmatrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1n} \\ x_{21} = x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2n} = x_{n2} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ x_{n1} = x_{1n} &amp; x_{n2} = x_{2n} &amp; \cdots &amp; x_{nn} \end{pmatrix}$ is semidefinitive.Notice that if we think about any semidefinite matrix $A$ and $B$,$x^TXx \ge 0$ and $x^TYx \ge 0$ for all $x \in \mathcal{R}^{n}$.As a result, $x^T(\lambda X + (1-\lambda)Y)x \ge 0$ for $0$ $\le$ $\lambda$ $\le$ $1$.Which means set of semidefinitive matrix is a convex.Therefore, we may solve semifefinitive program in a polynomial time.The program above is equivalent with the program below if $X = V^TV$.Notice that there is such an $V$ because $X$ is positive semidefinitive.Notice that If $V$ $=$ $\begin{pmatrix} v_1, v_2, \cdots, v_n\end{pmatrix}$ then $X$ $=$ $\begin{pmatrix} &lt;v_1, v_1&gt; &amp; &lt;v_1, v_2&gt; &amp; \cdots \\ \vdots &amp; \ddots &amp; \vdots \\ \cdots &amp; \cdots &amp; &lt;v_n, v_n&gt; \end{pmatrix}$.This program is so-called vector program because it has vector as variables.Vector programMinimize or maximize $\sum\limits_{i,j} c_{ij} &lt;v_i, v_j&gt;$such that $\sum\limits_{i,j} a_{ijk_1} &lt;v_i, v_j&gt;$ $=$ $b_{k_1}$ $\forall k_1$$\sum\limits_{i,j} a_{ijk_2} &lt;v_i, v_j&gt;$ $\ge$ $b_{k_2}$ $\forall k_2$$\sum\limits_{i,j} a_{ijk_3} &lt;v_i, v_j&gt;$ $\le$ $b_{k_3}$ $\forall k_3$Notice that number of vetices are the same with dimension of vector.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Approximation algorithm(12) - The uncapacitated facility location problem(2)]]></title>
      <url>/algorithm/approximation/2021/05/07/Approximation-algorithm(12)/</url>
      <content type="text"><![CDATA[Now, we will re visit the uncapacitated facility location problem.Before we proceed new methodology for the uncapacitated facility location problem, we can extend some terminology for assignment costs between clients or facilities either.For example, assignment costs between client $i$ and $j$ can be the shortest path between client $i$ and $j$.If we define this as so, it is known to reserve the solution because of the metric closure.Now, let’s recap the problem itself.Minimize $\sum\limits_{i \in F}f_iy_i + \sum\limits_{i \in F, j \in D} c_{ij}x_{ij}$ such that$\sum\limits_{i \in F}x_{ij} = 1$ $\forall j \in D$, $x_{ij} \le y_i$ $\forall i \in F, j \in D$ and $x_{ij}, y_i \in \{0, 1\}$ $\forall i \in F, j \in D$.Then, we can do LP-relaxation below.Minimize $\sum\limits_{i \in F}f_iy_i + \sum\limits_{i \in F, j \in D} c_{ij}x_{ij}$ such that $\sum\limits_{i \in F}x_{ij} = 1$ $\forall j \in D$, $x_{ij} \le y_i$ $\forall i \in F, j \in D$ and $x_{ij}, y_i \ge 0$ $\forall i \in F, j \in D$.With above, LP dual is like follow.Maximize $\sum\limits_{j \in D} v_j$ such that $w_{ij} \ge 0$ $\forall i \in F, j \in D$,$\sum\limits_{j \in D}w_{ij} \le f_i$ $\forall i \in F$,$v_j - w_{ij} \le c_{ij}$ $\forall i \in F, j \in D$.Now let’s define some terminologies for a given dual solution $v^{\star}, w^{\star}$.  Client $j$ is neighbor of facility $i$ if $v_{j}^{\star}$ $\ge$ $c_{ij}$.With that let denotes $N(i)$ as the set of neighbors of $i$.  Client $j$ contributes to $i$ if $w_{ij}^{\star} &gt; 0$.Now, let’s desing the algorithm.    $v \leftarrow 0, w \leftarrow 0$    $S \leftarrow D$ // Left clients to be allocated    $T \leftarrow \emptyset$ // Facility which are open    $\operatorname{while}$ $S$ $\neq$ $\emptyset$        Increase $v_j$ for all $j \in S$ and $w_{ij}$ for all $i$ $\in$ $N(j)$, $j$ $\in$ $S$ uniformly until some $j$ $\in$ $S$ neighbors some $i$ $\in$ $T$ or some $i$ $\not\in$ $T$ has a tight dual inequality        $\operatorname{if}$ some $j$ $\in$ $S$ neighbors some $i$ $\in$ $T$ // $j$ paid enough to assigned to facility $i$            $S \leftarrow S - \{j\}$                $\operatorname{if}$ some $i$ $\not\in$ $T$ has a tight dual inequality // $i$'s funding collected enough to be opened            $T \leftarrow T \cup \{i\}$            $S \leftarrow S - N(i)$                $T' \leftarrow \emptyset$ // $T'$ is the set of facility to be open    $\operatorname{while}$ $T$ $\neq$ $\emptyset$        Pick $i$ $\in$ $T$        $T' \leftarrow T' \cup \{i\}$        $T \leftarrow T - \{h \in T : \exists j \in D, w_{ij} &gt; 0 \text{ and } w_{hj} &gt; 0\}$    Now, if $j$ doesn’t have a neighbor in $T’$ then there exists $i \in T’$ such that $c_{ij} \le 3v_j$ if we work with algorithm above.Let’s think about $T$ at the end of “$\operatorname{while}$ $S$ $\neq$ $\emptyset$” and $T’$ at the end of the algorithm.Then, all $j$’s neighbors will be excluded in $T$ at the loop of “$\operatorname{while}$ $T$ $\neq$ $\emptyset$” to be not exists in $T’$.Let denotes $h$ as the neighbor of $j$ which $v_j$ stops to increase.Notice that $h$ is not in $T’$ and $h$ should be a neighbor of $j$.Then, there exists some $k$ that contributes $h$ and $i$ $\in$ $T’$ for all $h$.First of all, if $j$ contributes to $i$ then $j$ is neighbor of $i$.If we think about the algorithm itself, we increase $w_{ij}$ only if $i$ $\in$ $N(j)$.As a result, $k$ need to be a neighbor of both $h$ and $i$.At the same time $j$ is a neighbor of $h$. Now, $c_{ij}$ $\le$ $c_{ik}$ $+$ $c_{hk}$ $+$ $c_{hj}$ from the triangle inequality.Then, $c_{ik}$ $+$ $c_{hk}$ $+$ $c_{hj}$ $\le$ $v_{k}$ $+$ $v_{k}$ $+$ $v_{j}$ because each of them are in the neighbor relation.With above, $v_k$ $\le$ $v_j$ is true.Proof is like follow.Let’s assume not.$v_k, w_{hk}$ will stop increasing at the moment of between “$k$ neighbors $h$ $\in$ $T$” or “$h$ $\not\in$ $T$ become tight and $k$ neighbors $h$”.However we should select second case because $k$ can increase $w_{hk}$ only after $k$ neighbors $h$.At the same time, $v_j$ will stop increasing at the moment of between “$j$ neighbors $h$ $\in$ $T$” or “$j$ $\not\in$ $T$ become tight and $k$ neighbors $h$”.In the first case, $h$ $\in$ $T$ already but $k$ stoped increasing $v_k$ when $h$ become tight therefore $v_k$ $&lt;$ $v_j$.In the second case, both $v_j$ and $v_k$ stop increasing at the same time. As a result $v_k$ $=$ $v_j$.Therefore claim holds.Now, we can prove that this algorithm is a 3-approximation algorithm.Even though algorithm increase uniformly, we can just pick the smallest delta value of constraints to run this algorithm in polynomial time.With this, all other parts of algorithm are polynomial.Therefore, this algorithm runs in a polynomial time.Now, let’s think about the solution that assigning $i$ to $j$ which $i$ that contributes to $j$ and assigning $i$ that doesn’t contributes to the facility in $T’$ to facility $j$ in $T’$ such that maximizes $v_j$.Notice that from the algorithm we forced it to contributes to only one facility or none of facility.Let’s define $A(i)$ $\subseteq$ $N(i)$ as the set of assigned clients to faciltiy $i$ which clients neighbors $i$ $\in$ $T$, $Z$ as the set of clients that doesn’t neighbors any $j$ $\in$ $T$ and $X(j)$ $\in$ $T’$ as the facility that client $j$ assigned for $j$ such that contributes none of $T’$.Notice that all of clients that contributes none of $T’$ will not be in $A(i)$ because it should neighbor of some in $T’$ if it contributes.At the same time, all of clients that contributes some of $T’$ will be in $A(i)$ because it should neighbor of some in $T’$.Then, total cost will be $\sum\limits_{i \in T’}(f_i + \sum\limits_{j \in A(i)}c_{ij})$ $+$ $\sum\limits_{j \in Z}c_{X(j)j}$.Then, $\sum\limits_{i \in T’}(f_i + \sum\limits_{j \in A(i)}c_{ij})$ $=$ $\sum\limits_{i \in T’}\sum\limits_{j \in A(i)}(w_{ij} + c_{ij})$ $=$ $\sum\limits_{i \in T’}\sum\limits_{j \in A(i)}v_{j}$.Notice that first equality comes from that $i$ $\in$ $T’$ $\subseteq$ $T$ and $T$ is the set of facility that become tight.At the same time, $w_{ij}$ $+$ $c_{ij}$ $=$ $v_j$ because $j$ is tight.Notice that $j$ is tight because $j$ will contributes to only one facility and $j$ will be assigned to there.Now, $\sum\limits_{j \in Z}c_{X(j)j}$ $\le$ $3\sum\limits_{j \in Z}v_{X(j)}$ if we pick $X(i)$ as the maximum possible one for $v_j$.Notice that we proved that “if $j$ doesn’t have a neighbor in $T’$ then there exists $i \in T’$ such that $c_{ij} \le 3v_j$”.Therefore, claim holds.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Approximation algorithm(11) - Generalized Steiner tree problem]]></title>
      <url>/algorithm/approximation/2021/05/01/Approximation-algorithm(11)/</url>
      <content type="text"><![CDATA[There is a problem known as a steiner tree problem.The steiner tree problem is a problem to find a minimum edge set that connects every vertex in $S \subseteq V$ from a given graph $G = (V,E)$.Notice that this problem doesn’t care whether you included some vertices $v \in V - S$ in the edge set.Moreover, this problem is a superset of the minimum spanning tree problem and shortest path problem.We can construct problem like $S = V$ or $S = \{s, t\}$ to solve the minimum spanning tree problem and shortest path problem repectively.Notice that this problem will result in a tree because it will not have a cycle which doesn’t make sense for minimum edge set with connectivity.Generalized steiner tree problem is a problem that requires to connect some pairs of vertices of given graph $G$.If we select one vertex $v$ and set required pairs as $v$ and others, it’s the steiner tree problem.Which means Generalized steiner tree problem is a generalized version of steiner tree problem like it named.At the same time, it’s a special case of survivable network design problem which all $r_{ij} = 1$.Now let’s define $\delta(S) = \{e | e = (u,v) \text{ such that } u \in S, v \in V - S \}$ and $S_i = \{S \subseteq V : \left\vert S \cap \{s_i, t_i\} \right\vert = 1\}$.Notice that $S_i$ is a set of subsets that make a proper s-t cut for $s_i, t_i$.Therefore, problem can be written as LP follow.For given pairs of vetices $s_i, t_i$,minimize $\sum\limits_{e \in E}c_e x_e$ such that$\sum\limits_{e \in \delta(S)} x_e \ge 1$, for all $S \subseteq V$ which $S \in S_i$ for all $i$, $x_e \in \{0, 1\}$.Now IP above can be relaxed to LP below.For given pairs of vetices $s_i, t_i$,minimize $\sum\limits_{e \in E}c_e x_e$ such that$\sum\limits_{e \in \delta(S)} x_e \ge 1$, for all $S \subseteq V$ which $S \in S_i$ for all $i$, $x_e \ge 0$.Now let’s make a dual of primal like follow.Maximize $\sum\limits_{e \in \delta(S), S \in S_i, \forall i}y_S$ such that$\sum\limits_{S : e \in \delta(S), S \in S_i \forall i} y_S \le c_e$ for all $e \in E$$y_S \ge 0$.Now, let’s think it as following for intuition.$v \in V$ will be spread at the space with edge distance $c_e$.For any $S \subseteq V$, we can think about some moats surrounding that $S$.Now, let’s think a width of moat for $S$ as $y_S$.Then it can be thought as maximizing moat’s width.With constraint that moats can’t overlapped for subsets exist at the same edge.Then we can try algorithm follow.    $y \leftarrow 0$    $F \leftarrow \emptyset$    $\operatorname{while}$ not all $s_i-t_i$ pairs are connected in $(V,F)$        Let $C$ be a connected component of $(V,F)$ such that $\left\vert C \cap \{s_i, t_i\} \right\vert = 1$ for some $i$        Increase $y_C$ until there is an edge $e \in \delta(C)$ such that $c_{e}$ $=$ $\sum\limits_{S:e \in \delta(S)}y_S$        $F \leftarrow F \cup \{e\}$        return $F$Algorithm above works like choose any componenet such that are not tight then increase moat distance as possible as.Then, solution of algorithm is $\sum\limits_{e \in F}c_e$.However, $\sum\limits_{e \in F}c_e$ $=$ $\sum\limits_{e \in F}\sum\limits_{S:e \in \delta(S)} y_S$ $=$ $\sum\limits_{S}\sum\limits_{e \in F \cap \delta(S)} y_S$ $=$ $\sum\limits_{S}|F \cap \delta(S)|y_S$.However, algorithm above can’t guarantees to be nice approximation algorithm.Let’s think about the complete graph $G = (\{s,t_1,t_2,t_3,\cdots,t_n\},E)$ with $C_{(s,t_1)} = C_{(s,t_2)} = \cdots = C_{(s,t_n)} &gt; 0$.If pairs need to be connected is $(s,t_1), (s,t_2), \cdots, (s,t_n)$ then $\sum\limits_{S}|F \cap \delta(S)|y_S$ = $\sum\limits_{S}ny_S$.Which means this is $O(n)OPT$ approximation.However it’s not good enough even though the solution we will give a good solution.Therefore, we will modify algorithm above to below.    $y \leftarrow 0$    $l \leftarrow 0$    $F \leftarrow \emptyset$    $\operatorname{while}$ not all $s_i-t_i$ pairs are connected in $(V,F)$        $l \leftarrow l + 1$        Let $\mathcal{C}$ be the set of all connected components $C$ of $(V,F)$ such that $\left\vert C \cap \{s_i, t_i\} \right\vert = 1$ for some $i$        Increase $y_C$ for all $C \in \mathcal{C}$ uniformly until $c_{e_l}$ $=$ $\sum\limits_{S:e_l \in \delta(S)}y_s$ for some $e_l \in \delta(C)$        $F \leftarrow F \cup \{e_l\}$        $F' \leftarrow F$    $\operatorname{for} k \leftarrow l,l-1,\cdots,1$        $\operatorname{if}$ $F' - e_k$ is a feasible solution then            Remove $e_k$ from $F'$                return $F'$Notice that algorithm is a polynomial algorithm.Finding connected components is polynomial algorithm by BFS.Increasing $y_C$ is polynomial algorithm because we can pick some $\Delta y_C$ to be tight.Then it is enough to pick minimum of such an $\Delta y_C$ for uniform increasement.First of all, $\sum\limits_{C \in \mathcal{C}} \left\vert \delta(C) \cap F’ \right\vert$ $\le$ $2 \left\vert \mathcal{C} \right\vert$.Proof is like follow.Let $F_i = \{e_1,e_2,\cdots,e_{i-1}\}$, $H_i = F’ - F_i$.Notice that $F_i \cup H_i = F_i \cup F’$ is a feasible soltuion.Also, if we remove any edge from $H_i$ then $F_i \cup H_i$ will be a non-feasible soluiton.Notice that we only removed edges that doesn’t change feasibility on the progress.With above, $F_i$ is a foreset for all $i$ because algorithm will increase only $y_C$ such that $C \in \mathcal{C}$.However, two end points of $y_C$ need to in the seperate componenets to be that. Because $\mathcal{C}$ is the set of all connected components $C$ of $(V,F)$ such that $\left\vert C \cap {s_i, t_i} \right\vert = 1$ for some $i$ that are not connected.As a result, $F_i$ is a forest.Now let’s think about a super graph $G_i$ that changes every connected componenets of $(V, F_i)$ to a single vertex.Then $G_i$ is a forest because $G$ was a forest.With that, no edge in $H_i$ can’t have both end points in one connected component because it will make a cycle.Therefore, $G_i$ will not have a loop either.Now, let $V_i$ as the vertex set of $G_i$ and degree of that vertex to $\operatorname{deg}(v)$ for $v \in V_i$.With that, let $v \in V_i$ is active if $\left\vert C \cap \{s_j, t_j\}\right\vert$ $=$ $1$ for some $j$.Then, let $A_i$ as the set of active components in $V_i$.Now, let’s go back to the “$\sum\limits_{C \in \mathcal{C}} \left\vert \delta(C) \cap F’ \right\vert$ $\le$ $2 \left\vert \mathcal{C} \right\vert$”.Then, $\mathcal{C}$ should be active because we pick “$\mathcal{C}$ be the set of all connected components $C$ of $(V,F)$ such that $\left\vert C \cap {s_i, t_i} \right\vert = 1$ for some $i$”.Therefore, RHS is $2 \left\vert A_i \right\vert$.Also, no edge in $F_i$ can be in $\delta(C)$ for $C \in \mathcal{C}$ because $F’ \subseteq F_i \cup H_i$.Notice that even we used $\mathcal{C}$ but it’s $\mathcal{C}$ at the $i$ th iteration and $C$ is a connected componenet of $F_i$.Therefore, $\sum\limits_{C \in \mathcal{C}} \left\vert \delta(C) \cap F’ \right\vert$ = $\sum\limits_{C \in \mathcal{C}} \left\vert \delta(C) \cap H_i’ \right\vert$ $=$ $\sum\limits_{v \in A_i} deg(v)$ because there is no edge to be picked in side of ecah connected component.Therefore, showing $\sum\limits_{v \in A_i} deg(v)$ $\le$ $2 \left\vert A_i \right\vert$ is enough.Proof is like follow.First, no $v \in V_i - A_i$ has degree one.If there is any $v \in V_i - A_i$ has degree one, let $e = (v, u)$.However, it means $e$ was not removed because it is only degree that connects to the outside of the connected componenet.Which means $e$ was necessary for feasiblity.Which means $|C \cap \{s_j, t_j\}| = 1$ for some $j$ and $C$.Notice that $C$ is $v$ in $G_i$ in this case.However, this is contradict with $v \not\in A_i$.Let $B_i$ $=$ $\{v \in V_i - A_i \vert deg(v) &gt; 0\}$.Then, $\sum\limits_{v \in A_i} deg(v)$ $=$ $\sum\limits_{v \in A_i \cup B_i} deg(v)$ $-$ $\sum\limits_{v \in B_i} deg(v)$.Notice that $A_i \cap B_i = \emptyset$.Then $\sum\limits_{v \in A_i \cup B_i} deg(v)$ $-$ $\sum\limits_{v \in B_i} deg(v)$ $\le$ $2(\left\vert A \right\vert + \left\vert B \right\vert)$ $-$ $2\left\vert B \right\vert$.Notice that this graph is a forest and therefore sum of degree can’t bigger than twice of the size of the set because there are less edge than number of vertices.$\sum\limits_{v \in A_i} deg(v)$ $=$ $\sum\limits_{v \in A_i \cup B_i} deg(v)$ $-$ $\sum\limits_{v \in B_i} deg(v)$ $\le$ $2(\left\vert A_i \right\vert + \left\vert B_i \right\vert)$ $-$ $2\left\vert B_i \right\vert$ $=$ $2\left\vert A_i \right\vert$.As a result, claim holds.Now this is a 2-approximation algorithm.Proof is like follow.We will show that $\sum\limits_{S} \left\vert F’ \cap \delta(S) \right\vert y_S$ $\le$ $2\sum\limits_{S}y_S$ at the beginning and end of the iteration of $\operatorname{while}$ loop.First of all, $0$ $=$ $\sum\limits_{S} \left\vert F’ \cap \delta(S) \right\vert y_S$ $\le$ $2\sum\limits_{S}y_S$ $=$ $0$ at the first iteration.Now, let’s assume that it works for $l - 1$ iterations.Now let’s assume that $\epsilon$ of $y_C$ increased at $l$th iteration. Then, LHS of given inequality will incrase $\sum\limits_{C \in \mathcal{C}} \left\vert F’ \cap \delta(C) \right\vert \epsilon$.RHS of given inequality will incrase $2\sum\limits_{\mathcal{C}}\epsilon$.Then, LHS $\le$ RHS is true from the fact “$\sum\limits_{C \in \mathcal{C}} \left\vert \delta(C) \cap F’ \right\vert$ $\le$ $2 \left\vert \mathcal{C} \right\vert$”.As a result, claim holds.Notice that $\sum\limits_{S}y_S$ is a LP optimum which is less than optimal solution of original problem.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Approximation algorithm(10) - The uncapacitated facility location problem(1)]]></title>
      <url>/algorithm/approximation/2021/04/25/Approximation-algorithm(10)/</url>
      <content type="text"><![CDATA[The uncapacitated facility location problem is a problem to decide which facility to open and how to handle clients for it.Notice that every client will just go to the closest facility because there is no capacity limits.Now, let’s denote a given set of facility as $F$, clients as $D$.There are some costs for opening facility $f_i \ge 0$ $\forall i \in F$.With this costs, there are some assignment costs $c_{ij} \ge 0$ $\forall i \in F, j \in D$ such that triangular inequality holds.Which means $C_{ij}$ $\le$ $C_{il} + C_{kl} + C_{kj}$.This means cost of “assigning $j$ to $i$” is allways less or equal than cost of “assigning $l$ to $i$, $l$ to $k$ and $j$ to $k$”.It is not too hard constraint because $j$ can recieves item at $k$ delivered by $l$ from $i$ if it is.Now, problem is to minimize sum of costs to assign all clients to some facility which are open.Then, Problem can be written as IP follow.Minimize $\sum\limits_{i \in F}f_iy_i + \sum\limits_{i \in F, j \in D} c_{ij}x_{ij}$ such that$\sum\limits_{i \in F}x_{ij} = 1$ $\forall j \in D$, $x_{ij} \le y_i$ $\forall i \in F, j \in D$ and $x_{ij}, y_i \in \{0, 1\}$ $\forall i \in F, j \in D$.Notice that there is a problem similar with above but with some capacity for each facility.In such a case, the problem is so-called “The capacitated facility location problem”.Now IP above can be LP-relaxed to below.Minimize $\sum\limits_{i \in F}f_iy_i + \sum\limits_{i \in F, j \in D} c_{ij}x_{ij}$ such that $\sum\limits_{i \in F}x_{ij} = 1$ $\forall j \in D$, $x_{ij} \le y_i$ $\forall i \in F, j \in D$ and $x_{ij}, y_i \ge 0$ $\forall i \in F, j \in D$.Notice that LP above is the same with LP below.Minimize $\sum\limits_{i \in F}f_iy_i + \sum\limits_{i \in F, j \in D} c_{ij}x_{ij}$ such that $\sum\limits_{i \in F}x_{ij} = 1$ $\forall j \in D$, $y_i - x_{ij} \ge 0$ $\forall i \in F, j \in D$ and $x_{ij}, y_i \ge 0$ $\forall i \in F, j \in D$.Now let’s use $v_i, w_{ij}$ as multiplier for the first constraints and the second constraints respectively.we can change primal LP above to dual of it.Maximize $\sum\limits_{j \in D} v_j$ such that $w_{ij} \ge 0$ $\forall i \in F, j \in D$,$\sum\limits_{j \in D}w_{ij} \le f_i$ $\forall i \in F$,$v_j - w_{ij} \le c_{ij}$ $\forall i \in F, j \in D$.Notice that LP above is the same with LP below.Maximize $\sum\limits_{j \in D} v_j$ such that $w_{ij} \ge 0$ $\forall i \in F, j \in D$,$\sum\limits_{j \in D}w_{ij} \le f_i$ $\forall i \in F$,$v_j \le c_{ij} + w_{ij} $ $\forall i \in F, j \in D$.Now, we can think it as like follow.For each clients, they need to pay some portion of opening cost for a facility they want to go.Therefore, let $w_{ij}$ as client $j$’s payment for the portion of opening cost for facility $i$.With them, they need to pay costs for assigning them to a facility.As a result, $v_j$ is a payment which client $j$ wii do.Now, problem is like finding the maximum payment for clients.However, they will pay more than they need to but less possible as.Also, they need to pay at least the same costs for opening facility.Now, let $(x^{\star}, y^{\star}), (v^{\star}, w^{\star})$ be an optimumm of primal and dual respectively.Then define client $j$ is neighbor of facility $i$ if $x_{ij}^{\star} &gt; 0$.Therefore, let $N(j) = \{i \in F, x_{ij}^{\star} &gt; 0\}$.With this, we can define distance 2 neighborhood of $j$ like follow.$N^2(j) = \{i \in D, x_{kj}^{\star} &gt; 0, x_{ki}^{\star} &gt; 0\}$.Now, if $x_{ij}^{\star} &gt; 0$ then $c_{ij} \le v_j^{\star}$.Proof is like follow.By complementary slackness, $x_{ij}^{\star} &gt; 0$ means $v_{j}^{\star} - w_{ij}^{\star} = c_{ij}$.Therefore, $v_j^{\star} \ge c_{ij}$.Now, let $i_{k}^{\star}$ be the facility $i$ with the smallest $f_i$ in $N(j_{k})$ for $j_{k} \in D$.Then $f_{i_{k}^\star}$ $=$ $f_{i_{k}^\star}\sum\limits_{i \in N(j_{k})}x_{ij_{k}}^{\star}$ $\le$ $\sum\limits_{i \in N(j_{k})}f_{i}x_{ij_{k}}^{\star}$. Notice that $\sum\limits_{i \in N(j_{k})}x_{ij_{k}}^{\star}$ $=$ $1$, $f_{i_{k}^\star}$ $\le$ $f_i$ for $i \in N(j_{k})$.Then, $f_{i_{k}^\star}$ $\le$ $\sum\limits_{i \in N(j_{k})}f_{i}x_{ij_{k}}^{\star}$ $\le$ $\sum\limits_{i \in N(j_{k})}f_{i}y_{i}^{\star}$ from the constraint $x_{ij} \le y_i$.Now, here is an approximation algorithm follow.    Solve primal and dual LP and get optimum $(x^{\star}, y^{\star})$, dual optimum $(v^{\star}, w^{\star})$    $C \leftarrow D$    $k \leftarrow 0$    $\operatorname{while} C \neq \emptyset$        $k \leftarrow k + 1$        Choose $j_k \in C$ that minimizes $v_j^{\star}$ over all $j \in C$        Choose $i_k \in N(j_k)$ to be the cheapest facility in $N(j_k)$        Open $i_k$ and assign $j_k$ and all unassigned clients in $N^2(j_k)$ to $i_k$        $C \leftarrow C - \{j_k\}  - N^2(j_k)$    Then, the algorithm given is a 4-approximation algorithm.Proof is like follow.Now, let’s define $S_k$ as the set of assigned clients at the $k$th ieration and assume that algorithm terminates in $K$ iteration.Then, $\sum\limits_{k = 1}^{K} f_{i_k}$ $\le$ $\sum\limits_{k = 1}^{K}\sum\limits_{i \in N(j_k)} f_{i_k}y_i^{\star}$ $\le$ $\sum\limits_{i \in F} f_{i_k}y_i^{\star}$. Proof is like follow.First inequality is what just we’ve showen.For the second inequality, if we read about the algorithm, $N(j_{k_1}) \cap N(j_{k_2}) = \emptyset$ because we will assign all the $j$ in $N^2(j_k)$ at the each iteration. Therefore, claim holds.Now, $c_{i_kl}$ $\le$ $c_{i_kj_k} + c_{hj_k} + c_{hl}$ for $l \in N^2(j_k)$ and any $h \in N(j_k) \cap N(l)$ because of the triangular inequality. Then, $c_{i_kl}$ $\le$ $c_{i_kj_k} + c_{hj_k} + c_{hl}$ $\le$ $v_{j_k}^{\star} + v_{j_k}^{\star} + v_l^{\star}$ because “$x_{ij}^{\star} &gt; 0$ then $c_{ij}$ $\le$ $v_j^{\star}$”.Notice that $S_k \subseteq N^2(j_k)$ and $x_{i_kj_k} &gt; 0$ is true.First one is trivial.Second one is true because of following.  $x_{i_kj_k} &gt; 0$  because $i_k \in N(j_k)$  $x_{hj_k} &gt; 0$ because $h \in (N(j_k) \cap N(l)) \subseteq N(j_k)$  $x_{hl} &gt; 0$ because $h \in (N(j_k) \cap N(l)) \subseteq N(l)$If we think about $v_l^{\star}$, $v_{j_k}^{\star}$ $\le$ $v_l^{\star}$ because we’ve choosed the minimum of $v_j^{\star}$ at each iteration.As a result, $c_{i_kl}$ $\le$ $3v_l^{\star}$.Therefore, $\sum\limits_{k = 1}^{K}\sum\limits_{l \in S_k}c_{i_kl}$ $\le$ $\sum\limits_{k = 1}^{K}\sum\limits_{l \in S_k}3v_l^{\star}$ $\le$$\sum\limits_{l \in D}3v_l^{\star}$.As a result, $\sum\limits_{k = 1}^{K} f_{i_k}$ $+$ $\sum\limits_{k = 1}^{K}\sum\limits_{l \in S_k}c_{i_kl}$ $\le$ $\sum\limits_{i \in F} f_{i_k}y_i^{\star}$ $+$ $\sum\limits_{l \in D}3v_l^{\star}$ $\le$ $\sum\limits_{i \in F} f_{i_k}y_i^{\star}$ $+$ $\sum\limits_{i \in F, j \in D} c_{ij}x_{ij}^{\star}$ $+$ $\sum\limits_{l \in D}3v_l^{\star}$ $\le$ $\operatorname{OPT}$ + $3\operatorname{OPT}$ $=$ $4\operatorname{OPT}$.Notice that $\sum\limits_{i \in F} f_{i_k}y_i^{\star}$ $+$ $\sum\limits_{i \in F, j \in D} c_{ij}x_{ij}^{\star}$ is an optimum of primal and$\sum\limits_{l \in D}v_l^{\star}$ is an optimum of dual.Therefore both are less or equal than $\operatorname{OPT}$.Now, we know that the number of constraints are polynomial.Therefore, primal and dual of LP can be solved in polynomial time.Algorithm iterates at most $|D|$ iterations.Therefore, it will terminates in polynomial time.Proof stops here.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[LP Duality]]></title>
      <url>/algorithm/lp/2021/04/20/LP-Duality/</url>
      <content type="text"><![CDATA[For any given LP, we can construct LP like one of the format follow.Minimization problemFor given vectors$X = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}$, $C = \begin{pmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{pmatrix}$,$B = \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{pmatrix}$ and matrix $A = \begin{pmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \end{pmatrix}$ $=$ $\begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_m \end{pmatrix}$.Minimize $C^T X$ such that     $a_i X \ge b_i$ for $i \in C_p$,    $a_i X \le b_i$ for $i \in C_m$,    $a_i X = b_i$ for $i \in C_e$,    $x_i \ge 0$ for $i \in B_p$,    $x_i \le 0$ for $i \in B_m$,    No limitation $x_i$ for $i \in B_f$.Notice that $C_p \cup C_m \cup C_e = \{1, 2, \cdots, m\}$, $C_p \cap C_m$ $=$ $C_m \cap C_e$ $=$ $C_p \cap C_e$ $=$ $\emptyset$, $B_p \cup B_m \cup B_f = \{1, 2, \cdots, n\}$, $B_p \cap B_m$ $=$ $B_m \cap B_f$ $=$ $B_p \cap B_f$ $=$ $\emptyset$Maximization problemFor given vectors$X = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}$, $C = \begin{pmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{pmatrix}$,$B = \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{pmatrix}$ and matrix $A = \begin{pmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \end{pmatrix}$ $=$ $\begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_m \end{pmatrix}$.Maximize $C^T X$ such that     $a_i X \ge b_i$ for $i \in C_p$,    $a_i X \le b_i$ for $i \in C_m$,    $a_i X = b_i$ for $i \in C_e$,    $x_i \ge 0$ for $i \in B_p$,    $x_i \le 0$ for $i \in B_m$,    No limitation $x_i$ for $i \in B_f$.Notice that $C_p \cup C_m \cup C_e = \{1, 2, \cdots, m\}$, $C_p \cap C_m$ $=$ $C_m \cap C_e$ $=$ $C_p \cap C_e$ $=$ $\emptyset$, $B_p \cup B_m \cup B_f = \{1, 2, \cdots, n\}$, $B_p \cap B_m$ $=$ $B_m \cap B_f$ $=$ $B_p \cap B_f$ $=$ $\emptyset$DualityFor any given problem, it is so-called dual of primal problem if it fulfills following condition. Let’s assume that primal is a minimization problem like follow.For given vectors$X = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}$, $C = \begin{pmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{pmatrix}$,$B = \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{pmatrix}$ and matrix $A = \begin{pmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \end{pmatrix}$ $=$ $\begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_m \end{pmatrix}$.Minimize $C^T X$ such that     $a_i X \ge b_i$ for $i \in C_p$,    $a_i X \le b_i$ for $i \in C_m$,    $a_i X = b_i$ for $i \in C_e$,    $x_i \ge 0$ for $i \in B_p$,    $x_i \le 0$ for $i \in B_m$,    No limitation for $x_i$ for $i \in B_f$.Then dual of primal minimization problem is a maximization problem like follow.For given vectors$Y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{pmatrix}$, $C = \begin{pmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{pmatrix}$,$B = \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{pmatrix}$ and matrix $A = \begin{pmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \end{pmatrix}$ $=$ $\begin{pmatrix} A_1 &amp; A_2 &amp; \cdots &amp; A_n \end{pmatrix}$.Maximize $B^T Y$ such that     $y_i \ge 0$ for $i \in C_p$,    $y_i \le 0$ for $i \in C_m$,    No limitation for $y_i$ for $i \in C_e$,    $A_i^T Y \le c_i$ for $i \in B_p$,    $A_i^T Y \ge c_i$ for $i \in B_m$,    $A_i^T Y = c_i$ for $i \in B_f$.It’s the same for the opposite.For given vectors$Y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{pmatrix}$, $C = \begin{pmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{pmatrix}$,$B = \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{pmatrix}$ and matrix $A = \begin{pmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \end{pmatrix}$ $=$ $\begin{pmatrix} A_1 &amp; A_2 &amp; \cdots &amp; A_n \end{pmatrix}$.Maximize $B^T Y$ such that     $y_i \ge 0$ for $i \in C_p$,    $y_i \le 0$ for $i \in C_m$,    No limitation for $y_i$ for $i \in C_e$,    $A_i^T Y \le c_i$ for $i \in B_p$,    $A_i^T Y \ge c_i$ for $i \in B_m$,    $A_i^T Y = c_i$ for $i \in B_f$.Then dual of primal maximization problem is a minimization problem like follow.For given vectors$X = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}$, $C = \begin{pmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{pmatrix}$,$B = \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{pmatrix}$ and matrix $A = \begin{pmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \end{pmatrix}$ $=$ $\begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_m \end{pmatrix}$.Minimize $C^T X$ such that     $a_i X \ge b_i$ for $i \in C_p$,    $a_i X \le b_i$ for $i \in C_m$,    $a_i X = b_i$ for $i \in C_e$,    $x_i \ge 0$ for $i \in B_p$,    $x_i \le 0$ for $i \in B_m$,    No limitation for $x_i$ for $i \in B_f$.ExampleNow, we’ve understood what is dual of primal.However, where does it come from?Let’s think about the following LP.Minimize $-x_1 + 3x_2 + x_3$ such that     $x_2 + 2x_3 \ge 6$,    $x_1 - x_2 + x_3 \ge 4$,    $x_1 - 3x_2 \le 2$,    $2x_1 - x_2 + x_3 = 7$,    $x_1 \ge 0$,    $x_2 \le 0$,    No limitation for $x_3$.Notice that optimal solution is $1$ with $(x_1, x_2, x_3) = (2,0,3)$.Even though we know the optimum is $1$, we may want to provide some lower bound of the problem.In this case, we can think about sumation of constraints.If we think about any feasible solution $X = \begin{pmatrix}x_1 \\ x_2 \\ x_3 \end{pmatrix}$, all of constraints should hold.As a result all of following is true.  $x_2 + 2x_3$ $\ge$ $6$  $x_1 - x_2 + x_3$ $\ge$ $4$  $x_1 - 3x_2$ $\le$ $2$  $2x_1 - x_2 + x_3$ $=$ $7$Now, let’s define $Y = \begin{pmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \end{pmatrix}$ and multiply it to all of them.Then all of following is true.Notice that each $y_i$ has positive or negative values to match direction of inequality.  $(x_2 + 2x_3) y_1$ $\ge$ $6y_1$, $y_1$ $\ge$ $0$  $(x_1 - x_2 + x_3)y_2$ $\ge$ $4y_2$, $y_2$ $\ge$ $0$  $(x_1 - 3x_2)y_3$ $\ge$ $2y_3$, $y_3$ $\le$ $0$  $(2x_1 - x_2 + x_3)y_4$ $=$ $7y_4$, no limitation for $y_4$Now, let’s sum up all the constraints.$(x_2 + 2x_3)y_1$ $+$ $(x_1 - x_2 + x_3)y_2$ $+$ $(x_1 - 3x_2)y_3$ $+$ $(2x_1 - x_2 + x_3)y_4$ $\ge$ $6y_1 + 4y_2 + 2y_3 + 7y_4$.Now, just reorder the formula with the $x_1, x_2, x_3$.Then, $(y_2 + y_3 + 2y_4)x_1$ $+$ $(y_1 - y_2 - 3y_3 - y_4)x_2$ $+$ $(2y_1 + y_2 + y_4)x_3$ $\ge$ $6y_1 + 4y_2 + 2y_3 + 7y_4$.Now, if each of factor of $x_1, x_2, x_3$ is less or equal than original problem then it should be less or equal than before.Therefore, $-x_1 + 3x_2 + x_3$ $\ge$ $(y_2 + y_3 + 2y_4)x_1$ $+$ $(y_1 - y_2 - 3y_3 - y_4)x_2$ $+$ $(2y_1 + y_2 + y_4)x_3$ should hold if followings are true.  $y_2 + y_3 + 2y_4$ $\le$ $-1$ because $x_1$ $\ge$ $0$  $y_1 - y_2 - 3y_3 - y_4$ $\ge$ $3$ because $x_2$ $\le$ $0$  $2y_1 + y_2 + y_4$ $=$ $1$ because there was no limitation for $x_3$Then $-x_1 + 3x_2 + x_3$ $\ge$ $6y_1 + 4y_2 + 2y_3 + 7y_4$ is true.As a result, we can define a maximization problem of lower bound of minization problem like follow.Maximize $6y_1 + 4y_2 + 2y_3 + 7y_4$ such that     $y_2 + y_3 + 2y_4$ $\le$ $-1$,    $y_1 - y_2 - 3y_3 - y_4$ $\ge$ $3$,    $2y_1 + y_2 + y_4$ $=$ $1$,    $y_1 \ge 0$,    $y_2 \ge 0$,    $y_3 \le 0$,    No limitation for $y_4$.Notice that optimum of this maximization problem is $1$ with $(y_1, y_2, y_3, y_4) = (\frac{5}{9}, 0, -\frac{7}{9}, -\frac{1}{9})$.Like the process above, we can change the problem to dual of it which finds lower or upper bound of primal.Weak dualityLet’s assume that primal problem is a minimzation problem for $C^T X$ and dual of it is a maximization problem for $B^T Y$.Now, let’s define optimum of primal as $X$ and optimum of dual as $Y$.Then, $C^T X \ge B^T Y$ is valid because optimum of dual is a lower bound of primal.Strong dualityWith the above, there is a stronger statement for it.If there is a fesible optimum for both primal and dual, $C^T X = B^T Y$.Proof will be updated later.Notice that there are three type of situations for LP.  LP has an optimum.  LP has no feasible solution.  LP has unbounded solution.(Infinitely small or big)For each case, the possible situation is like follow.                   Primal has an optimum      Primal has no feasible solution      Primal has unbounded solution                  Dual has an optimum      Possible      Impossible      Impossible              Dual has no feasible solution      Impossible      Possible      Possible              Dual has unbounded solution      Impossible      Possible      Impossible      Complementary slacknessFor given vectors$X = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}$, $Y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{pmatrix}$, $C = \begin{pmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{pmatrix}$,$B = \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{pmatrix}$ and matrix $A = \begin{pmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \end{pmatrix}$ $=$ $\begin{pmatrix} A_1 &amp; A_2 &amp; \cdots &amp; A_n \end{pmatrix}$ $=$ $\begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_m \end{pmatrix}$.Primal problem is like follow.Minimize $C^T X$ such that     $a_i X \ge b_i$ for $i \in C_p$,    $a_i X \le b_i$ for $i \in C_m$,    $a_i X = b_i$ for $i \in C_e$,    $x_i \ge 0$ for $i \in B_p$,    $x_i \le 0$ for $i \in B_m$,    No limitation for $x_i$ for $i \in B_f$.With the primal problem, dual is like follow.Maximize $B^T Y$ such that     $y_i \ge 0$ for $i \in C_p$,    $y_i \le 0$ for $i \in C_m$,    No limitation for $y_i$ for $i \in C_e$,    $A_i^T Y \le c_i$ for $i \in B_p$,    $A_i^T Y \ge c_i$ for $i \in B_m$,    $A_i^T Y = c_i$ for $i \in B_f$.Then, there is so-called complementary slackness condition which is follow.  $(a_i X - b_i) y_i = 0$ for all $0 \le i \le m$  $x_j(A_j^T Y \le c_j) = 0$ for all $0 \le j \le n$Now, there is a theorem as known as complementary slackness.Let $X$ and $Y$ be feasible solutions for primal and dual of LP repectively.Then, $X$ and $Y$ fulfill complementary slackness condition if and only if they are optimum of primal and dual repectively.Proof will be updated later.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> LP </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Approximation algorithm(9) - survivable network design]]></title>
      <url>/algorithm/approximation/2021/04/17/Approximation-algorithm(9)/</url>
      <content type="text"><![CDATA[Survivable network design is a problem that designing a network that can survive from disconnecting some edges.Let’s assume that there is a given undirected graph $G = (V,E)$ with costs $c_e \ge 0$ $\forall e \in E$.Now, let’s define connectivity requirments $r_{ij} \in \mathbb{Z}^{+} \cup \{0\}$ for all pairs of vertices $i,j \in V$, where $i \neq j$.Then problem is a find a minimum cost set of edges $F \subset E$ such that $G’ = (V,F)$ has $r_{ij}$ edge distinct paths to connecting $i$ and $j$.Now, we can make a integer programming over this and it is like follow.“Minimize $\sum\limits_{e \in E} c_e x_e$ such that $\sum\limits_{e \in \delta(S)} x_e \ge \max\limits_{i \in S, j \not\in S} r_{ij}$ $\forall S \subset V$, $x_e \in \{0,1\}$” which $\delta(S)$ denotes the set of edges between $S$ and $V - S$.Notice that if there is $r_{ij}$ edge distinct paths, we can make a flow of value $r_{ij}$ between $i$ and $j$.Then, there should be a mincut that corresponding to that flow from the network flow theory.As a result, constraint above should be fulfilled.It’s the same in the opposite direction.If we have a such min-cut then we have such flow either.Now, we can do a linear programming relaxation like we did in the set cover.“Minimize $\sum\limits_{e \in E} c_e x_e$ such that $\sum\limits_{e \in \delta(S)} x_e \ge \max\limits_{i \in S, j \not\in S} r_{ij}$ $\forall S \subset V$, $0 \le x_e \le 1$”.We can solve this LP in polynomial time with ellipsoid method.We can construct a seperation orcale like follow.  Make a graph that consists of $G = (V,E)$ and set capacity of $E$ as $x_e$.  Solve the network flow problem between all pair $i$ and $j$ such that $i \neq j$ and $i,j \in V$.  If there is a flow such that the value of flow $f_{ij}$ is less than $r_{ij}$ then it’s an infeasible solution otherwise it is feasible.Now, let’s define $f(S) = \max\limits_{i \in S, j \not\in S} r_{ij}$.Then problem will be shifted to “Minimize $\sum\limits_{e \in E} c_e x_e$ such that $\sum\limits_{e \in \delta(S)} x_e \ge f(S)$ $\forall S \subset V$, $0 \le x_e \le 1$”.If we make $f(S)$ as so then $f(S)$ is so-called weakly supermodular.To be a weakly supermodular, there are some requirements.  $f(\emptyset) = f(U) = 0$ for ground set $U$.  one of $f(A) + f(B)$ $\le$ $f(A \cap B) + f(A \cup B)$ or $f(A) + f(B)$ $\le$ $f(A - B) + f(B - A)$ should be true.Proof is like follow.First, it is trivial that $f(\emptyset) = f(V) = 0$ because there is no such an edge between $V$ and $\emptyset$.Before we prove the second requirments, $f(A) = f(V - A)$ is trivial either.Also, we have $max(f(A), f(B)) \ge f(A \cup B)$.Because if we think about $i \in A \cup B, j \not\in A \cup B$ then it should be one of “$i \in A, j \not\in A$ or $i \in B, j \not\in B$”.Now, we can make 4 property of $f$ from the fact above.  $f(A)$ $\le$ $\max(f(A - B), f(A \cap B))$  $f(A)$ $=$ $f(V - A)$ $\le$ $\max(f(B - A),$ $f(V - (A \cup B)))$ $=$ $\max(f(B - A),$ $f(A \cup B))$  $f(B)$ $\le$ $\max(f(B - A), f(A \cap B))$  $f(B)$ $=$ $f(V - B)$ $\le$ $\max(f(A - B),$ $f(V - (A \cup B)))$ $=$ $\max(f(A - B),$ $f(A \cup B))$Notice that $(A - B)$ $\cup$ $(A \cap B)$ $=$ $A$, $(V - (A \cup B))$ $\cup$ $(B - A)$ $=$ $V$ $-$ $A$, $(B - A)$ $\cup$ $(A \cap B)$ $=$ $B$ and $(V- (A \cup B))$ $\cup$ $(A - B)$ $=$ $V$ $-$ $B$.This implies 4 inequalities.  $f(A) + f(B)$ $\le$ $\max(f(A - B), f(A \cap B))$ $+$ $\max(f(B - A), f(A \cap B))$  $f(A) + f(B)$ $\le$ $\max(f(A - B), f(A \cap B))$ $+$ $\max(f(A - B), f(A \cup B))$  $f(A) + f(B)$ $\le$ $\max(f(B - A), f(A \cup B))$ $+$ $\max(f(B - A), f(A \cap B))$  $f(A) + f(B)$ $\le$ $\max(f(B - A), f(A \cup B))$ $+$ $\max(f(A - B), f(A \cup B))$Then, we can make possible inequality for weakly supermodular in any case.  $f(A) + f(B)$ $\le$ $\max(f(A - B), f(A \cap B))$ $+$ $\max(f(B - A), f(A \cap B))$ $=$ $f(A - B)$ $+$ $f(B - A)$ if $f(A - B), f(B - A)$ $\ge$ $f(A \cap B)$  $f(A) + f(B)$ $\le$ $\max(f(A - B), f(A \cap B))$ $+$ $\max(f(A - B), f(A \cup B))$ $=$ $f(A \cap B)$ $+$ $f(A \cup B)$ if $f(A \cap B), f(A \cup B)$ $\ge$ $f(A - B)$  $f(A) + f(B)$ $\le$ $\max(f(B - A), f(A \cup B))$ $+$ $\max(f(B - A), f(A \cap B))$ $=$ $f(A \cap B)$ $+$ $f(A \cup B)$ if $f(A \cap B), f(A \cup B)$ $\ge$ $f(B - A)$  $f(A) + f(B)$ $\le$ $\max(f(B - A), f(A \cup B))$ $+$ $\max(f(A - B), f(A \cup B))$ $=$ $f(A - B)$ $+$ $f(B - A)$ if $f(A - B), f(B - A)$ $\ge$ $f(A \cup B)$Notice that if all requirements is false then all following should be true.  $\min(f(A - B), f(B - A))$ $&lt;$ $f(A \cap B)$  $\min(f(A \cap B), f(A \cup B))$ $&lt;$ $f(A - B)$  $\min(f(A \cap B), f(A \cup B))$ $&lt;$ $f(B - A)$  $\min(f(A - B), f(B - A))$ $&lt;$ $f(A \cup B)$Then, it is a contraction because of following reasoning.$\min(f(A - B), f(B - A))$ $&lt;$ $\min(f(A \cap B), f(A \cup B))$ from 1, 4$\min(f(A \cap B), f(A \cup B))$ $&lt;$ $\min(f(A - B), f(B - A))$ from 2, 3$\min(f(A \cap B), f(A \cup B))$ $&lt;$ $\min(f(A - B), f(B - A))$ $&lt;$ $\min(f(A \cap B), f(A \cup B))$ from above two.As a result, $\min(f(A \cap B), f(A \cup B))$ $&lt;$ $\min(f(A \cap B), f(A \cup B))$ and it can’t be true.Therefore, $f$ is a weakly supermodular.Now, there is a nice property of a weakly supermodular $f$.There exists an edge $e \in E$ such that $x_e \ge \frac{1}{2}$ if we solve linear problem “Minimize $\sum\limits_{e \in E} c_e x_e$ such that $\sum\limits_{e \in \delta(S)} x_e \ge f(S)$ $\forall S \subset V$, $0 \le x_e \le 1$” for any weakly supermodular $f$.Proof will be followed at the end of the post.From the fact above, we can construct an algorithm.    $F \leftarrow \emptyset$    $i \leftarrow 1$    $\operatorname{while}$ $F$ is not a feasible solution $\textbf{do}$        Solve "Minimize $\sum\limits_{e \in E - F} c_e x_e$ such that $\sum\limits_{e \in \delta(S), e \in E - F} x_e \ge f_i(S) = f(S) - |\delta(S) \cap F|$ $\forall S \subset V$, $0 \le x_e \le 1$"        $F_i \leftarrow \{ e \in E - F \vert x_e \ge \frac{1}{2} \}$        $F \leftarrow F \cup F_i$        $i \leftarrow i + 1$        $\textbf{return} \text{ } F$Notice that it is so-called $\operatorname{iterative rounding}$ because it uses LP-relxation to extend solution and iterates it many times.If the algorithm above terminates, solution should be feasible.Now, we will show that solution will be in $2\operatorname{OPT}$ and it terminates.First of all, we will show “If we define $z(e) \ge 0$ for all $e \in E$ and define $z(E) = \sum\limits_{e \in E}z(e)$ then $z(\delta(A)) + z(\delta(B))$ $\ge$ $z(\delta(A \cup B)) + z(\delta(A \cap B))$ and $z(\delta(A)) + z(\delta(B))$ $\ge$ $z(\delta(A - B)) + z(\delta(B - A))$ for any $A, B \subset V$”.Proof is like follow.If you think about the category of edges in $\delta(A)$, it will be one of follows.  $i \in A - B$, $j \in V - (A \cup B)$  $i \in A \cap B$, $j \in V - (A \cup B)$  $i \in A - B$, $j \in B - A$  $i \in A \cap B$, $j \in B - A$It’s the same for the B either.Therefore, $z(\delta(A)) + z(\delta(B))$ will be sum of $z(e)$ in following 8 categories.  $i \in A - B$, $j \in V - (A \cup B)$  $i \in A \cap B$, $j \in V - (A \cup B)$  $i \in A - B$, $j \in B - A$  $i \in A \cap B$, $j \in B - A$  $i \in B - A$, $j \in V - (A \cup B)$  $i \in A \cap B$, $j \in V - (A \cup B)$  $i \in B - A$, $j \in A - B$  $i \in A \cap B$, $j \in A - B$If you think about the category of edges in $\delta(A \cup B)$, it will be one of follows.  $i \in A - B$, $j \in V - (A \cup B)$  $i \in A \cap B$, $j \in V - (A \cup B)$  $i \in B - A$, $j \in V - (A \cup B)$If you think about the category of edges in $\delta(A \cap B)$, it will be one of follows.  $i \in A \cap B$, $j \in V - (A \cup B)$  $i \in A \cap B$, $j \in B - A$  $i \in A \cap B$, $j \in A - B$Therefore, $z(\delta(A \cup B)) + z(\delta(A \cap B))$ will be sum of $z(e)$ in following 6 categories.  $i \in A - B$, $j \in V - (A \cup B)$  $i \in A \cap B$, $j \in V - (A \cup B)$  $i \in B - A$, $j \in V - (A \cup B)$  $i \in A \cap B$, $j \in V - (A \cup B)$  $i \in A \cap B$, $j \in B - A$  $i \in A \cap B$, $j \in A - B$Now, we can do a mapping from this categories of $z(\delta(A \cup B)) + z(\delta(A \cap B))$ to one of 8 categories of $z(\delta(A)) + z(\delta(B))$.$1 \rightarrow 1$, $2 \rightarrow 2$, $3 \rightarrow 5$, $4 \rightarrow 6$, $5 \rightarrow 4$, $6 \rightarrow 8.$Now we have category 3, 7 lefts.As a result, $z(\delta(A)) + z(\delta(B))$ $\ge$ $z(\delta(A \cup B)) + z(\delta(A \cap B))$.Like above, we can do the same thing for $z(\delta(A)) + z(\delta(B))$ $\ge$ $z(\delta(A - B)) + z(\delta(B - A))$.If you think about the category of edges in $\delta(A - B)$, it will be one of follows.  $i \in A - B$, $j \in V - (A \cup B)$  $i \in A - B$, $j \in A \cap B$  $i \in A - B$, $j \in B - A$If you think about the category of edges in $\delta(B - A)$, it will be one of follows.  $i \in B - A$, $j \in V - (A \cup B)$  $i \in B - A$, $j \in A \cap B$  $i \in B - A$, $j \in A - B$Therefore, $z(\delta(A - B)) + z(\delta(B - A))$ will be sum of $z(e)$ in following 6 categories.  $i \in A - B$, $j \in V - (A \cup B)$  $i \in A - B$, $j \in A \cap B$ $\leftrightarrow$ $i \in A \cap B$, $j \in A - B$  $i \in A - B$, $j \in B - A$  $i \in B - A$, $j \in V - (A \cup B)$  $i \in B - A$, $j \in A \cap B$ $\leftrightarrow$ $i \in A \cap B$, $j \in B - A$  $i \in B - A$, $j \in A - B$We can map $1 \rightarrow 1$, $2 \rightarrow 8$, $3 \rightarrow 3$, $4 \rightarrow 5$, $5 \rightarrow 4$, $6 \rightarrow 7$.Now we have category 2, 6 lefts.As a result, $z(\delta(A)) + z(\delta(B))$ $\ge$ $z(\delta(A - B)) + z(\delta(B - A))$.As a summary, both followings are true.  $z(\delta(A)) + z(\delta(B))$ $\ge$ $z(\delta(A \cup B)) + z(\delta(A \cap B))$  $z(\delta(A)) + z(\delta(B))$ $\ge$ $z(\delta(A - B)) + z(\delta(B - A))$Now, let $z_F(e) = 1$ if $e \in F$ and $z_F(e) = 0$ otherwise.Then, $f_i(S) = f(S) - |\delta(S) \cap F| = f(S) - z_F(\delta(S))$.As a result $f_i(S)$ should fulfill one of follows.  $f_i(A)$ $+$ $f_i(B)$ $=$ $f(A)$ $+$ $f(B)$ $-$ $z_F(\delta(A))$ $-$ $z_F(\delta(B))$ $\le$ $f(A \cup B)$ $+$ $f(A \cap B)$ $-$ $z_F(\delta(A \cup B))$ $-$ $z_F(\delta(A \cap B))$ $=$ $f_i(A \cup B)$ $+$ $f_i(A \cap B)$.  $f_i(A)$ $+$ $f_i(B)$ $=$ $f(A)$ $+$ $f(B)$ $-$ $z_F(\delta(A))$ $-$ $z_F(\delta(B))$ $\le$ $f(A - B)$ $+$ $f(B - A)$ $-$ $z_F(\delta(A - B))$ $-$ $z_F(\delta(B - A))$ $=$ $f_i(A - B)$ $+$ $f_i(B - A)$.Now, we showed that $f_i$ is a weakly supermodular.Therefore, we can found at least one edge such that $x_e \ge \frac{1}{2}$ for any $f_i$.Now if we can show that “Minimize $\sum\limits_{e \in E - F} c_e x_e$ such that $\sum\limits_{e \in \delta(S), e \in E - F} x_e \ge f_i(S) = f(S) - |\delta(S) \cap F|$ $\forall S \subset V$, $0 \le x_e \le 1$” can be solved in polynomial time, it will the end of the proof for polynomial execution time for the algorithm.Notice that we can do this at most $O(\left\vert E \right\vert)$.We still can use network flow algorithm to solve this.Therefore, we will make a seperation orcale like below.  Make a graph that consists of $G = (V,E)$ and set capacity of $E - F$ as $x_e$ and capacity of $F$ as 1.  Solve the network flow problem between all pair $i$ and $j$ such that $i \neq j$ and $i,j \in V$.  If there is a flow such that the value of flow $f_{ij}$ is less than $r_{ij}$ then it’s an infeasible solution otherwise it is feasible.Notice that if it is feasible then it means $\sum\limits_{e \in \delta(S)} f_e + |\delta(S) \cap F| \ge r_{ij}$ which $\delta(S)$ denotes the set of edges between $S$ and $V - S$ in $E - F$.As a result, $\sum\limits_{e \in \delta(S)} f_e \ge r_{ij} - |\delta(S) \cap F|$.If there is some $i, j$ such that flow between $i$ and $j$ is less than $r_{ij}$ then there should be $S$ such that $\sum\limits_{e \in \delta(S)} f_e + |\delta(S) \cap F|&lt; r_{ij}$.As a result, we can use this as a seperation orcale.Now only left thing to show is that solution is in $2\operatorname{OPT}$.To show this, we will prove generalized version of the claim.The claim is “For any weakly supermodular function $f$, let’s define $x^i$ as the solution of LP in $i$th iteration.If we can solve iterative rounding algorithm above in $k$ iteration then solution of iterative rounding $\operatorname{ANS}$ $=$ $\sum\limits_{e \in F} c_e$ is in $2\sum\limits_{e \in E} c_e x_e^1$”.Notice that this implies $\operatorname{ANS}$ $=$ $\sum\limits_{e \in F} c_e$ $\le$ $2\sum\limits_{e \in E} c_e x_e^1$ $\le$ $2\operatorname{OPT}$.We will show this in the inductive method.If $k = 1$, we will solve “Minimize $\sum\limits_{e \in E} c_e x_e$ such that $\sum\limits_{e \in \delta(S), e \in E} x_e \ge f(S)$ $\forall S \subset V$, $0 \le x_e \le 1$” once and that’s all.From the above, we will select $\{x_e : x_e \ge \frac{1}{2}\}$.As a result, $\sum\limits_{e \in F_1} c_e$ $\le$ $2\sum\limits_{e \in F_1} c_e x_e^1$.If $k \ge 2$, we have three facts follow.First, $\sum\limits_{e \in F_1} c_e$ $\le$ $2\sum\limits_{e \in F_1} c_e x_e^1$ is true because we selected $x_e^1 \ge \frac{1}{2}$ in $x_e^1$.Also, if we consider $\{x_e^1 | e \in E - F_1\}$ $=$ $\Lambda$ then $\Lambda$ is a feasible solution of LP in the second iteration either.The reason is like follow.Second constraint is trivial to be hold because $0 \le x_e^1 \le 1$.First constraint holds either because of follows.$\sum\limits_{e \in \delta(S), e \in E - F_1} x_e^1$ $=$ $\sum\limits_{e \in \delta(S), e \in E} x_e^1 - \sum\limits_{e \in \delta(S), e \in F_1} x_e^1$ $\ge$ $f_1(S) - |\delta(S) \cap F_1|$ $=$$f_2(S)$.Therefore, $\Lambda$ is a feasible solution.As a result, $\sum\limits_{e \in E - F_1} c_e x_e^1$ $\ge$ $\sum\limits_{e \in E - F_1} c_e x_e^2$.For the last, we can use iterative rounding for $E - F_1$ and $f_2(S)$ because it will terminted in $k - 1$ iterations.Notice that we’ve proved that $f_2$ is a weakly supermodular.As a result, $\sum\limits_{e \in F - F_1} c_e$ $\le$ $2\sum\limits_{e \in E - F_1} c_e x_e^2$.In a summary, we have three facts.  $\sum\limits_{e \in F_1} c_e$ $\le$ $2\sum\limits_{e \in F_1} c_e x_e^1$  $\sum\limits_{e \in E - F_1} c_e x_e^2$ $\le$ $\sum\limits_{e \in E - F_1} c_e x_e^1$  $\sum\limits_{e \in F - F_1} c_e$ $\le$ $2\sum\limits_{e \in E - F_1} c_e x_e^2$.If we combine three facts above,$\sum\limits_{e \in F} c_e$ $=$ $\sum\limits_{e \in F - F_1} c_e + \sum\limits_{e \in F_1} c_e$ $\le$ $2\sum\limits_{e \in E - F_1} c_e x_e^2 + 2\sum\limits_{e \in F_1} c_e x_e^1$ $\le$$2\sum\limits_{e \in E - F_1} c_e x_e^1 + 2\sum\limits_{e \in F_1} c_e x_e^1$ $=$$2\sum\limits_{e \in E} c_e x_e^1$.Now proof stops here for survivable network design problem.However, we need to show one following fact.There exists an edge $e \in E$ such that $x_e \ge \frac{1}{2}$ if we solve linear problem “Minimize $\sum\limits_{e \in E} c_e x_e$ such that $\sum\limits_{e \in \delta(S)} x_e \ge f(S)$ $\forall S \subset V$, $0 \le x_e \le 1$” for any weakly supermodular $f$.To show this proof, we need some definitions to make a proof.  $\chi_E$ is a vector such that $(\chi_E)_e$ $=$ $\cases{ 1, e \in E, x_e &gt; 0\cr 0, \text{otherwise}}$  $A$ and $B$ are intersecting if all of $A \cap B$, $A - B$ and $B - A$ are not empty.  $A$ is tight if $\sum\limits_{e \in \delta(A)} x_e = f(A)$ for $A \in V$.  A collection of sets $\mathcal{L}$ is $\operatorname{laminor}$ if no pair of sets in $\mathcal{L}$ is intersecting.  $\operatorname{Span}(\mathcal{L})$ $=$ $\operatorname{Span}\{\chi_{\delta(S)} \mid S \in \mathcal{L}\}$Now, we need some $\operatorname{Lemma}$s.If $A$ and $B$ are tight and intersecting, at least one of the following is true.  $A \cap B$, $A \cup B$ are both tight and $\chi_{\delta(A \cap B)} + \chi_{\delta(A \cup B)}$ $=$ $\chi_{\delta(A)} + \chi_{\delta(B)}$  $A - B$, $B - A$ are both tight and $\chi_{\delta(A - B)} + \chi_{\delta(B - A)}$ $=$ $\chi_{\delta(A)} + \chi_{\delta(B)}$Proof is like follow. one of $f(A) + f(B)$ $\le$ $f(A \cap B) + f(A \cup B)$ or $f(A) + f(B)$ $\le$ $f(A - B) + f(B - A)$ is true because $f$ is a weakly supermodular.Then, there are two cases.If we think about the first case, we can do a reasoning follow.$\sum\limits_{e \in \delta(A \cap B)} x_e$ $+$ $\sum\limits_{e \in \delta(A \cup B)} x_e$ $\ge$ $f(A \cap B) + f(A \cup B)$ because of constraints of linear problem.$f(A \cap B) + f(A \cup B)$ $\ge$ $f(A) + f(B)$ because $f$ is a weakly super modular.$f(A) + f(B)$ $=$ $\sum\limits_{e \in \delta(A)} x_e$ $+$ $\sum\limits_{e \in \delta(B)} x_e$ becacuse $A$ and $B$ is tight.As a result, $\sum\limits_{e \in \delta(A \cap B)} x_e$ $+$ $\sum\limits_{e \in \delta(A \cup B)} x_e$ $\ge$ $\sum\limits_{e \in \delta(A)} x_e$ $+$ $\sum\limits_{e \in \delta(B)} x_e$.With this fact, $\sum\limits_{e \in \delta(A \cap B)} x_e$ $+$ $\sum\limits_{e \in \delta(A \cup B)} x_e$ $\ge$ $\sum\limits_{e \in \delta(A)} x_e$ $+$ $\sum\limits_{e \in \delta(B)} x_e$ $\ge$ $\sum\limits_{e \in \delta(A \cap B)} x_e$ $+$ $\sum\limits_{e \in \delta(A \cup B)} x_e$ if we use the fact follow “If we select $z(e) \ge 0$ for all $e \in E$ and define $z(E) = \sum\limits_{e \in E}z(e)$ then $z(\delta(A)) + z(\delta(B))$ $\ge$ $z(\delta(A \cup B)) + z(\delta(A \cap B))$ and $z(\delta(A)) + z(\delta(B))$ $\ge$ $z(\delta(A - B)) + z(\delta(B - A))$ for any $A, B \subset V$”.As a result, $\sum\limits_{e \in \delta(A \cap B)} x_e$ $+$ $\sum\limits_{e \in \delta(A \cup B)} x_e$ $=$$f(A \cap B) + f(A \cup B)$ $=$$f(A) + f(B)$ $=$$\sum\limits_{e \in \delta(A)} x_e$ $+$ $\sum\limits_{e \in \delta(B)} x_e$ $=$$\sum\limits_{e \in \delta(A \cap B)} x_e$ $+$ $\sum\limits_{e \in \delta(A \cup B)} x_e$.Which means that both $A \cap B$, $A \cup B$ are tight.More over, if we recap the process of prooving the statement of $z$, there were 8 categories of edges and 2 are lefts in each cases.  $i \in A - B$, $j \in V - (A \cup B)$  $i \in A \cap B$, $j \in V - (A \cup B)$  $i \in A - B$, $j \in B - A$  $i \in A \cap B$, $j \in B - A$  $i \in B - A$, $j \in V - (A \cup B)$  $i \in A \cap B$, $j \in V - (A \cup B)$  $i \in B - A$, $j \in A - B$  $i \in A \cap B$, $j \in A - B$Now we have category 2, 6 lefts for $\delta(A - B)$ and $\delta(B - A)$.Similarly we have category 3, 7 lefts for $\delta(A \cup B)$ and $\delta(A \cap B)$.However, $x_e$ for edges in category 2, 6 should be $0$ because $\sum\limits_{e \in \delta(A \cap B)} x_e$ $+$ $\sum\limits_{e \in \delta(A \cup B)} x_e$ $=$ $\sum\limits_{e \in \delta(A)} x_e$ $+$ $\sum\limits_{e \in \delta(B)} x_e$ and $0 \le x_e \le 1$. Then, we can know that $\delta(A \cap B)$, $\delta(A \cup B)$ and $\delta(A)$, $\delta(B)$ should have the same set of edges. As a result, $\chi_{\delta(A \cap B)} + \chi_{\delta(A \cup B)}$ $=$ $\chi_{\delta(A)} + \chi_{\delta(B)}$.Simillarly, both $A - B$, $B - A$ are tight and $\chi_{\delta(A - B)} + \chi_{\delta(B - A)}$ $=$ $\chi_A + \chi_B$ should be hold in the second case.Now, we will claim below with the lemma above.Then, there is $\mathcal{L} \subset 2^{V}$ which satisfies 4 things.  $S$ is tight for all $S \in \mathcal{L}$  $\{\chi_{\delta(S)}\}_{S \in \mathcal{L}}$ are linear independent.  $\left\vert \mathcal{L} \right\vert$ $=$ $\left\vert E \right\vert$  $\mathcal{L}$ is $\operatorname{laminor}$.Proof is like follow.Let $\mathcal{T} \subset 2^{V}$ be the family of all tight sets and $\mathcal{L}$ as a maximal $\operatorname{laminor}$ subfamily of $\mathcal{T}$.Which means $\mathcal{L}$ will not be a $\operatorname{laminor}$ if we add anything more to $\mathcal{L}$.First, we will claim $\operatorname{Span}(\mathcal{T})$ $\subset$ $\operatorname{Span}(\mathcal{L})$ to show $\operatorname{Span}(\mathcal{T})$ $=$ $\operatorname{Span}(\mathcal{L})$.Notice that $\operatorname{Span}(\mathcal{L})$ $\subset$ $\operatorname{Span}(\mathcal{T})$ is trivial because $\mathcal{L} \subset \mathcal{T}$.Let’s assume not then there are some $S$ $\in$ $\mathcal{T}$ such that $\chi_{\delta(S)}$ $\not\in$ $\operatorname{Span}(\mathcal{L})$.Between possible $S$s, let’s assume that we’ve choose $S$ such that $S$ has the fewest intersecting sets in $\mathcal{L}$.However, $S$ should intersect with at least one set in $\mathcal{L}$.The reason is that we can add $S$ to $\mathcal{L}$ because it doesn’t change $\mathcal{L}$ to be not $\operatorname{laminor}$ if it don’t intersect.However, it’s contradiction to “$\mathcal{L}$ is maximal $\operatorname{laminor}$”.Now, we can find $T \in \mathcal{L}$ such that $T$ is intersecting with $S$.Then one of two need to be true because both $S$, $T$ are tight because $T \in \mathcal{L} \subset \mathcal{T}$ and $S \in \mathcal{T}$.  $S \cap T$, $S \cup T$ are both tight and $\chi_{\delta(S \cap T)} + \chi_{\delta(S \cup T)}$ $=$ $\chi_{\delta(S)} + \chi_{\delta(T)}$  $S - T$, $T - S$ are both tight and $\chi_{\delta(S - T)} + \chi_{\delta(T - S)}$ $=$ $\chi_{\delta(S)} + \chi_{\delta(T)}$Let’s assume that it’s the case 1.Then both $\chi_{\delta(S \cap T)}$, $\chi_{\delta(S \cup T)}$ can’t be in $\operatorname{Span}(\mathcal{L})$ at the same time.Proof is like follow.$\chi_{\delta(S)}$ $=$ $\chi_{\delta(S \cap T)}$ + $\chi_{\delta(S \cup T)}$ - $\chi_{\delta(T)}$ and $\chi_{\delta(S)}$ should be in $\operatorname{Span}(\mathcal{L})$ if $\chi_{\delta(S \cup T)}$, $\chi_{\delta(S \cap T)}$ are in $\operatorname{Span}(\mathcal{L})$ at the same time.It’s the same of case 2 because $\chi_{\delta(S)}$ $=$ $\chi_{\delta(S - T)}$ + $\chi_{\delta(T)}$ - $\chi_{\delta(T - S)}$.Now, let’s think about $X$ such that $\chi_{\delta(X)} \not\in \operatorname{Span}(\mathcal{L})$ and $X \in \{S \cap T, S \cup T\}$ in the first case or $\{S - T, T - S\}$ in the second case.Then, $S$ and $Y$ are intersecting for any $Y \in \mathcal{L}$ such that $Y$ is intersecting with $X$.Proof is like follow.Notice that $Y$ should be one of three followings because $Y \in \mathcal{L}$, $T \in \mathcal{L}$.  $Y \cap T = \emptyset$  $T - Y = \emptyset$  $Y - T = \emptyset$Notice that all of followings are true.  $X \cap Y \neq \emptyset$  $X - Y \neq \emptyset$  $Y - X \neq \emptyset$  $S \cap T \neq \emptyset$  $S - T \neq \emptyset$  $T - S \neq \emptyset$If $X = S \cap T$, $Y$ should fulfill one of three followings.  $Y \cap T = \emptyset$ $Y$ can’t intersect with $X$ because $Y \cap X$ $\subseteq$ $Y \cap T$ $=$ $\emptyset$. Therefore, such $Y$ doesn’t exist.  $T - Y = \emptyset$ $Y$ can’t intersect with $X$ because $X - Y$ $\subseteq$ $T - Y$ $=$ $\emptyset$. Therefore, such $Y$ doesn’t exist.  $Y - T = \emptyset$ $\emptyset$ $\neq$ $X \cap Y$ $\subseteq$ $S \cap Y$. $\emptyset$ $\neq$ $X - Y$ $\subseteq$ $S - Y$. $\emptyset$ $\neq$ $Y - X$ $=$ $Y - (S \cap T)$ $=$ $(Y - S)$ $\cup$ $(Y - T)$ $=$ $Y - S$. As a result, $S$ intersects with $Y$.If $X = S \cup T$, $Y$ should fulfill one of three followings.  $Y \cap T = \emptyset$ $\emptyset$ $\neq$ $X \cap Y$ $=$ $(S \cup T) \cap Y$ $=$ $(S \cap Y) \cup (T \cap Y)$ $=$ $S \cap Y$. $\emptyset$ $\neq$ $S \cap T$ $=$ $(S \cap T) - (S \cap (T \cap Y))$ $=$ $(S \cap T) - ((S \cap T) \cap Y)$ $=$ $(S \cap T) - Y$ $\subseteq$ $((S \cap T) - Y) \cup ((S - T) - Y)$ $=$ $S - Y$. $\emptyset$ $\neq$ $Y - X$ $\subseteq$ $Y - S$. As a result, $S$ intersects with $Y$.  $T - Y = \emptyset$ $\emptyset$ $\neq$ $S \cap T$ $=$ $S \cap ((T \cap Y) \cup (T - Y))$ $=$ $S \cap (T \cap Y)$ $=$ $S \cap T \cap Y$ $=$ $(S \cap Y) \cap T$ $\subseteq$ $S \cap Y$. $\emptyset$ $\neq$ $X - Y$ $=$ $(S \cup T) - Y$ $=$ $(S - Y) \cup (T - Y)$ $=$ $S - Y$. $\emptyset$ $\neq$ $Y - X$ $\subseteq$ $Y - S$. As a result, $S$ intersects with $Y$.  $Y - T = \emptyset$ $Y$ can’t intersect with $X$ because $Y - X$ $\subseteq$ $Y - T$ $=$ $\emptyset$. Therefore, such $Y$ doesn’t exist.If $X = S - T$, $Y$ should fulfill one of three followings.  $Y \cap T = \emptyset$ $\emptyset$ $\neq$ $X \cap Y$ $=$ $(S - T) \cap Y$ $\subseteq$ $S \cap Y$. $\emptyset$ $\neq$ $X - Y$ $\subseteq$ $S - Y$. $\emptyset$ $\neq$ $Y - X$ $=$ $Y - (S - T)$ $=$ $Y - (Y \cap (S - T))$ $=$ $Y - ((Y \cap S) - (Y \cap T))$ $=$ $Y - (Y \cap S)$ $=$ $Y - S$. As a result, $S$ intersects with $Y$.  $T - Y = \emptyset$ $\emptyset$ $\neq$ $X \cap Y$ $=$ $(S - T) \cap Y$ $=$ $(S \cap  Y) - (T \cap Y)$ $\subseteq$ $S \cap Y$. $\emptyset$ $\neq$ $X - Y$ $\subseteq$ $S - Y$. $\emptyset$ $\neq$ $T - S$ $\subseteq$  $(Y \cup T) - S$ $=$ $(Y \cup (T - Y)) - S$ $=$ $(Y - S) \cup ((T - Y) - S)$ $=$ $Y - S$. As a result, $S$ intersects with $Y$.  $Y - T = \emptyset$ $Y$ can’t intersect with $X$ because $X \cap Y$ $=$ $(S - T) \cap Y$ $=$ $(S \cap Y) - (T \cap Y)$ $=$ $(S \cap Y) - ((T \cap Y) \cup (Y - T))$ $\subseteq$ $((S \cap Y) \cup (Y - S)) - ((T \cap Y) \cup (Y - T))$ $=$ $Y - Y$ $=$ $\emptyset$. Therefore, such $Y$ doesn’t exist.If $X = T - S$, $Y$ should fulfill one of three followings.  $Y \cap T = \emptyset$ $Y$ can’t intersect with $X$ because $X \cap Y$ $=$ $(T - S) \cap Y$ $\subseteq$ $T \cap Y$ $=$ $\emptyset$. Therefore, such $Y$ doesn’t exist.  $T - Y = \emptyset$ $Y$ can’t intersect with $X$ because $X - Y$ $=$ $(T - S) - Y$ $\subseteq$ $T - Y$ $=$ $\emptyset$. Therefore, such $Y$ doesn’t exist.  $Y - T = \emptyset$ $\emptyset$ $\neq$ $Y - X$ $=$ $Y - (T - S)$ $=$ $Y - ((T \cup (Y - T)) - S)$ $=$ $Y - ((Y \cup T) - S)$ $=$ $Y - ((Y \cup (T - Y)) - S)$ $\subseteq$ $Y - (Y - S)$ $=$ $S \cap Y$. $\emptyset$ $\neq$ $S - T$ $=$ $S - (T \cup (Y - T))$ $=$ $S - (Y \cup T)$ $=$ $S - (Y \cup (T - Y))$ $\subseteq$ $S - Y$. $\emptyset$ $\neq$ $X \cap Y$ $=$ $(T - S) \cap Y$ $=$ $(T - (S \cap T)) \cap Y$ $=$ $(T \cap Y) - ((S \cap T) \cap Y)$ $=$ $(T \cap Y) - (S \cap (T \cap Y))$ $=$ $(T \cap Y) - S$ $=$ $((T \cap Y) \cup (Y - T)) - S$ $=$ $Y - S$. As a result, $S$ intersects with $Y$.As a result, $S$ and $Y$ are intersecting for any $Y \in \mathcal{L}$ such that $Y$ is intersecting with $X$.Now, let’s think about $X$.Then, $X$ should have strictly fewer intersecting sets with $\mathcal{L}$ than $S$.The reason is like follow.If $X$ is intersecting with $Y \in \mathcal{L}$ then, $S$ does so either.However, there is at least one set such that intersects with $S$ but not with $X$ which is $T$.Notice that $X \in \{S \cap T, S \cup T, S - T, T - S\}$ and$(S \cap T) - T = \emptyset$,$T - (S \cup T) = \emptyset$,$(S - T) \cap T = \emptyset$,$(T - S) - T= \emptyset$.However, this is contradiction that we’ve choosed $S$ such that has $S$ has the fewest intersecting sets in $\mathcal{L}$.Therefore, such an $S$ can’t exists and $\operatorname{Span}(\mathcal{T})$ $=$ $\operatorname{Span}(\mathcal{L})$ is true.Now, we’ve showend that there is at least one of a maximal $\operatorname{laminor}$ subfamily of $\mathcal{T}$ namely $\mathcal{L}$ such that fulfills following two.  $S$ is tight for all $S \in \mathcal{L}$  $\mathcal{L}$ is $\operatorname{laminor}$.Now, we can find some set $S \in \mathcal{T}$ which is lineary dependent from other vectors.Then, we can just remove it without losing property above.Now, we keep doing it untill every vectors are lineary independent and let the result to be $\mathcal{L}^\star$.Then, $\mathcal{L}^\star$ should have $\left\vert E \right\vert$ independent vectors because they are all independent and $\operatorname{Span}(\mathcal{L}^\star)$ Should be still $\operatorname{Span}(\mathcal{T})$ because we removed only lineary dependent vectors.Notice that each vector has $\left\vert E \right\vert$ elements inside.Therefore, $\mathcal{L}^\star$ fulfills every property below.  $S$ is tight for all $S \in \mathcal{L}^\star$  $\{\chi_{\delta(S)}\}_{S \in \mathcal{L}^\star}$ are linear independent.  $\left\vert \mathcal{L}^\star \right\vert$ $=$ $\left\vert E \right\vert$  $\mathcal{L}^\star$ is $\operatorname{laminor}$.Now, we will show the following is true.There exists an edge $e \in E$ such that $x_e \ge \frac{1}{2}$ if we solve linear problem “Minimize $\sum\limits_{e \in E} c_e x_e$ such that $\sum\limits_{e \in \delta(S)} x_e \ge f(S)$ $\forall S \subset V$, $0 \le x_e \le 1$” for any weakly supermodular $f$.To show this, let’s think about such an $\mathcal{L}$ satisfies 4 properties from given LP and let’s assume that $0 &lt; x_e &lt; \frac{1}{2}$ for all $e \in E$.Now, let’s define set $A$ is bigger than $B$ when $B \subset A$.Then we can construct a forest since $\mathcal{L}$ is a $\operatorname{laminor}$.Notice that one of following should be true for $X, Y \in \mathcal{L}$.  $X \cap Y$ $=$ $\emptyset$  $X - Y$ $=$ $\emptyset$ $\leftrightarrow$ $X \subseteq Y$  $Y - X$ $=$ $\emptyset$ $\leftrightarrow$ $Y \subseteq X$From the fact above, we will construct a forest like follow.  Select biggest sets in $\mathcal{L}$ and make them to the roots.  Select biggest sets in $S$ for any root $S$ and make them to child of $S$.  Keep do this recursively untill every set is in the forest.Notice that $X$ is the child of $S$ if and only if there is no set $Y$ exists such that $Y$ is the child of $S$ but $X$ is child of $Y$ either.It means there is no double depth child relation.With this $\mathcal{L}$, we can pass some costs to the sets.Let’s pass the costs like follow.  Pass $x_e$ to set $X$ if $u$ or $v$ is in $X$ and $X$ is the smallest set among such sets for any edge $e = (u,v)$.  Pass $1 - 2x_e$ to set $X$ if both $u$ and $v$ are in $X$ and $X$ is the smallest set among such sets for any edge $e = (u,v)$.Then, select a root of forest $S$.With this $S$, let’s define child of $S$ as $C_1, C_2, \cdots, C_n$ and $C = \bigcup\limits_{k = 1}^{n} C_k$.Then, we can define 4 categories for edge $e \in (\delta(S) \cup \delta(C))$.  $E_{cc}$ is the set of edges $e \in E$ such that one end point of edge is in $C_i$ and other is in $C_j$ for $i \neq j$.  $E_{cp}$ is the set of edges $e \in E$ such that one end point of edge is in $C_i$ and other is in $S - C$.  $E_{co}$ is the set of edges $e \in E$ such that one end point of edge is in $C_i$ and other is in $V - S$.  $E_{po}$ is the set of edges $e \in E$ such that one end point of edge is in $S - C$ and other is in $V - S$.If we count every cost gain for $S$ from each categories, it is like follow.  $1 - 2x_e$ from $E_{cc}$.  $1 - 2x_e + x_e = 1 - x_e$ from $E_{cp}$.  Nothing from $E_{co}$.  $x_e$ from $E_{po}$.Then, the total costs $S$ gain is $\left\vert E_{cc} \right\vert$ $-$ $2x(E_{cc})$ $+$ $\left\vert E_{cp} \right\vert$ $-$ $x(E_{cp})$ $+$ $x(E_{po})$ $=$ $\left\vert E_{cc}\right\vert$ $+$ $\left\vert E_{cp} \right\vert$ $-$ $2x(E_{cc})$ $-$ $x(E_{cp})$ $+$ $x(E_{po})$.Notice that $E_{cc} \cup E_{cp} \cup E_{po} \neq \emptyset$.Proof is like follow.Let’s assume not then $E_{cc} = E_{cp} = E_{po} = \emptyset$.Which means $x(\delta(S))$ $=$ $x(E_{po} + E_{co})$ $=$ $x(E_{co})$ $=$ $x(E_{co} \cup E_{cc} \cup E_{cp})$ $=$ $x(\delta(C))$ $=$ $x(\delta(\bigcup\limits_{k = 1}^{n} C_k))$ $=$ $\sum\limits_{k = 1}^n x(\delta(C_k))$ because $C_i \cap C_j = \emptyset$.Notice that $C_i$ is the sibling of $C_j$ in the forest.However, $x(\delta(S))$ $=$ $\sum\limits_{k = 1}^n x(\delta(C_k))$ can’t be true because it’s a contradiction for the fact that $\chi_{\delta(S)}$, $\chi_{\delta(C_1)}$, $\cdots$, $\chi_{\delta(C_n)}$ are linear independent.Therefore, $\left\vert E_{cc} \right\vert$ $-$ $2x(E_{cc})$ $+$ $\left\vert E_{cp} \right\vert$ $-$ $x(E_{cp})$ $+$ $x(E_{po})$ $&gt;$ $0$.Notice that every value is positive because we assumed $0 &lt; x_e &lt; \frac{1}{2}$.Now, let’s think about the categories of edges for $E_{cc}, E_{cp}, E_{po}$.Then $x(\delta(S))$ $=$ $x(E_{po})$ $+$ $x(E_{co})$ and $x(\delta(C))$ $=$ $x(E_{cp})$ $+$ $2x(E_{cc})$ $+$ $x(E_{co})$.Reason is like follow.  For $\delta(S)$, all the edges should be one of $E_{co}$ or $E_{po}$ because one of end point should be in $V - S$.  For $\delta(C)$, there are three type of edges like just described. One for from $C_i$ to $V - S$. One for from $C_i$ to $S - C$. One for from $C_i$ to $C_j$ for $i \neq j$. However, $x(\delta(C))$ will count twice for “One for from $C_i$ to $C_j$ for $i \neq j$”.As a result, $\left\vert E_{cc} \right\vert$ $+$ $\left\vert E_{cp} \right\vert$ $-$ $2x(E_{cc})$ $-$ $x(E_{cp})$ $+$ $x(E_{po})$ $=$ $\left\vert E_{cc} \right\vert$ $+$ $\left\vert E_{cp} \right\vert$ $+$ $x(E_{po})$ $+$ $x(E_{co})$ $-$ $(x(E_{cp}) + 2x(E_{cc}) + x(E_{co}))$$=$ $\left\vert E_{cc} \right\vert$ $+$ $\left\vert E_{cp} \right\vert$ $+$ $x(\delta(S))$ $-$ $x(\delta(C))$.However, $\left\vert E_{cc} \right\vert$ $+$ $\left\vert E_{cp} \right\vert$ $+$ $x(\delta(S))$ $-$ $x(\delta(C))$ should be an integer because all of elements in the equation are intergers.Therefore, each $S$ should have at least 1 costs.Which means total cost of $\mathcal{L}$ $\ge$ $\left\vert E \right\vert$ since $\left\vert \mathcal{L} \right\vert$ $=$ $\left\vert E \right\vert$.However, total cost of $\mathcal{L}$ $&lt;$ $\left\vert E \right\vert$ is true because of following reasons.Each edge passes at most $x_e + x_e + (1 - 2x_e) = 1$ cost to sets.Therefore, total cost of $\mathcal{L} \le \left\vert E \right\vert$.However, there is at least one edge that outgoing of $S$.If there is no such an edge, $S$ should contain every vertices and it means $S = V$.However if so, $\delta(S) = \emptyset$ because there is no edge between $S$ and $S - V = V - V = \emptyset$.Which means $S$ can’t be in $\mathcal{L}$ because $\chi_{\delta(S)} = \vec{0}$ can’t be the vector in the lineary indepdendent set.As a result, total cost of $\mathcal{L} &lt; \left\vert E \right\vert$.Therefore, it’s a contradiction with the fact above.As a result, assumption that “$0 &lt; x_e &lt; \frac{1}{2}$ for all $e \in E$” is false.Therefore, there should be at least $x_e$ shuch that $x_e \ge \frac{1}{2}$ or $x_e = 0$.However, if we think about the LP with removing $x_e$s such that $x_e = 0$ then solution of problem should be the same with before.At the same time, there should be at least one $x_e$ such that $x_e \ge \frac{1}{2}$ or $x_e = 0$.However, that should be the case of $x_e \ge \frac{1}{2}$ because we just removed every $x_e = 0$.Therefore, claim holds.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Approximation algorithm(8) - Integer multicommodity flows]]></title>
      <url>/algorithm/approximation/2021/04/04/Approximation-algorithm(8)/</url>
      <content type="text"><![CDATA[Integer multicommodity flow problem is a problem to avoiding congetstion in edges.It can be used at the field of circuit design because avoiding critical path is a typical problem in a circuit design.Now, let’s fomulating a problem.Let’s think about a given graph $G = (V,E)$ and pairs of vertices $\{s_i, t_i\} \subset V$ for $1 \le i \le k$.There is a set of paths $P_i$ which consists of feasible paths from $s_i$ to $t_i$.Let $P = \{P_1, P_2, \cdots, P_k\}$.Then, problem can be described like “Minimize $W$ such that $\sum\limits_{P \in P_i} x_P = 1$, $\sum\limits_{P: e \in P} x_P \le W$ $\forall e \in E$, $x_P \in \{0, 1\}$”.Notice that $\sum\limits_{P \in P_i} x_P = 1$ forces to choose one path and $\sum\limits_{P: e \in P} x_P \le W$ $\forall e \in E$ forces to limits each edges to have at most $W$ paths going through.Now, it can be relaxed to linear programming like follow.“Minimize $W$ such that $\sum\limits_{P \in P_i} x_P = 1$, $\sum\limits_{P: e \in P} x_P \le W$ $\forall e \in E$, $x_P \ge 0$”.Then we can make a solution that is in $O(\log{n})\operatorname{OPT}$ with high probability by randomized rounding.Algorithm is like follow.  Solve “Minimize $W$ such that $\sum\limits_{P \in P_i} x_P = 1$, $\sum\limits_{P: e \in P} x_P \le W$ $\forall e \in E$, $x_P \ge 0$”.  Choose $P \in P_i$ with the distribution of $P_i$.Notice that $\sum\limits_{P \in P_i} x_P = 1$ means that we don’t need a normalization for $P \in P_i$.Now, let’s think about the solution $x^\star_P, W^\star$ which is given from the algorithm.If we define a probability that a number of edges go through edge $e$ is greater than $x$ as $Pr[Y_e &gt; x]$,$E[Y_e]$ $=$ $\sum\limits_{i = 1}^k \sum\limits_{P \in P_i, e \in P} x^\star_P$ $=$ $\sum\limits_{e \in P} x^\star_P$ $\le$ $W^\star$.Notice that $x^\star_P$ is an independent random variable.If we think about chernoff bounds, we can get follow.$Pr[Y_e \ge (1 + \delta)c\ln{n}W^\star]$ $&lt;$ $e^{-c\ln{n}W^\star\delta^2/3}$.Notice that $\mu$ $=$ $E[Y_e]$ $\le$ $c\ln{n}W^\star$.Now, $W^\star \ge 1$ is not a hard assumption from the requirement of problem.If we select $\delta = 1$, $Pr[Y_e \ge (1 + \delta)c\ln{n}W^\star]$ $&lt;$ $e^{-c\ln{n}W^\star\delta^2/3}$ $=$ $e^{-c\ln{n}/3}$ $=$ $n^{-c/3}$.As a result, $Pr[\max(Y_e) \ge (1 + \delta)c\ln{n}W^\star]$ $\le$ $\sum\limits_{e \in E}Pr[Y_e \ge (1 + \delta)c\ln{n}W^\star]$ $\le$$\sum\limits_{e \in E}n^{-c/3}$ $=$ $\left\vert E \right\vert n^{-c/3}$ $\le$ $n^{2-c/3}$.As a result, we can select $c$ to make resonable probability with resonable maximum $Y_e$.If $W^\star \ge c\ln{n}$, we can select $\delta = \sqrt{\frac{c\ln{n}}{W^\star}} \le 1$ then$Pr[Y_e \ge (1 + \delta)W^\star]$ $&lt;$ $e^{-W^\star\delta^2/3}$ $=$ $e^{\frac{-W^\star c\ln{n}}{3W^\star}}$ $=$$e^{-c\ln{n}/3}$ $=$ $n^{-c/3}$.As a result, $Pr[\max(Y_e) \ge (1 + \delta)W^\star]$ $=$$Pr[\max(Y_e) \ge (1 + \sqrt{\frac{c\ln{n}}{W^\star}})W^\star]$ $=$$Pr[\max(Y_e) \ge W^\star + \sqrt{c\ln{n}W^\star}]$ $\le$$\sum\limits_{e \in E}Pr[Y_e \ge W^\star + \sqrt{c\ln{n}W^\star}]$ $\le$$\sum\limits_{e \in E}n^{-c/3}$ $=$$\left\vert E \right\vert n^{-c/3}$ $\le$$n^{2-c/3}$What about the running time of the algorithm?We can’t solve the problem above in polynomial time because the number of paths is NP.As a result, the number of variable is NP either.Therefore, we can’t even see the whole variable.Therefore, algorithm above will be changed to equivalent algorithm.Algorithm is like follow.  Solve “Minimize $W$ such that $\sum\limits_{i = 1}^{k} f_i(u, v) + f_i(v, u) \le W$, $\forall (u, v) \in E$ and $f_i$ should be a flow of value 1 from $s_i$ to $t_i$”.  Choose path $p_i$ by choosing a vertex $u$ that adjacent to $s_i$ with probability $f_i(s_i, u)/\sum\limits_{u : u\text{ is adjacent to }s_i}f_i(s_i, u)$.  Extend path $p_i$ by choosing a vertex $u$ that adjacent to the last extended vertex $v$ with probability of $f_i(v, u)/\sum\limits_{u : u\text{ is adjacent to }v}f_i(v, u)$.Notice that step 1 is equal with “Minimize $W$ such that $\sum\limits_{i = 1}^{k} f_i(u, v) + f_i(v, u) \le W$, $\forall (u, v) \in E$, $\sum_{u : (u,v) \in E, v \neq \{s_i, t_i\}}f_i(u, v)$ $=$ $\sum_{w : (v,w) \in E, v \neq \{s_i, t_i\}}f_i(v, w)$, $\sum_{v : (s_i, v) \in E}f_i(s_i, v) - \sum_{v : (v, s_i) \in E}f_i(v, s_i)$ $=$ $\sum_{v : (v, t_i) \in E}f_i(v, t_i) - \sum_{v : (t_i, v) \in E}f_i(t_i, v)$ $=$ $1$”.Now it takes polynomial time to solve.The number of variable is $O(k\left\vert E \right\vert)$.The number of first contraint “$\sum\limits_{i = 1}^{k} f_i(u, v) + f_i(v, u) \le W$” is at most $O(\left\vert E \right\vert)$.The number of second contraint “$\sum_{u : (u,v) \in E, v \neq \{s_i, t_i\}}f_i(u, v)$ $=$ $\sum_{w : (v,w) \in E, v \neq \{s_i, t_i\}}f_i(v, w)$” is at most $O(k\left\vert E \right\vert|V|)$.The number of Third contraint “$\sum_{v : (s_i, v) \in E}f_i(s_i, v) - \sum_{v : (v, s_i) \in E}f_i(v, s_i)$ $=$ $\sum_{v : (v, t_i) \in E}f_i(v, t_i) - \sum_{v : (t_i, v) \in E}f_i(t_i, v)$ $=$ $1$” is at most $O(k\left\vert E \right\vert)$.Then, why this algorithm makes the same situation of previous algorithm?Let’s recap the previous algorithm.“Minimize $W$ such that $\sum\limits_{P \in P_i} x_P = 1$, $\sum\limits_{P: e \in P} x_P \le W$ $\forall e \in E$, $x_P \ge 0$”If we can select $x_P$, we can make a flow of value $x_P$ following $P$.Then, we can decompose such flow to paths.Notice that we can make a flow without cycle that makes better solution than flow with cycle because cycle will not change a value of flow but increase the congestion of edges.As a result, problem above can be changed to find a flows of paths and it is the same problem we’ve just changed. In other word, proof can be easily summariezed to “New algorithm finds a feasible solution of original algorithm because we can compose flow from paths.In the same time, old algorithm can find a feasible solution of new algoroithm because we can decompose flow to paths.”]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Chernoff bounds]]></title>
      <url>/algorithm/approximation/2021/04/03/Chernoff-bounds/</url>
      <content type="text"><![CDATA[Let $X_1, X_2, \cdots, X_n$ be $n$ independent varaible which are between $0$ and $1$.Then, $Pr[X \ge (1 + \delta)U] \lt (\frac{e^{\delta}}{(1 + \delta)^{1 + \delta}})^U$,$Pr[X \le (1 - \delta)L] \lt (\frac{e^{-\delta}}{(1 - \delta)^{1 - \delta}})^L$.For $X = \sum\limits_{i=1}^n X_i$, $\mu = E[X]$, $L \le \mu \le U$ and $\delta &gt; 0$.If $0 \le \delta &lt; 1$ then $Pr[X \ge (1 + \delta)U] &lt; (\frac{e^{\delta}}{(1 + \delta)^{1 + \delta}})^U &lt; e^{-U\delta^2/3}$,$Pr[X \le (1 - \delta)L] &lt; (\frac{e^{-\delta}}{(1 - \delta)^{1 - \delta}})^L &lt; e^{-L\delta^2/2}$Proof will be updated later.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Network decomposition]]></title>
      <url>/algorithm/network/2021/04/02/Network-flow-algorithm/</url>
      <content type="text"><![CDATA[A graph is a typical data structure which is used to represent some relations between nodes.From any graph, we can imagine a flow of a graph $G$.Formally, flow of the graph is defined like follow.For a given graph $G= (V, E)$, flow $f$ is a mapping function $V \times V \rightarrow \mathcal{R}$.To be a flow, there are some requirements.If there is a capacity $c$ for edeges $c : V \times V \rightarrow \mathcal{R}$, which $(u,v) \in E$ iff $c(u,v)$ exists.Then flow of graph can’t exceed that capacity.Which means $f(u,v) \le c(u, v)$.Also it requires two variables which are source $s$ and destination $t$.Then $\sum\limits_{(i, v) \in E, v \neq s,t}f(i, v) = \sum\limits_{(v, j) \in E, v \neq s,t}f(v, j)$.Which means that every value goes in $v$ should go out from $v$ either.Then we can evaluate this flow $f$ as $\sum\limits_{(s, v) \in E}f(s, v) - \sum\limits_{(v, s) \in E}f(v, s)$ $=$ $\sum\limits_{(v, t) \in E}f(v, t) - \sum\limits_{(t, v) \in E}f(t, v)$.Now, let’s think about a directed graph $G = (V,E)$ and some flow $f$ of $G$.Then we can decompose $f$ to some simple paths $P_1, \cdots, P_N$ and some cycles $C_1, \cdots, C_M$.Which $N + M \le \left\vert E \right\vert$.Without loosing much of generality, we can assume that $f$’s value is positive or negative.In the proof of flow decomposition, it doesn’t necessarily to be positive or negative.However, let’s assume it to be positive for easy.Then, if $f$ has a cycle $C$, we can eliminate that $C$ from $f$ without changing value of the flow because it is a cycle.If we reduce every value of edges with smallest value inside of the $C$ then we can remove that cycle.Notice that we can reduce value of edges because we picked the smallest value inside of the $C$,cycle should be disappear after reducing values because one of edge in $C$ should be 0 after reducing values.If we remove every cycle, it should have some paths because it has a flow.If we remove any flow that has the smallest value, value of flow $f$ should be 0 or some positive value.If it is the first case (flow of $f$ become 0) then it is decomposed.Otherwise, we can do this process again.Therefore, there sould be at least some cycles and paths.Notice that we should remove at least 1 edge to remove a path or a cycle because it choosed a smallest value on graph.As a result, we can do this at most $\left\vert E \right\vert$.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> network </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Approximation algorithm(7) - Minimizing sum of completion time]]></title>
      <url>/algorithm/approximation/2021/03/26/Approximation-algorithm(7)/</url>
      <content type="text"><![CDATA[Unweighted versionMinimizing the sum of completion time is a typical scheduling problem.Let’s think that there are $n$ works to be done.With these works, let $r_1, r_2, \cdots, r_n$ as release dates and $p_1, p_2, \cdots, p_n$ as processing times.Now we will schedule that $n$ works in some order.With that order, let’s define $C_1, C_2, \cdots, C_n$ as times that each work finished.The problem is to find a schedule that minimize $\sum\limits_{i=1}^n C_i$.Notice that this machine is a nonpreemptively which means each work can’t interrupt other works.Now we can do a relaxation at this problem by just changing nonpreemptive machine to preemptive machine which means each work may interrupt other works.If we change it to the preemptive machine, we can solve this problem with an algorithm below.    $\operatorname{while}$ there is any work not finished.        $S \leftarrow \{$ work $i$ which didn't finished yet and $r_i \le$ current date $\}$.        Choose a work $i \in S$ that has the smallest left processing time.        Process work $i$ until work $i$ is done or new $r_j$ release.    This algorithm is so-called SRPT(shorted remaining processing time) rule.This algorithm works in a polynomial time because we can update a left processing time and get a minimum processing time by heap.We also can check whether new $r_j$ released or not by just sorting $r_i$ and checking it later.However, notice that schedule from preemptive machine can’t be run on the nonpreemptive machine.Therefore, we need to make a strict order for the execution.Therefore, we will reorder the index of works to the order of $C_i^P$ which $C_i^P$ is the termination time of preemptive machine.Now after this reordering, we will just do work in the order of $1,2,\cdots,n$.After this reordering, it will result in $2$-approximation algorithm.The proof is like follow.Let $C_i^N$ is the termination time of nonpreemptive machine.Since, schedule of nonpreemptive machine can be run on preemptive machine, $\sum\limits_{i=1}^n C_i^P \le \operatorname{OPT}$ such that $\operatorname{OPT}$ is an optimal solution.Now, there is two simple facts that $C^P_i \ge \max\limits_{k=1,\cdots,i}r_k$ and $C^P_i \ge \sum\limits_{k=1}^i p_k$ because it is reordered in the order of $C^P_i$.Notice that if work $j$ is done then every work $i &lt; j$ should be done from the definition.Now, let’s think about work $i$ in the nonpreemptive machine.Then we can know that $i$ should finished at most $\max\limits_{k=1,2,\cdots,i}r_k$ $+$ $\sum\limits_{k=1}^i p_k$.The reason is that work $i$ can only be idle because work $1, 2, \cdots, i - 1$ are still working.Therefore, $C^N_i$ $\le$ $\max\limits_{k=1,2,\cdots,i}r_k$ $+$ $\sum\limits_{k=1}^i p_k$ $\le$ $2C^P_i$.Which means $\sum\limits_{i=1}^n C^N_i$ $\le$ $2\sum\limits_{i=1}^n C^P_i$ $\le$ $2\operatorname{OPT}$.Weighted versionThe weighted version of the problem is simillar with the unweighted version.Let’s think that there are $n$ works to be done.With these works, let $r_1, r_2, \cdots, r_n$ as release dates, $p_1, p_2, \cdots, p_n$ as processing times and $w_1, w_2, \cdots, w_n$ as weights of each works.Notice that $w_i \ge 0$ $\forall 1 \le i \le n$Now we will schedule that $n$ works in some order.With that order, let’s define $C_1, C_2, \cdots, C_n$ as times that each work finished.The problem is to find a schedule that minimize $\sum\limits_{i=1}^n w_i C_i$.Notice that this machine is a nonpreemptively which means each work can’t interrupt other works.Now, we can’t solve problem with preemptive machine unlike unweighted verison of the problem in polynomail time else P is NP.Therefore, we will use a relaxation with linear programming in it.Relaxation of this problem is like below.Minimize $\sum\limits_{i=1}^n w_i C_i$ such that $C_i \ge r_i + p_i$ $\forall i \in N$ and $\sum\limits_{i \in S} p_iC_i \ge \frac{1}{2}\rho(S)^2$ $\forall S \subset N$ which $N = \{1, 2, \cdots, n$}.Objective function is trivial to be “Minimize $\sum\limits_{i=1}^n w_i C_i$” and first constraint is trivial either that “$C_i \ge r_i + p_i$ $\forall i \in N$” from the meaning.How about the second constraint?If you think about any $S \subset N$, each works in $S$ will be ordered in an optimal solution.Therefore, we can imagine that $S^\star$ is a ordered list in the order of an optimal solution from $S$.Then,         $\sum\limits_{i \in S} p_iC_i$         $=$ $\sum\limits_{i \in S^\star} p_iC_i$         $\ge$ $\sum\limits_{i,j \in S^\star, j \le i} p_ip_j$         $=$ $\frac{1}{2}\sum\limits_{i,j \in S^\star, j \le i} p_ip_j$ + $\frac{1}{2}\sum\limits_{i,j \in S^\star, j \le i} p_ip_j$        $=$ $\frac{1}{2}\sum\limits_{i,j \in S^\star, j \le i} p_ip_j$ + $\frac{1}{2}\sum\limits_{j,i \in S^\star, i \le j} p_jp_i$        $=$ $\frac{1}{2}\sum\limits_{i,j \in S^\star, j \le i} p_ip_j$ + $\frac{1}{2}\sum\limits_{j,i \in S^\star, i &lt; j} p_jp_i$ $+$ $\frac{1}{2}\sum\limits_{i \in S^\star} p_ip_i$        $=$ $\frac{1}{2}\sum\limits_{i,j \in S^\star,} p_ip_j$ + $\frac{1}{2}\sum\limits_{i \in S^\star} p_i^2$         $=$ $\frac{1}{2}(\sum\limits_{i \in S^\star} p_i)^2$ + $\frac{1}{2}\sum\limits_{i \in S^\star} p_i^2$         $\ge$ $\frac{1}{2}(\sum\limits_{i \in S^\star} p_i)^2$.Now let $\rho(S) = \sum\limits_{i \in S} p_i$ then constraint holds.Now, if we can solve that linear problem in polynomial time.We can make a strict order by termination time $C_i$ like unweighted version did.Then that solution should be in $3\operatorname{OPT}$.Proof is like follow.Let’s define $C^L_i$ as a termination time founded by linear programming.However we can sort that works in the order of $C^L_i$ so let’s assume so.Which means $C^L_1$ $\le$ $C^L_2$ $\le$ $C^L_3$ $\le$ $\cdots$ $\le$ $C^L_n$.Let’s define one more solution $C^R_i$ as a termination time of nonpreemptive machine’s work $i$ with processing order in $C^L_i$.Then, we can get 2 facts like unweighted version did and we can know other two facts below With these facts.  $\sum\limits_{i = 1}^nC^L_i \le \operatorname{OPT}$  $C^R_i$ $\le$ $\max\limits_{k=1,2,\cdots,i}r_k$ $+$ $\sum\limits_{k=1}^i p_k$  $\max\limits_{k=1,2,\cdots,i}r_k  \le C^L_i$  $\frac{1}{2}\sum\limits_{k=1}^i p_k = \frac{1}{2}\rho(\{1,2,\cdots,i\}) \le C^L_i$Third fact is a trivial because $C^L_i$ is ordered in $i$ and this means $C^L_i \ge C^L_j$ $\forall 1 \le j \le i$ and from the constraint $C^L_i \ge C^L_j \ge r_j$ $\forall 1 \le j \le i$.Simillary, fourth fact can be proven in following way.From the constaint, $\frac{1}{2}\rho(\{1,2,\cdots,i\})^2$ $\le$ $\sum\limits_{j = 1}^{i} p_jC^L_j$.Then, $\frac{1}{2}\rho(\{1,2,\cdots,i\})^2$ $\le$ $\sum\limits_{j = 1}^{i} p_jC^L_j$ $\le$ $\sum\limits_{j = 1}^{i} p_jC^L_i$ $=$ $C^L_i\sum\limits_{j = 1}^{i} p_j$ $=$ $C^L_i\rho(\{1,2,\cdots,i\})$.As a result, $\frac{1}{2}\rho(\{1,2,\cdots,i\})$ $\le$ $C^L_i$ by dividing $\rho(\{1,2,\cdots,i\})$ for each side of $\frac{1}{2}\rho(\{1,2,\cdots,i\})^2$ $\le$ $C^L_i\rho(\{1,2,\cdots,i\})$.Therefore, fact 3, 4 has been proven.Which means $C^R_i$ $\le$ $\max\limits_{k=1,2,\cdots,i}r_k$ $+$ $\sum\limits_{k=1}^i p_k$ $\le$ $C^L_i + 2C^L_i$ $=$ $3C^L_i$.Therefore, $\sum\limits_{i=1}^n C^R_i$ $\le$ $\sum\limits_{i=1}^n 3C^L_i$ $\le$ $3\operatorname{OPT}$We didn’t discussed about the running time and it look exponential because the number of contraints is exponential.However, we still can solve this problem in the polynomial time with ellipsoid method which doesn’t bounded in the number of contraints.To solve this problem with ellipsoid method, we need a seperation oracle that runs in the polynomial time and checks whether a solution is a feasible or not.If we have a seperation oracle, ellipsoid method can solve the linear programming in $O(poly(log u, n))$ which $u$ is bit in use for saving the data.However we need to shirnk the number of constraint to check because it is exponential of number of variables.Therefore, we claim below.“Let’s define $S_1 = \{1$}, $S_2 = \{1, 2$}, $\cdots$, $S_n = \{1,2,\cdots,n$}.If every variable satisfies constraints for all $S_i$s then that solution statisfies all constraints.”Before proving this, we will check brief 2 facts below.Notice that S is an arbitrary set in $N$, $a$ is in $S$ and $b$ isn’t in S.  $\rho(S)^2 - \rho(S - \{a\})^2$ $=$ $2p_a\rho(S - \{a\}) + p_a^2$ $=$ $p_a(2\rho(S - \{a\}) + p_a)$  $\rho(S + \{b\})^2$ - $\rho(S)^2$ $=$ $2p_b\rho(S) + p_b^2$ $=$ $p_b(2\rho(S) + p_b)$Proof is simple that $\rho(S)^2$ $=$ $\rho(S- \{a\} + \{a\})^2$ $=$ $(\rho(S- \{a\}) + p_a)^2$ $=$ $\rho(S - \{a\})^2 + 2p_a\rho(S - \{a\}) + p_a^2$ and simillar for fact 2.Now, let’s think about $f(S) = \sum\limits_{i \in S}p_iC_i - \frac{1}{2}\rho(S)$.Then with the fact above, $f(S - \{a\}) - f(S)$ $=$ $-p_aC_a + \frac{1}{2}(\rho(S)^2 - \rho(S - \{a\})^2)$ $=$ $-p_aC_a + \frac{1}{2}p_a(2\rho(S - \{a\}) + p_a)$ $=$ $p_a(-C_a + \rho(S - \{a\}) + \frac{1}{2}p_a)$.Simillary $f(S + \{b\}) - f(S)$ $=$ $p_bC_b -\frac{1}{2}(\rho(S + \{b\}) - \rho(S))$ $=$ $p_bC_b - \frac{1}{2}p_b(2\rho(S) + p_b)$ $=$ $p_b(C_b - \rho(S) - \frac{1}{2}p_b)$.As a result, removing $a$ from $S$ will decrease $f(S)$ if $C_a &gt; \rho(S - \{a\}) + \frac{1}{2}p_a$ andadding $b$ to $S$ will decrease $f(S)$ if $C_b &lt; \rho(S) + \frac{1}{2}p_b$.Now let’s assume that every variable satisfies for all $S_i$s but there is some constraint that doesn’t matches.Which means $\sum\limits_{j \in X}p_jC_j &lt; \frac{1}{2}\rho(X)^2$.Notice that $f(X) &lt; 0$.Now, let’s remove the biggest $j$ in $X$ if removing $j$ decreases $f(X)$.Let’s define $S_h$ as the termination of such an operation.Then, it will stop when $C_h \le \rho(S_h - \{h\}) + \frac{1}{2}p_h$ for the biggest $j = h$ in $X$.However we can add any $1 \le j &lt; h$ to decrease $f(S_h)$ because $C_j$ $\le$ $C_h$ $\le$ $\rho(S_h - \{h\}) + \frac{1}{2}p_h$ $&lt;$ $\rho(S_h - \{h\}) + p_h$ $=$ $\rho(S_h)$ $&lt;$ $\rho(S_h) + \frac{1}{2}p_j$.If we define $S_e=\{1,2,\cdots,h$} then, $f(S_e) \le f(S_h) \le f(X) &lt; 0$.However, it can’t be true because $f(S_e) = \sum\limits_{i = 1}^e p_iC_i - \frac{1}{2}\rho(S_e) \ge 0$ from the algorithm.As a result, we don’t need to see all the contraint but only for $S = \{1,2,\cdots,i$} is enough.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Linear programming]]></title>
      <url>/algorithm/lp/2021/03/24/Linear-programming/</url>
      <content type="text"><![CDATA[Linear programming is constructed with two factors.  Objective function which is a mizimizing/maximizing target.  Constraints Which are requirements for the problem.For example, minimize $2x + 5y + 6z$ such that $3x + 4y + 3z \ge 5$, $x + y \ge 10, x,y,z \ge 0$.There are two major methods to solve this.Simplex methodSimplex algorithm is based on the fact that an optimal solution exists on one of the intersection of contraints or on one of constarint itself.The reason is that a fesaible soltuion space is a convex and problem is a linear combination of axis.    For a given linear programming problem, choose a feasible solution $V = (v_1, v_2, \cdots, v_n)$.    $\operatorname{while}$ moving $V$ following one of intersection of contraints makes $v$ better.        Move $V$ to any way which makes $V$ better.         Return $V$.In practice, it is a fast algorithm but it can’t be guaranteed to be run in the polynimal time.Therefore, there is an alternative method for it.Ellipsoid methodElliposid algorithm uses so-called a partition orcale.Partition orcale is an orcale that tells us whether a solution is feasible or not in polynoimal time.If we have such an orcale, we can bipartite solution space to two seperate spaces.One has an optimal solution and the other has lefts.With this, we can solve any linear problem in polynomial time by the ellipsoid method.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> LP </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Approximation algorithm(6) - Minimum-degree spanning tree]]></title>
      <url>/algorithm/approximation/2021/03/24/Approximation-algorithm(6)/</url>
      <content type="text"><![CDATA[Minimum-degree spanning tree problem is a problem to minimize the degree of the spanning tree from given graph.Formally, problem is like below.Given a graph $G = (V,E)$, minimized the $\max_{v \in V} deg(v)$ from a spanning tree $T$ of $G$.Notice that this is a NP-hard problem.There is an approximation algorithm for this problem.Approximation algorithm will work with a local search.Let $l = \left[ log_2 n \right]$, $d_T(u)$ denotes the degree of $u$ in $T$ and $\Delta(T) = max_{u \in V} d_T(u)$    Make an arbitrary spanning tree of $G$.    $\operatorname{while} \exists (v,w) \not\in T.E$ but $(v,w) \in G.E$        such that there is a $u$ such that $u$ is a vertex in a cycle $C$ of $T.E$ + $(v,w)$,            $d_T(u) \ge \Delta(T) - l$ and $max(d_T(v), d_T(w)) \le d_T(u) - 2$        $T \leftarrow T + (v,w) - $ an edge which is incident to $u$ in a cycle $C$.        return $T$From the algorithm above, it guarantess that a local search will imporve the solution and it will terminates.Notice that “$max(d_T(v), d_T(W)) \le d_T(u) - 2$” gurantees that “a local search will imporve the solution” and“$\operatorname{while} \exists (v,w) \not\in T.E$ but $(v,w) \in G.E$        such that there is a $u$ such that $u$ is a vertex in a cycle $C$ of $T.E$ + $(v,w)$,            $d_T(u) \ge \Delta(T) - l$ and $max(d_T(v), d_T(w)) \le d_T(u) - 2$” gurantees that “it will terminates”.Now, let’s prove that $\Delta(T) \le 2\operatorname{OPT} + l$ which $\operatorname{OPT}$ is an optimal solution.Let’s assume that we removes $k$ edges from any spanning tree $T$ and re-connecting them again to be a connected graph.Notice that it will become $k + 1$ distinct components after removing $k$ edges because it is a tree.Now, let’s define the set of one end point of inter-connecting edges between distinct components to $S$.Then, we have $\operatorname{OPT} \ge \frac{k}{|S|}$.The reason is like follow.It should be connected after re-connecting them again and this requires $k$ edges at least.However, edges can be choosesn in the $S$ only.Which means each components need to have at least $\frac{k}{|S|}$ edges.Now, let’s think about an output of algorithm $T$ from above.Let $S_i$ denote the set of nodes of $deg(v) \ge i$ in $T$.Then there is some $i^{\star}$ $\ge$ $\Delta(T) - l + 1$ such that $|S_{i^{\star}-1}|$ $\le$ $2|S_i^{\star}|$.The reason is like follow.If we don’t have any $i^{\star}$ such fits above then $|S_{\Delta(T)} - l|$ $&gt;$ $2|S_{\Delta(T)} - l + 1|$ $&gt;$ $2^2|S_{\Delta(T)} - l + 2|$ $&gt;$ $\cdots$ $&gt;$ $2^l|S_{\Delta(T)}|$.However it can’t be ture because $|S_{\Delta(T)} - l|$ $&gt;$ $2^l|S_{\Delta(T)}|$ $\ge$ $2^l$ $\ge$ $n$ from the fact “$|S_{\Delta(T)}| \ge 1$”.Now, let’s think about $i \ge \Delta(T) - l + 1$, then we have at leat $(i - 1)|S_i| + 1$ distinct edges of $T$ which are incident to $v \in S_i$.The reason is like follow.Since, they have at least $i$ edges for each node.They will have at least $i|S_i|$ edges.However, they may have some edges in share which is at most $|S_i| - 1$ because it is a tree.Therefore, they need to have at least $i|S_i| - (|S_i| - 1)$ $=$ $(i - 1)|S_i| + 1$ edges.Now, let’s think about the situation that we remove all the edges which are incident to $v \in S_i$ from $T$ then all vetices which one end point of vertex’s edge is connecting different components are in $S_{i-1}$.If any edges that connecting different components doesn’t incident to $S_{i-1}$ then it should have less than $i - 2$ degree which means that it should be removed from the algorithm.Notice that we can make a cycle with this edge and one of edges in $S_i$ because each componenets are connected in itself.From the facts above, $\operatorname{OPT}$ $\ge$ $\frac{k}{\mid S\mid}$ $\ge$ $\frac{(i^{\star}-1)\mid S_{i^{\star}} + 1\mid}{\mid S_{i^{\star}-1} \mid}$ $\ge$ $\frac{(i^{\star}-1)\mid S_{i^{\star}} + 1 \mid}{2\mid S_{i^{\star}}\mid}$ $\ge$ $\frac{i^{\star}-1}{2}$ $\ge$ $\frac{\Delta(T) - l}{2}$.Let’s prove that this algorithm runs in polynomial time.Let’s define a potential function $\Phi$ of tree $T$ such that $\Phi(T)$ $=$ $\sum\limits_{v \in V} 3^{d_T(v)}$.Notice that $n$ $&lt;$ $\Phi(T)$ $\le$ $n3^{\Delta(T)}\le n3^n$.After doing a local search once, degree of $v$ and $w$ will increase and $u$ will decrease.Let’s assume that $u$ has a degree $i \ge \Delta(T) -l$.As a result, potential of $T$ will increase no more than $2(3^{i-1} - 3^{i-2})$ $=$ $4 \times 3^{i-2}$ and will decrease $3^i - 3^{i - 1} = 2 \times 3^{i - 1}$.Which means potential of $T$ will decrease at least $\frac{2}{9}3^i$ $\ge$ $\frac{2}{9}3^{\Delta(T) - l}$ $=$ $\frac{2}{9 \times 3^{l}}3^{\Delta(T)}$ $\ge$ $\frac{2}{27 \times 3^{log_2 n}}3^{\Delta(T)}$ $\ge$ $\frac{2}{27 \times 4^{log_2 n}}3^{\Delta(T)}$ $=$ $\frac{2}{27n^2}3^{\Delta(T)}$ $\ge$ $\frac{2}{27n^3}\Phi(T)$.Now, potential decrase at least by a factor of $(1-\frac{2}{27n^3})$ at each iteration.From the fact above, potential function will be $n$ after at most $\frac{27}{2}n^4 ln 3$ iteration because $(1-\frac{2}{27n^3})^{\frac{27}{2}n^4 ln 3} \times (n3^n)$ $\le$ $e^{-n ln 3} \times (n3^n)$ $=$ $n$.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Parallel Page rank/BFS]]></title>
      <url>/algorithm/concurrency/graph/2021/03/19/Parallel-Page-rank-and-BFS/</url>
      <content type="text"><![CDATA[Page rankPage rank is an algorithm to determine which vertices are important and which aren’t.For each iteration, each vertex will be updated by nearby vertices.Let’s define some terms.  $r_i^t$ as the value of vertex $i$ at the $t$th iteration such that $\sum\limits_{i \in V}r_i^0 = 1$.  $\delta(i)$ as the set of neighbor vetices of vertex $i$.  $\beta$ is a convergance ratio which means $0 \le \beta \le 1$.With terms above, $r_i^{t + 1}$ $=$ $\beta\sum\limits_{j \in \delta(i)}\frac{r_j^t}{|\delta(j)|}$ $+$ $(1 - \beta)\frac{1}{|V|}$.If we keep compute this, it will converges to some values and that is the result of the page rank.Notice that if we have $\sum\limits_{i \in V}r_i^t$ $=$ $1$ then it will keep consistant after $t$th iteration either.Proof is like follow.$\sum\limits_{i \in V}r_i^{t + 1}$$=$ $\sum\limits_{i \in V}(\beta\sum\limits_{j \in \delta(i)}\frac{r_j^t}{|\delta(j)|}$ $+$ $(1 - \beta)\frac{1}{|V|})$$=$ $\beta\sum\limits_{i \in V}\sum\limits_{j \in \delta(i)}\frac{r_j^t}{|\delta(j)|}$ $+$ $(1 - \beta)\sum\limits_{i \in V}\frac{1}{|V|}$$=$ $\beta\sum\limits_{(i,j) \in E}\frac{r_j^t}{|\delta(j)|}$ $+$ $(1 - \beta)\frac{|V|}{|V|}$$=$ $\beta\sum\limits_{j \in V}\sum\limits_{i \in \delta(j)}\frac{r_j^t}{|\delta(j)|}$ $+$ $(1 - \beta)$$=$ $\beta\sum\limits_{j \in V}\frac{r_j^t}{|\delta(j)|}\sum\limits_{i \in \delta(j)}1$ $+$ $(1 - \beta)$$=$ $\beta\sum\limits_{j \in V}\frac{r_j^t}{|\delta(j)|}|\delta(j)|$ $+$ $(1 - \beta)$$=$ $\beta\sum\limits_{j \in V}r_j^t$ $+$ $(1 - \beta)$$=$ $\beta$ $+$ $(1 - \beta)$ $=$ $1$.    $\operatorname{for} i \leftarrow 0,\cdots,|V| - 1$        $s[i] \leftarrow 1/|V|$        $\operatorname{while} error \le threshold$        $error \leftarrow 0$        $\operatorname{for} v \leftarrow 0,\cdots,|V| - 1$            $o[v] \leftarrow s[v]$            $s[v] \leftarrow 0$                $\operatorname{for} v \leftarrow 0,\cdots,|V| - 1$            $\operatorname{for} u \in \delta(v)$                $s[v] \leftarrow s[v] + \beta \frac{o[u]}{|\delta(u)|}$                        $s[v] \leftarrow s[v] + (1 - \beta) \frac{1}{|V|}$            $error \leftarrow error + |o[v] - s[v]|$            If we see the algorithm above, it is known to be parallelizable for each $\operatorname{for}$ loops because there is no dependancy between each $\operatorname{for}$ loops.As a result, page rank can be parallelized like below.    $\operatorname{for} i \leftarrow 0,\cdots,|V| - 1 \operatorname{in} \operatorname{parallel}$        $s[i] \leftarrow 1/|V|$        $\operatorname{while} error \le threshold$        $error \leftarrow 0$        $\operatorname{for} v \leftarrow 0,\cdots,|V| - 1 \operatorname{in} \operatorname{parallel}$            $o[v] \leftarrow s[v]$            $s[v] \leftarrow 0$                $\operatorname{for} v \leftarrow 0,\cdots,|V| - 1 \operatorname{in} \operatorname{parallel}$            $\operatorname{for} u \in \delta(v) \operatorname{in} \operatorname{reduction}$                $s[v] \leftarrow s[v] + \beta \frac{o[u]}{|\delta(u)|}$                        $s[v] \leftarrow s[v] + (1 - \beta) \frac{1}{|V|}$                $\operatorname{for} v \leftarrow 0,\cdots,|V| - 1 \operatorname{in} \operatorname{reduction}$            $error \leftarrow error + |o[v] - s[v]|$            There are two major parallelization techniques used above.  $\operatorname{for} \operatorname{in} \operatorname{parallel}$  $\operatorname{for} \operatorname{in} \operatorname{reduction}$$\operatorname{for} \operatorname{in} \operatorname{parallel}$ means that it can run in the fully parallelized manner.$\operatorname{for} \operatorname{in} \operatorname{reduction}$ means that it can be run in the fully parallelized manner but it will collect data in the tree order.First technique usually used for operations that have no dependancies between.Second technique usually used for there is some dependancies for data but it will accumulates outputs only.Notice that it is easy to check about the dependancies by reading the algorithm above and there may exists read dependancy but there is no write dependancy.BFSBFS is a search algorithm which reads vertices from source.It doesn’t need to be a specific algorithm.It may updates the edge distance from source, it may finds component by checking boolean variable of vertices.In this case, let’s assume that BFS works for set a value of vertex to a distance from the source.For do this, let’s define $\delta(v)$ as the set of neighbor of $v$.    $\operatorname{for} i \leftarrow 0, \cdots, |V| - 1$        $d[i] = -1$        $Q \leftarrow \emptyset$    $Q.push(s)$    $d[s] = 0$    $\operatorname{while} Q \neq \emptyset$        $v \leftarrow Q_{top}$        $Q.pop()$        $\operatorname{for} u \in \delta(v)$            $\operatorname{if} d[u] = -1$                $d[u] = d[v] + 1$                $Q.insert(u)$                        Now, it can be parallelized per vetices at the same depth.    $\operatorname{for} i \leftarrow 0, \cdots, |V| - 1 \operatorname{in} \operatorname{parallel}$        $d[i] = -1$        $Q \leftarrow \emptyset$    $Q.push(s)$    $d[s] = 0$    $\operatorname{while} Q \neq \emptyset$        $Q^n \leftarrow \emptyset$        $\operatorname{for} v \in Q \text{ with the same depth } \operatorname{in} \operatorname{parallel}$            $\operatorname{for} u \in \delta(v)$                $\operatorname{if} d[u] = -1$                    Critical section                        $d[u] = d[v] + 1$                        $Q^n.insert(u)$                                                                $Q \leftarrow Q^n$    However, there are two problems at this approach.One is that we need a critical section to handle every thing.Therefore, it’s not good for many cases.We may can avoid critical section for $Q$ by using a local queue.However, we need critical section for $d[u]$ anyway.Also, there could be potentially many collision for $\delta(v)$.Therefore many of them is actually waste of performance.Therefore, there is another approach for BFS known as bottom-up-approach.To do this, let’s define $\delta(v)$ as incoming edges in this case.    $\operatorname{for} i \leftarrow 0, \cdots, |V| - 1 \operatorname{in} \operatorname{parallel}$        $d[i] = -1$        $Q \leftarrow \emptyset$    $Q.push(s)$    $d[s] = 0$    $\operatorname{while} \text{distance changed}$        $Q^n \leftarrow \emptyset$        $\operatorname{for} v \in V \operatorname{in} \operatorname{parallel}$            $\operatorname{if} d[v] = -1$                $\operatorname{for} u \in \delta(v)$                    $\operatorname{if} u \in Q$                        $d[v] = d[u] + 1$                        Critical section                            $Q^n.insert(v)$                                                                                        $Q \leftarrow Q^n$    In this case, we can utilize full computing unit because it can pass many collisons.Also, this can be parallelized better because there is no critical section to update each node’s depth at the moment.Therefore, this works well than before some times.However, it is known to be better to use both with some criteria.Therefore, there is a finite automaton to switching between these two.It starts with top-down approach first.If it has lower number of edges to explor then it switches to bottom-up approach.If it has few vertices to work then it switchs back to top-down approach.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> concurrency </category>
        
          <category> graph </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[MPI]]></title>
      <url>/concurrency/multi%20computer%20computing/2021/03/18/MPI/</url>
      <content type="text"><![CDATA[MPI is a programming model for parallel programming.It transfers data between any processing unit without detailed set-up.For example, let’s assume there are 4 machines with 16 core CPU.Then, it will have 64 cores in total.Then, MPI uses 64 cores with out any consideration but just equally.Notice that there is a bunch of difference in actual performance between data transfer.If we transfer between core in the same machine, it will be relative fast.If we transfer  between core in other machine, it will be relative slow.Following functions can be used for MPI programming.This post will be updated later with more information.Sendint MPI_Send(void *smessage, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm)It sends a message at smessage buffer with the size of count.You can specify the type of data by datatype.You must mark the rank of your target processor in dest.If you want to, you may use tag to check is this message valid or not.Also, you must pass comunity structure by comm.Receiveint MPI_Recv(void *rmessage, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status *status)It receives a message at rmessage buffer with the size of count.You can specify the type of data by datatype.You must mark the rank of your source processor in source.If you want to, you may use tag to check is this message valid or not.Also, you must pass comunity structure by comm.You can recieve the status of message by status.tagYou may use MPI_ANY_SOURCE as tag to receive a message with any tag.DeadlockMPI also can cause the deadlock.If every processor waits for other processor, it will just sutck in the moment.MPI_sendsMPI also supports asynchronous version of MPI_Send.Infact, MPI doesn’t guarantees whether is it asynchronous or synchronous function.To be specific there are two version for Send and Recv and we can select one of them to be used by function itself.Followings are such things; MPI_Bsend(), MPI_Rsend(), MPI_Ssend(), MPI_Isend().  MPI_Bsend() : (Buffered mode) Copy data to the buffer and return.  MPI_Rsend() : (Ready) Wait untill opponent’s recv() called then return.  MPI_Ssend() : (Synchronous)  Wait untill opponent’s recv() finished then return.  MPI_Send()  : (Base) Nothing guaranteed, depending on situation  MPI_Isend() : (Asynchronous) Non-blocking send. Making a sender thread and make a ticket to check it.Asynchronous sendint MPI_Isend(void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request *request)It’s almost the same with MPI_Send() but it uses request as a ticket to check whether it has been done or not.Asynchronous receiveint MPI_Irecv(void* buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Request *request)Like send operation, there is a asynchronous recv operation either.It’s almost the same with MPI_Recv() but it uses request as a ticket to check whether it has been done or not.Waitint MPI_Wait(MPI_Request *request, MPI_Status *status) Now, there is a wait function to wait for asynchronous operation ends.It uses ticket that we receives from asynchronous operations.Wait allint MPI_Waitall(int count, MPI_Request array_of_requests[], MPI_Status array_of_statuses[])It has a generalized version of MPI_Wait.It supports to check multiple number of tickets at once.Testint MPI_Test(MPI_Request *request, int *flag, MPI_Status *status)It has some trial verison of wait operation.It returns immediately and just tells you whether operation ended or not.Testallint MPI_Testall(int count, MPI_Request array_of_requests[], int *flag, MPI_Status array_of_statuses[])Test also have some generalized version.Notice that it has only a flag to handle this.Therefore, it will be all passed or not manner.ExampleMPI example is like follow.#include &lt;mpi.h&gt;int main(int argc, char* argv[]) {    //Initialize the MPI    MPI_Init(&amp;argc, &amp;argv);     //Get number of threads    int num_ranks;    MPI_Comm_size(MPI_COMM_WORLD, &amp;num_ranks);     //Get current program index    int rank;    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);    //Get processor name    int len;     char hostname[MPI_MAX_PROCESSOR_NAME];     MPI_Get_processor_name(hostname, &amp;len);    printf("hostname: %s, I am %d/%d\n", hostname, rank, num_ranks);    //Preparing buffers    constexpr int MSGLEN = 30;    char *recvbuf = new char[MSGLEN];    char *sendmsg = new char[MSGLEN];    //How many time to repeat    constexpr int REPEAT = 10;        //Each of them will pass index of this iteration as tag    for(int i=0; i&lt;REPEAT; i++) {         if(rank % 2==0) {//If it's even rank, it sends a message            sprintf(sendmsg, "hello from %s_%d #%d",hostname, rank, i); //Prepare the message            MPI_Send(sendmsg, MSGLEN, MPI_CHAR, rank + 1, i, MPI_COMM_WORLD);        }         else {//If it's odd rank, it receives a message            MPI_Status status;             MPI_Recv(recvbuf, MSGLEN, MPI_CHAR, rank ‐ 1, i, MPI_COMM_WORLD, &amp;status);             printf("#%d:recved %lld bytes : %s\n", rank, MSGLEN, recvbuf);//Received message        }    }    //Notice that this will work with only even number of worker.    //Terminates the MPI    MPI_Finalize();    return 0;}Now there are some design patterns to transfer data to be used.Therefore, MPI supports some of common designs.Broadcastint MPI_Bcast(void *message, int count, MPI_Datatype type, int root, MPI_Comm comm)                    Worker 1      Worker 2      Worker 3      Worker 4                  Before      A                                   After      A      A      A      A      Broadcast is a easiest way to send a data to all processors.It sends a message to all other processors at message.You can consider it as a synchronization step for whole processors.Reduceint MPI_Reduce(void *sendbuf, void *recvbuf, int count, MPI_Datatype type, MPI_Op op, int root, MPI_Comm comm)It’s a reduce function for all operations.It’s like accumulating datas with op.For example if op is MPI_SUM then, it will sum all the data in sendbuf.Followings are possible op types.  MPI_MAX : max operation  MPI_MIN : min operation  MPI_MAXLOC : max operation with indexing  MPI_MINLOC : min operation with indexing  MPI_SUM : sum operation  MPI_PROD : product operation  MPI_LAND : logical and operation  MPI_BAND : bit-wise and operation  MPI_LOR : logical or operation  MPI_BOR : bit-wise or operation  MPI_LXOR : logical xor operation  MPI_BXOR : bit-wise xor operation                   Worker 1      Worker 2      Worker 3      Worker 4                  Before      A      B      C      D              After      A op B op C op D      B      C      D      Scatterint MPI_Scatter(const void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm)It’s simillar with broadcast but it doesn’t sending them as it is but it sends all data in split to processors.                   Worker 1      Worker 2      Worker 3      Worker 4                  Before      A,B,C,D                                   After      A      B      C      D      Gatherint MPI_Gather(const void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm)It’s simillar with reduce but it doesn’t do anything but collects data from all other processors.                   Worker 1      Worker 2      Worker 3      Worker 4                  Before      A      B      C      D              After      A,B,C,D                           All Gatherint MPI_Allgather(const void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm)It’s some thing that equivalent with Gather and Scatter.It collects every data and scatter them to all.                   Worker 1      Worker 2      Worker 3      Worker 4                  Before      A      B      C      D              After      A,B,C,D      A,B,C,D      A,B,C,D      A,B,C,D      Reduce ScatterIt’s some thing that equivalent with doing reduce and scatter at the same time.It do reduce per data and scatter them at the same time.int MPI_Reduce_scatter(void *sendbuf, void *recvbuff, int *revcnt, MPI_Datatype type, MPI_Op op, MPI_Comm comm)                   Worker 1      Worker 2      Worker 3      Worker 4                  Before      $A_1,B_1,C_1,D_1$      $A_2,B_2,C_2,D_2$      $A_3,B_3,C_3,D_3$      $A_4,B_4,C_4,D_4$              After      $A_1$ op $A_2$ op $A_3$ op $A_4$      $B_1$ op $B_2$ op $B_3$ op $B_4$      $C_1$ op $C_2$ op $C_3$ op $C_4$      $D_1$ op $D_2$ op $D_3$ op $D_4$      All ReduceIt’s some thing that equivalent with doing reduce and broadcast at the same time.It do reduce per data and broadcast them at the same time.int MPI_Allreduce(const void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm)                   Worker 1      Worker 2      Worker 3      Worker 4                  Before      $A_1,B_1,C_1,D_1$      $A_2,B_2,C_2,D_2$      $A_3,B_3,C_3,D_3$      $A_4,B_4,C_4,D_4$              After      $A_1$ op $A_2$ op $A_3$ op $A_4$, $B_1$ op $B_2$ op $B_3$ op $B_4$, $C_1$ op $C_2$ op $C_3$ op $C_4$, $D_1$ op $D_2$ op $D_3$ op $D_4$      $A_1$ op $A_2$ op $A_3$ op $A_4$, $B_1$ op $B_2$ op $B_3$ op $B_4$, $C_1$ op $C_2$ op $C_3$ op $C_4$, $D_1$ op $D_2$ op $D_3$ op $D_4$      $A_1$ op $A_2$ op $A_3$ op $A_4$, $B_1$ op $B_2$ op $B_3$ op $B_4$, $C_1$ op $C_2$ op $C_3$ op $C_4$, $D_1$ op $D_2$ op $D_3$ op $D_4$      $A_1$ op $A_2$ op $A_3$ op $A_4$, $B_1$ op $B_2$ op $B_3$ op $B_4$, $C_1$ op $C_2$ op $C_3$ op $C_4$, $D_1$ op $D_2$ op $D_3$ op $D_4$      All to AllIt’s some thing like transpoising the data matrix.int MPI_Alltoall(const void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm)                   Worker 1      Worker 2      Worker 3      Worker 4                  Before      $A_1,B_1,C_1,D_1$      $A_2,B_2,C_2,D_2$      $A_3,B_3,C_3,D_3$      $A_4,B_4,C_4,D_4$              After      $A_1,A_2,A_3,A_4$      $B_1,B_2,B_3,B_4$      $C_1,C_2,C_3,C_4$      $D_1,D_2,D_3,D_4$      Other extension for functionsNotice that all of function above has asynchronous version and variable sizable version.Which can be changed by adding $I$ at the beginning of the function or $v$ at the end of the function respectively.For exmaple, Alltoallv is a variable sizable version for all to all, Igather is a asynchronous version of gather.Notice that varaible sizable means each of data may have different sizes.CommunityWe assumed that all the processor will join these operations.However, it can be changed by the code.You may can make a subgroup or some complex group from the MPI.Example is like below and more detail will be updated later.MPI_Group world_group; MPI_Comm_group(MPI_COMM_WORLD, &amp;world_group); MPI_Group mpi_local_group;int num_participants=3;int participants[3] = {0,2,4}; MPI_Group_incl(world_group, num_participants, participants, &amp;mpi_local_group);MPI_Comm new_comm; MPI_Comm_create_group(MPI_COMM_WORLD, mpi_local_group, 0, &amp;new_comm); if(new_comm != MPI_COMM_NULL) {     int local_rank;    MPI_Comm_rank(new_comm, &amp;local_rank);     printf("%d:local :%d\n", rank, local_rank );     MPI_Bcast(buf, MSGLEN, MPI_INT, 0, new_comm);}]]></content>
      <categories>
        
          <category> concurrency </category>
        
          <category> multi computer computing </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Family of algorithms]]></title>
      <url>/algorithm/approximation/2021/03/17/Family-of-algorithms/</url>
      <content type="text"><![CDATA[There are some algorithm families.PTAS(Polynomial-time approximation scheme)$\operatorname{PTAS}$ is a family of algorithms which is a $(1 + \epsilon)$ - approximation for minimization problem and a $(1 - \epsilon)$ - approximation for maximization problem.Notice that running time may depend on $\epsilon$.However, without polynomial restriction for it.FPAS, FPTAS(Fully polynomial-time approximation scheme)$\operatorname{FPAS}$ is a family of algorithms which is $\operatorname{PTAS}$ and running time is also bounded to a polynomial in $\frac{1}{\epsilon}$.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Approximation algorithm(5) - K-center]]></title>
      <url>/algorithm/approximation/2021/03/17/Approximation-algorithm(5)/</url>
      <content type="text"><![CDATA[We can imagine some clustering problem with $k$-centers.For $G = (V,E)$ with distance $d_{ij} \ge 0$ between each pair of vertices $i,j \in V$.We assume that $d_{ii} = 0, d_{ij} = d_{ji}, d_{ik} + d_{kj} \ge d_{ij}$ for each $i,j,k \in V$.Now, problem is find $S \subset V$ which $|S| = k$.Which makes $max(d(j,S))$ smallest.It’s like making clusters efficiently.There is a $2$-approximation algorithm for this.Let’s define $d(i,S) = \min_{j \in S} d_{ij}$.    Pick arbitrary $i \in V$    $S \leftarrow$ {$i$}    $\operatorname{while}$ $|S| &lt; k$ $\operatorname{do}$        $j \leftarrow \operatorname{argmax}_{j \in V} d(j,S)$        $S \leftarrow S \cup$ {$j$}    It’s running time is trivial to be in polynomial time.Now, let’s define that $O$ $=$ {$e_1, e_2, \cdots, e_k$} is an optimal solution, $S^O$ $=$ {$S_1, S_2, \cdots, S_k$} as each cluster and $r^{\star}$ as its radius.Then two points in each $S_i$ can’t be greater than $2r^{\star}$ by triangle inequality.Now, let’s think about the soltuion of an approximation algorithm $A$.Between the progress, if $A \neq O$ then in some moment algorithm needs to choose two points from some $S_i$.However, it means that farest points are already bounded in $2r^{\star}$ from the fact above.As a result, it can’t make bigger radius than $2r^{\star}$.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Approximation algorithm(4) - Knapsack]]></title>
      <url>/algorithm/approximation/2021/03/17/Approximation-algorithm(4)/</url>
      <content type="text"><![CDATA[Problem is like below.There is a set of items $I = \{1,2,\cdots,n$}, where each item $i$ has a value $v_i \gt 0$ and a size $s_i \gt 0$.Now, knapsack problem is a “Find the maximum possible value with capacity $B$”.There is a well-known dynamic programming algorithm to solve this.    $\operatorname{for} j \leftarrow 0, \cdots, B$         $\operatorname{OPT}_{0, j} \leftarrow 0$        $\operatorname{for} i \leftarrow 1, \cdots, n$        $\operatorname{for} j \leftarrow 0, \cdots, B$            $\operatorname{if} w_i &gt; j$                $\operatorname{OPT}_{i, j} \leftarrow \operatorname{OPT}_{i - 1, j}$                        $\operatorname{else}$                $\operatorname{OPT}_{i, j} \leftarrow \max(\operatorname{OPT}_{i -1, j}, \operatorname{OPT}_{i - 1, j - w_i} + v_i)$                            $\operatorname{return} OPT_{n, B}$In this case, $\operatorname{OPT}_{i,j}$ means an optimal value possible with $1,2,\cdots, i$ items and capacity below $j$.There is an alternative algorithm to solve this.    $V \leftarrow \sum\limits_{i = 1}^{n} v_i$    $\operatorname{for} j \leftarrow 0, \cdots, V$        $\operatorname{OPT}_{0, j} \leftarrow \infty$        $\operatorname{for} i \leftarrow 1, \cdots, n$        $\operatorname{for} j \leftarrow 0, \cdots, V$            $\operatorname{if} j &lt; v_i$                $\operatorname{OPT}_{i, j} \leftarrow \operatorname{OPT}_{i - 1, j}$                        $\operatorname{else}$                $\operatorname{OPT}_{i, j} \leftarrow \max(\operatorname{OPT}_{i -1, j}, \operatorname{OPT}_{i - 1, j - v_i} + s_i)$                            $\operatorname{return}(\max_{j=1}^{W}{j | OPT_{n, j} \le B})$In this case, $\operatorname{OPT}_{i,j}$ means the smallest size possible with $1,2,\cdots, i$ items with value below $j$.Now, it is shown that we have two options.Algorithm bounded in O(nB) or O(nW).However, $B$ and $W$ may arbitrary big and running time may become too long.Now, we will try to make a quantization for it.Quantization    $M \leftarrow \max_{i \in I} v_i$    $\mu \leftarrow \epsilon \frac{M}{n}$    $\operatorname{for} i \leftarrow 1, \cdots, n$        $v_i \leftarrow$ [$\frac{v_i}{\mu}$]        $V \leftarrow \sum\limits_{i = 1}^{n} v_i$    $\operatorname{for} j \leftarrow 0, \cdots, V$        $\operatorname{OPT}_{0, j} \leftarrow \infty$        $\operatorname{for} i \leftarrow 1, \cdots, n$        $\operatorname{for} j \leftarrow 0, \cdots, V$            $\operatorname{if} j &lt; v_i$                $\operatorname{OPT}_{i, j} \leftarrow \operatorname{OPT}_{i - 1, j}$                        $\operatorname{else}$                $\operatorname{OPT}_{i, j} \leftarrow \max(\operatorname{OPT}_{i -1, j}, \operatorname{OPT}_{i - 1, j - v_i} + s_i)$                            $\operatorname{return}(\max_{j=1}^{W}{j | OPT_{n, j} \le B})$From the algorithm above, we can see that an approximation algorithm runs in polynomial time.Let’s define $v_i’$ $=$ [$\frac{v_i}{\mu}$].Then for the new $V$ value, $V’$ $=$ $\sum\limits_{i = 1}^n v_i’$ $\le$ $\sum\limits_{i = 1}^n (\frac{v_i}{\mu})$ $=$ $\sum\limits_{i = 1}^n (\frac{v_i n}{M \epsilon})$ $\le$ $\sum\limits_{i = 1}^n (\frac{n}{\epsilon})$ $=$ $O(n^2/\epsilon)$.Now, we need to show that it returns at least $(1 - \epsilon)$ of $\operatorname{OPT}$ for show it is a $\operatorname{FPAS}$.To prove this, we need some facts, 1, $M \le OPT$, 2. $\mu v_i’ \le v_i \le \mu(v_i’ + 1)$.Also, we can get $\mu v_i’ \ge v_i - \mu$, $\epsilon M = n \mu$.Now, let $S$ as a set of items which an algorithm above gives and $O$ as a set of items in an optimal solution.Then, $\sum\limits_{i \in S} v_i$ $\ge$ $\sum\limits_{i \in S} \mu v_i’$ $\ge$ $\sum\limits_{i \in O} \mu v_i’$ $\ge$ $\sum\limits_{i \in O} (v_i - \mu)$ $=$ $\sum\limits_{i \in O}v_i - |O|\mu$ $\ge$ $\sum\limits_{i \in O}v_i - n\mu$ $=$ $\sum\limits_{i \in O}v_i - \epsilon M$ $=$ $\operatorname{OPT} - \epsilon M$ $\ge$ $\operatorname{OPT} - \epsilon \operatorname{OPT}$ $=$ $(1 - \epsilon)\operatorname{OPT}$.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[CUDA]]></title>
      <url>/concurrency/gpu/2021/03/16/CUDA_Stream/</url>
      <content type="text"><![CDATA[CUDA is a parallel computing model, architectural API model for GPU.It supports multiple abstractions to handle NVIDIA GPU.Following functions can be used for CUDA programming.This post will be updated later with more information.Memory allocation​cudaError_t cudaMalloc ( void** devPtr, size_t size ) This instruction allocates some memory space to GPU with size.It will return address of memory space to devPtr.If it fails, it will return some values as the return of function.Memory deallocationcudaError_t cudaFree ( void* devPtr )This instruction deallocates memory in devPtr on GPU.If it fails, it will return some values as the return of function.Memory copycudaError_t cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind )This instruction copies memory from CPU memory to GPU memory or between them.It copies memory from src to dst with size of count.It has 5 kinds of data transfer and following is that.  cudaMemcpyHostToHost : Copy memory between CPU memory space.  cudaMemcpyHostToDevice : Copy CPU memory space to GPU memory space.  cudaMemcpyDeviceToHost : Copy GPU memory space to CPU memory space.  cudaMemcpyDeviceToDevice : Copy memory between GPU memory space.  cudaMemcpyDefault : Automatically transfer the data. It requires to be unified memory.kernel callTo run the kernel of GPU, you need to use it like below.Notice that kernel call passes three parameters.nameOfKernelCall&lt;&lt;&lt;Number of Block, Number of Thread&gt;&gt;&gt;(parameters);ExampleWith the function above, it needs some kernel function to be used.Example code is like below.#include &lt;iostream&gt;#include &lt;time.h&gt;__global__ void _vectorAdd(int* data1, int* data2, int* result, int n){    int globalIdx = threadIdx.x + blockIdx.x * blockDim.x;    if(globalIdx &gt;= n)//Thread index overflow        return;    //Vector addition    result[globalIdx] = data1[globalIdx] + data2[globalIdx];}void vectorAdd(int *data1, int *data2, int* result, int n){    //Memory spaces in the GPU device    int *dataAtGpu1, *dataAtGpu2, *resultAtGpu;    cudaMalloc(&amp;dataAtGpu1, sizeof(int) * n);    cudaMalloc(&amp;dataAtGpu2, sizeof(int) * n);    cudaMalloc(&amp;resultAtGpu, sizeof(int) * n);    //Memory transfer from CPU to GPU    cudaMemcpy(dataAtGpu1, data1, sizeof(int) * n, cudaMemcpyHostToDevice);    cudaMemcpy(dataAtGpu2, data2, sizeof(int) * n, cudaMemcpyHostToDevice);    //Kernel call    _vectorAdd&lt;&lt;&lt;(n + 1023)/1024, n%1024&gt;&gt;&gt;(dataAtGpu1, dataAtGpu2, resultAtGpu, n);    //Result transfer from GPU to CPU    cudaMemcpy(result, resultAtGpu, sizeof(int) * n, cudaMemcpyDeviceToHost);}int main(){    srand(time(NULL));    constexpr int numVector = 1000;    int data1[numVector], data2[numVector];    for(int i = 0; i &lt; numVector; ++i){//Initialize vector1, vector2 to some random values        data1[i] = rand() % 100;        data2[i] = rand() % 100;    }    int resCPU[numVector];//Result of the vector addition on CPU    for(int i = 0; i &lt; numVector; ++i){        resCPU[i] = data1[i] + data2[i];//CPU type of vector addition    }        int resGPU[numVector];//Result of the vector addition on CGPU    vectorAdd(data1, data2, resGPU, numVector);//GPU(CUDA) type of vector addition    for(int i = 0; i &lt; numVector; ++i){        if(resGPU[i] != resCPU[i]){//Error handler            printf("Wrong result\n");            return 0;        }    }    printf("Test passed\n");    return 0;}Notice that this can be run in parallel with CPU at the same time.]]></content>
      <categories>
        
          <category> concurrency </category>
        
          <category> gpu </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Kronecker product]]></title>
      <url>/algorithm/graph/2021/03/15/Kronecker-product/</url>
      <content type="text"><![CDATA[Kronecker product is a mathmatical tools to find all possible combination of something.It can be used for many combinational problem like Traveling-salesman problem.Now, kronecker product $\otimes$ defines like follow.For $A$ $=$ $\begin{pmatrix}A_{1,1} &amp; A_{1,2} &amp; \cdots &amp; A_{1,n}\\ A_{2,1} &amp; A_{2,2} &amp; \cdots &amp; A_{2,n}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ A_{m,1} &amp; A_{m,2} &amp; \cdots &amp; A_{m,n}\\ \end{pmatrix}$,$B$ $=$ $\begin{pmatrix}B_{1,1} &amp; B_{1,2} &amp; \cdots &amp; B_{1,p}\\ B_{2,1} &amp; B_{2,2} &amp; \cdots &amp; B_{2,p}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ B_{q,1} &amp; B_{q,2} &amp; \cdots &amp; B_{q,p}\\ \end{pmatrix}$,$A \otimes B$ $=$ $\begin{pmatrix}A_{1,1}B &amp; A_{1,2}B &amp; \cdots &amp; A_{1,n}B\\ A_{2,1}B &amp; A_{2,2}B &amp; \cdots &amp; A_{2,n}B\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ A_{m,1}B &amp; A_{m,2}B &amp; \cdots &amp; A_{m,n}B\\ \end{pmatrix}$ $=$ $\begin{pmatrix}A_{1,1}B_{1,1} &amp; A_{1,1}B_{1,2} &amp; \cdots &amp; A_{1,1}B_{1,p} &amp; A_{1,2}B_{1,1} &amp; A_{1,2}B_{1,2} &amp; \cdots &amp; A_{1,2}B_{1,p} &amp; A_{1,3}B_{1,1} &amp; \cdots &amp; A_{1,n}B_{1,p}\\ A_{1,1}B_{2,1} &amp; A_{1,1}B_{2,2} &amp; \cdots &amp; A_{1,1}B_{2,p} &amp; A_{1,2}B_{2,1} &amp; A_{1,2}B_{2,2} &amp; \cdots &amp; A_{1,2}B_{2,p} &amp; A_{1,3}B_{2,1} &amp; \cdots &amp; A_{1,n}B_{2,p}\\ \vdots         &amp; \vdots         &amp; \ddots &amp; \vdots         &amp; \vdots         &amp; \vdots         &amp; \ddots &amp; \vdots         &amp; \vdots         &amp; \ddots &amp; \vdots        \\ A_{1,1}B_{q,1} &amp; A_{1,1}B_{q,2} &amp; \cdots &amp; A_{1,1}B_{q,p} &amp; A_{1,2}B_{q,1} &amp; A_{1,2}B_{q,2} &amp; \cdots &amp; A_{1,2}B_{q,p} &amp; A_{1,3}B_{q,1} &amp; \cdots &amp; A_{1,n}B_{q,p}\\ A_{2,1}B_{1,1} &amp; A_{2,1}B_{1,2} &amp; \cdots &amp; A_{2,1}B_{1,p} &amp; A_{2,2}B_{1,1} &amp; A_{2,2}B_{1,2} &amp; \cdots &amp; A_{2,2}B_{1,p} &amp; A_{2,3}B_{1,1} &amp; \cdots &amp; A_{2,n}B_{1,p}\\ A_{2,1}B_{2,1} &amp; A_{2,1}B_{2,2} &amp; \cdots &amp; A_{2,1}B_{2,p} &amp; A_{2,2}B_{2,1} &amp; A_{2,2}B_{2,2} &amp; \cdots &amp; A_{2,2}B_{2,p} &amp; A_{2,3}B_{2,1} &amp; \cdots &amp; A_{2,n}B_{2,p}\\ \vdots         &amp; \vdots         &amp; \ddots &amp; \vdots         &amp; \vdots         &amp; \vdots         &amp; \ddots &amp; \vdots         &amp; \vdots         &amp; \ddots &amp; \vdots        \\ A_{2,1}B_{q,1} &amp; A_{2,1}B_{q,2} &amp; \cdots &amp; A_{2,1}B_{q,p} &amp; A_{2,2}B_{q,1} &amp; A_{2,2}B_{q,2} &amp; \cdots &amp; A_{2,2}B_{q,p} &amp; A_{2,3}B_{q,1} &amp; \cdots &amp; A_{2,n}B_{q,p}\\ \vdots         &amp; \vdots         &amp; \ddots &amp; \vdots         &amp; \vdots         &amp; \vdots         &amp; \ddots &amp; \vdots         &amp; \vdots         &amp; \ddots &amp; \vdots        \\ A_{m,1}B_{q,1} &amp; A_{m,1}B_{q,2} &amp; \cdots &amp; A_{m,1}B_{q,p} &amp; A_{m,2}B_{q,1} &amp; A_{m,2}B_{q,2} &amp; \cdots &amp; A_{m,2}B_{q,p} &amp; A_{m,3}B_{q,1} &amp; \cdots &amp; A_{m,n}B_{q,p}\\ \end{pmatrix}$.Notice that this can be used for something recursive because it copies $B$s to submatrices.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> graph </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Matrix representation]]></title>
      <url>/algorithm/graph/matrix/2021/03/14/Matrix-representation/</url>
      <content type="text"><![CDATA[Matrix representationThere are many formats to represent matrices.            Format      Adjacency matrix      COO(Edge list(Array))      Edge list(linked list)      Adjacency list      CSR/CSC                  Space      $O (\left\vert V \right\vert^2)$      $O(\left\vert E \right\vert)$      $O(\left\vert E \right\vert)$      $O(\left\vert V \right\vert + \left\vert E \right\vert)$      $O(\left\vert V \right\vert + \left\vert E \right\vert)$              Read edge      $O (1)$      $O(log \left\vert E \right\vert)$      $O(\left\vert E \right\vert)$      $O(deg(v))$      $O(deg(v))$              Add edge      $O (1)$      $O(\left\vert E \right\vert)$      $O(\left\vert E \right\vert)$      $O(deg(v))$      $O(\left\vert V \right\vert + \left\vert E \right\vert)$              Delete edge      $O (1)$      $O(\left\vert E \right\vert)$      $O(\left\vert E \right\vert)$      $O(deg(v))$      $O(\left\vert V \right\vert + \left\vert E \right\vert)$              Read neighbors      $O (\left\vert V \right\vert)$      $O(log \left\vert E \right\vert + deg(v))$      $O(\left\vert E \right\vert)$      $O(deg(v))$      $O(deg(v))$              Get degree      $O (\left\vert V \right\vert)$      $O(log \left\vert E \right\vert + deg(v))$      $O(\left\vert E \right\vert)$      $O(deg(v))$      $O(1)$      Adjacency matrix is a matrix representation as it is.Adjacency matrix uses 2D-array to store every value.Edge list is a matrix representation which represents edges to an array(or a linked list).Notice that we can find a specific edge by binary search.Adjacency list is a matrix representation which repreents edges as a list per vertex and keep array of vertex as an array.CSR/CSC is a matrix representation which repreents matrix like adjacency matrix but using an array.CSC/CSC uses a row_ptr/col_ptr to noticing the starting and ending index of each edges of a row/column.With these indices, it represents the column/row indices and values.Each format has own pros and cons.However, there are some reasons which makes some formats aren’t be used in the mordern science. In real world matrices, may of them are sprase.Therefore, adjacency matrix isn’t used anymore.Edge list and Adjacency list(which uses linked list) aren’t used because of dynamic memory hierarchy.COO representation are rarely used but CSR/CSC are more standards because of memory usage is slightly different.CSC/CSC uses $\left\vert V \right\vert + 2\left\vert E \right\vert$ memories but COO uses $3\left\vert E \right\vert$ memories.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> graph </category>
        
          <category> matrix </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Approximation algorithm(3) - SET COVER]]></title>
      <url>/algorithm/approximation/2021/03/14/Approximation-algorithm(3)/</url>
      <content type="text"><![CDATA[Recap $\operatorname{SET COVER}$ can be LP-relaxed to “Minimize the $\sum\limits_{j=1}^{m} w_j x_j$ such that $\sum\limits_{j:e_i \in S_j} x_j \ge 1$, $x_j \ge 0$ then choose $x_j$ to $1$ which $x_j \ge \frac{1}{f}$ and otherwise to 0”. Which is $f$-approximation algorithm. Which $e_i$ denotes each elements, $S_j$ denotes each subsets, $w_j$ denotes costs of each subsets, $x_j$ is the chocie of each subset. With $f$ is the maximum number of existance of elements.From the above, we can make $f$-approximation algorithm.However it may arbitrary big in comparison.Therefore, we prefer the better approximation.Here are some different approaches to this $\operatorname{SET COVER}$ with $O(ln(n))$-approximation.Greedy algorithm    $I \leftarrow \emptyset$    $\hat S_j \leftarrow S_j \forall j$    $\operatorname{while}$ $I$ is not a set cover $\operatorname{do}$        $l \leftarrow \operatorname{argmin}_{j:\hat S_j \neq \emptyset} \frac{w_j}{|\hat S_j|}$        $I \leftarrow I \cup$ {$l$}        $\hat S_j \leftarrow \hat S_j - S_l \forall j$    Algorithm above is a natural algorithm for $\operatorname{SET COVER}$ and it is the $O(ln(n))$-approximation algorithm.First, it is trivial that it returns a valid solution because it terminates iff returned solution is a $\operatorname{SET COVER}$.Second, this algorithm runs in polynomial time because, loops ends up in polynomial time and each step works in only polynomial time.Now, only left part is that proving solution is bounded by $O(ln(n))$ factor of original solution.First of all, let’s define $I$ as the final solution, $w_1, w_2, \cdots, w_{|I|}$ to weights of each start of iteration and $\hat S_1, \hat S_2, \cdots, \hat S_{|I|}$ to uncovered elements in each start of iteration. From the fact that we will choose the minimum average weights, $w_j \le \frac{|\hat S_j| - |\hat S_{j + 1}|}{|\hat S_j|}\operatorname{OPT}$.Which means $\sum\limits_{j \in I} w_j$ $\le$ $\sum\limits_{j = 1}^{|I|} \frac{|\hat S_j| - |\hat S_{j + 1}|}{|\hat S_j|}\operatorname{OPT}$ $\le$ $\operatorname{OPT}\sum\limits_{j \in I}(\frac{1}{|\hat S_j|} + \frac{1}{|\hat S_j - 1|} + \frac{1}{|\hat S_j - 2|} + \cdots + \frac{1}{|\hat S_{j+1}| + 1})$ $=$ $\operatorname{OPT}\sum\limits_{i}^{n}\frac{1}{i}$ $\le$ $\operatorname{OPT}(1 + ln(n))$.Random roundingOne another approach is that choosing 1 in probability of itselft.Minimize the $\sum\limits_{j=1}^{m} w_j x_j$ such that $\sum\limits_{j:e_i \in S_j} x_j \ge 1$, $x_j \ge 0$ then choose $x_j$ to $1$ with probability of $x_j$ and $0$ with probability $1 - x_j$. Notice that This algorithm will not choose $x_j &gt; 1$.Now, we can’t guarantee that it returns good solution. (It may return the solution with all $1$ inside.)However, we may make a guarantee for average and it is usual for approximation algorithm with random.$E[\sum\limits_{j=1}^{m} w_j X_j]$ $=$ $\sum\limits_{j=1}^{m} Pr[X_j = 1]$ $=$ $\sum\limits_{j=1}^{m} w_j x_j \le \operatorname{OPT}$.As a result, it gives a solution which is better than $\operatorname{OPT}$.However, it never can’t better than the optimal solution unless it doesn’t cover all elements in the ground set.Let’s check whether it really can’t cover all elements.$Pr[e_i$ not covered$]$ $=$ $\prod\limits_{j:e_i \in S_j} (1 - x_j)$ $\le$ $\prod\limits_{j:e_i \in S_j} e^{-x_j}$ $=$ $e^{-\sum_{j:e_i \in S_j} x_j}$ $\le$ $e^{-1}$As a result, it will return a solution with non-covered elements in upper bound of $e^{-1}$.Which means that this algorithm highly likely returns a non-solution.Therefore, we need some improvements.What if we try $c \operatorname{ln} n$ times to get $1$?$Pr[e_i$ not covered$]$ $=$ $\prod\limits_{j:e_i \in S_j} (1 - x_j)^{c \operatorname{ln} n}$ $\le$ $\prod\limits_{j:e_i \in S_j} e^{-x_j (c \operatorname{ln} n)}$ $=$ $e^{-(c \operatorname{ln} n) \sum_{j:e_i \in S_j} x_j}$ $\le$ $\frac{1}{n^c}$.As a result, $Pr[$ there exsits an uncovered element $]$ $\le$ $\sum\limits_{i=1}^{n}Pr[e_i$ not covered$]$ $\le$ $\frac{1}{n^{c - 1}}$.Now, it can show that this random algorithm can produce a feasible solution with resonably high probability.How about the average of the solution?First of all, let’s define $p(x_j)$ as the probability of “$x_j$ is included in the solution”.Then, $p(x_j)$ $=$ $1 - (1-x_j)^{c \operatorname{ln} n}$.Which makes $p(x_j)’$ $=$ $(c \operatorname{ln} n)(1 - (1-x_j)^{(c \operatorname{ln} n) - 1})$ $\le$ $c \operatorname{ln} n$.Which also makes $p(x_j)$ $\le$ $(c \operatorname{ln} n) x_j$.As a result, $E[\sum\limits_{j=1}^m w_j X_j]$ $=$ $\sum\limits_{j=1}^m w_j Pr[X_j = 1]$ $\le$ $\sum_{j=1}^m w_j (c \operatorname{ln} n) x_j$ $=$ $(c \operatorname{ln} n)\sum_{j=1}^m w_j x_j$ $\le$ $(c \operatorname{ln} n) OPT$This is a summary of approximation algorithm for $\operatorname{SET COVER}$.We will see some other techniques with some examples.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Approximation algorithm(2) - SET COVER]]></title>
      <url>/algorithm/approximation/2021/03/12/Approximation-algorithm(2)/</url>
      <content type="text"><![CDATA[Recap $\operatorname{SET COVER}$ can be LP-relaxed to “Minimize the $\sum\limits_{j=1}^{m} w_j x_j$ such that $\sum\limits_{j:e_i \in S_j} x_j \ge 1$, $x_j \ge 0$ then choose $x_j$ to $1$ which $x_j \ge \frac{1}{f}$ and otherwise to 0”. Which is $f$-approximation algorithm. Which $e_i$ denotes each elements, $S_j$ denotes each subsets, $w_j$ denotes costs of each subsets, $x_j$ is the chocie of each subset. With $f$ is the maximum number of existance of elements.Here are some different approaches to this $\operatorname{SET COVER}$.Dual solutionMaximize the $\sum\limits_{i=1}^{n} y_i$ such that $\sum\limits_{i:e_i \in S_j} y_i \le w_j$, $y_i \ge 0$.This is so-called dual of original LP.Now, how do we rounding?We can choose $\sum\limits_{i:e_i \in S_j} y_i = w_j$.This apporach is like make the sum of cost of elements to expensive possible but every subset can cover costs.Also, choose only subset fully utilize the cost of own.Now, let’s show this is a $f$-appoximation either.First, let’s show that solution of dual, $I$ is a set cover.Suppouse that we didn’t cover some $e_k$ so for every subset $e_k \in S_j$, $\sum\limits_{i:e_i \in S_j} y_i &lt; w_j $.Now find the minimum difference $w_j$ which $\epsilon = \min_{j:e_k \in S_j}(w_j - \sum\limits_{i:e_i \in S_j} y^o_i)$.Then, a new solution with $y^b_k = y^o_k + \epsilon \le w_j$ is still a feasible soltuion.It means the contradiction which $y^o_k$ was the maximum.Therefore dual problem should produced a feasible solution.From the original LP, it can be known that $\sum\limits_{i=1}^{n} y_i$ $\le$ $\sum\limits_{i=1}^{n} (y_i \sum\limits_{j:e_i \in S_j} x_j)$ $=$ $\sum\limits_{i=1}^{n}\sum\limits_{j:e_i \in S_j} y_i x_j$ $=$ $\sum\limits_{j=1}^{m}\sum\limits_{i:e_i \in S_j} y_i x_j$ $=$ $\sum\limits_{j=1}^{m} (x_j \sum\limits_{i: e_i \in S_j} y_i )$ since $\sum\limits_{j:e_i \in S_j} x_j \ge 1$.From the constrain $\sum\limits_{j=1}^{m} (x_j \sum\limits_{i: e_i \in S_j} y_i)$ $\le$ $\sum\limits_{j=1}^{m} x_j w_j$ is true.As a result, $\sum\limits_{i=1}^{n} y_i \le \sum\limits_{j=1}^{m} x_j w_j$.Since it works with any feasible solution $x_j$ and $y_i$, $\sum\limits_{i=1}^{n} y^o_i$ $\le$ $\sum\limits_{j=1}^{m} x^o_j w_j$ is true for a solution of original LP $x^o_j$ and a solution of dual $y^o_i$.In summary, $\sum\limits_{i=1}^{n} y^o_i$ $\le$ $\sum\limits_{j=1}^{m} x^o_j w_j$ $\le$ $\operatorname{OPT}$.Now with the fact above, $\sum\limits_{j \in I} w_j$ $=$ $\sum\limits_{j \in I} (\sum\limits_{i:e_i \in S_j} y^o_i)$ from the constraint.$\sum\limits_{j \in I} (\sum\limits_{i:e_i \in S_j} y^o_i)$ $=$ $\sum\limits_{i=1}^{n}(\sum\limits_{e_i \in S_j, j \in I} y^o_i)$ $=$ $\sum\limits_{i=1}^{n}(|j \in I, e_i \in S_j| y^o_i)$ $\le$ $\sum\limits_{i=1}^{n}f y^o_i$ $\le$ $f\sum\limits_{i=1}^{n}y^o_i$ $\le$ $f \operatorname{OPT}$.Now we proved this is the $f$-approximation algorithm.Primal-dual algorithmNow, there are some major points in this algorithm.  $\sum\limits_{i \in l} y_i \le \operatorname{OPT}$  We can make a better solution by choose $y^b_k$ $=$ $y^o_k + \epsilon$ $=$ $w_j$ without lossing valid constraint.  We will choose $\sum\limits_{i:e_i \in S_j} y_i = w_j$.From the above, we can make better algorithmic approach for it.    $y \leftarrow 0$    $l \leftarrow \emptyset$    $\operatorname{while}$ there exists $e_i \not\in \bigcup\limits_{j \in I} S_j \operatorname{do}$        Increase the dual variable $y_i$ untill there is some $l$ with $e_i \in S_l$ such that $\sum\limits_{j:e_j \in S_l} y_j = w_l$        $I \leftarrow I \cup$ {$l$}    From the facts above, we can notice that after inreasing $y_j$, we still have 1.From the fact 2 and 3, we can choose any $y_j$ to make it still feasible.Proof is ommited in here because it just follows exact the same process of dual program.However, it can remove linear programming.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Approximation algorithm(1) - SET COVER]]></title>
      <url>/algorithm/approximation/2021/03/07/Approximation-algorithm(1)/</url>
      <content type="text"><![CDATA[An optimization problem is a problem to find minimum/maximum possible soltuion.For example, $\operatorname{SET COVER}$ is a well-known optimization problem.For a universe set $U = \{e_1,e_2,e_3,\cdots,e_n$} and subsets of $U$, $S_j(1 \le j \le n)$. $\operatorname{SET COVER}$ is a problem to find a collection of subsets which minimize the sum of costs of subset $W_j$ and it contains every element in $U$.For example, if $U = \{e_1,e_2,e_3,e_4,e_5$}, $S_1 = \{e_1,e_2$}, $S_2 = \{e_3,e_4$}, $S_3 = \{e_5$}, $S_4 = \{e_2,e_3$}, $S_5 = \{e_1$}, $S_6 = \{e_4$}, $W_1=10$, $W_2=10$, $W_3=1$, $W_4=15$, $W_5=1$, $W_6=1$.Optimal solution would be $W_3 + W_4 + W_5 + W_6 = 1 + 15 + 1 + 1 = 18$.Notice that $S_3 \cup S_4 \cup S_5 \cup S_6 = U$.However, it is known that $\operatorname{SET COVER}$ is a NP-hard.Which means that requires more than polynomial time if P $\neq$ NP.Now, here the approximation algorithm works.$\alpha$-approximation algorithm is an algorithm that solves problem with maximum of $\alpha$ factor difference.Which means if problem’s solution is $\operatorname{OPT}$, solution of $\alpha$-approximation algorithm $ans$ needs to be $ans \le \alpha OPT$.For the easy of explanation, $\operatorname{SET COVER}$ can be converted like below.Minimize the $\sum\limits_{j=1}^{m} w_j x_j$ such that $\sum\limits_{j:e_i \in S_j} x_j \ge 1$, $x_j \in \{0,1$}.Notice that this kinds of problem is known as integer problem.However, if we do relaxation, we can do approximation over this.New problem is like below.Minimize the $\sum\limits_{j=1}^{m} w_j x_j$ such that $\sum\limits_{j:e_i \in S_j} x_j \ge 1$, $x_j \ge 0$.Notice that this kinds of problem is known as linear problem.Which is known to be solvable in the polynomial time.After solving problem above, we can get $x_j$ as the solution.However, to make it as a vaild solution, we need to do rounding.To achieve this, we need to find the maximum number of existance of element in unverse set $e_i$ in $S$s.For example, if $U = \{e_1,e_2,e_3,e_4,e_5$}, $S_1 = \{e_1,e_2$}, $S_2 = \{e_3,e_4$}, $S_3 = \{e_5$}, $S_4 = \{e_2,e_3$}, $S_5 = \{e_1$}, $S_6 = \{e_4$}, $W_1=10$, $W_2=10$, $W_3=1$, $W_4=15$, $W_5=1$, $W_6=1$.Number of existances are like $f_{e_1} = 2$, $f_{e_2} = 2$, $f_{e_3} = 2$, $f_{e_4} = 2$, $f_{e_5} = 1$.So, maximum of $f_{e_i}$, $f = 2$.Now, we only set $x_j \ge \frac{1}{f}$ to 1, otherwise to 0.Algorithm above can be done in polynomial time.Now, we need to prove that this is a proper $f$-approximation algorithm for original problem.      Since, we choosed $x_j \ge \frac{1}{f}$ to 1 and maximum number of existance of element is $f$, we need to choose at least one of such $x_j \ge \frac{1}{f}$.Otherwise, $\sum\limits_{j|e_i \in S_j} x_j \ge 1$ can’t be happen.Therefore, $\operatorname{ans}$ is a valid solution.        Let’s assume that answer before rouding to $\operatorname{round}$.Since $\operatorname{ans}$ choosed only  $x_j \ge \frac{1}{f}$ to 1, $\operatorname{ans} \le f \operatorname{round}$.Notice that $\operatorname{round}$ will have no $x &gt; 1$.However $\operatorname{round} \le \operatorname{OPT}$ because we did relaxation.As a result, $\operatorname{ans} \le f \operatorname{round} \le f\operatorname{OPT}$.Proof stops in here.  Now, we made an efficient approach to this $\operatorname{SET COVER}$.]]></content>
      <categories>
        
          <category> algorithm </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[ABA problem]]></title>
      <url>/concurrency/problem/2020/11/22/ABAProblem/</url>
      <content type="text"><![CDATA[ABA is a well-known problem at CAS based lock-free data structure.C++ supports std::atomic for abitrary data type but it isn’t sometimes lock free.You can check it with below.#include &lt;atomic&gt;#include &lt;iostream&gt;typedef struct _bigStructure{    int a;    int b;    float c;    char d;}bigStructure;int main(){    std::atomic&lt;int&gt; atomicInt;    std::atomic&lt;bigStructure&gt; atomicStructure;    std::cout &lt;&lt; "std::atomic&lt;int&gt; atomicInt                  is it lock free? : " &lt;&lt; atomicInt.is_lock_free() &lt;&lt; std::endl;    std::cout &lt;&lt; "std::atomic&lt;bigStructure&gt; atomicStructure   is it lock free? : " &lt;&lt; atomicStructure.is_lock_free() &lt;&lt; std::endl;    return 0;}Result will be looks like.std::atomic&lt;int&gt; atomicInt                  is it lock free? : 1std::atomic&lt;bigStructure&gt; atomicStructure   is it lock free? : 0In fact, CAS-possible data size is limited to the HW support.However, most of HW supports only memory size CAS operation.If it is 32-bit computing unit, it usually supports 32-bit CAS.If it is 64-bit computing unit, it usually supports 64-bit CAS.Therefore, most of CAS based data structures use a pointer to work.Below lock-free queue uses this.#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;#include &lt;atomic&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;mutex&gt;template&lt;typename T&gt;int __coutArgs__(T t){    std::cout &lt;&lt; t;}template&lt;typename T, typename ... Args&gt;int __coutArgs__(T t, Args... args){    std::cout &lt;&lt; t;    __coutArgs__(args...);}std::mutex __print_sync_mutex__;//Synchronous printing for cpptemplate&lt;typename ... Args&gt;int printSync(Args... args){    __print_sync_mutex__.lock();    __coutArgs__(args...);    __print_sync_mutex__.unlock();}//List structure to save some profileclass profile_t{public:    profile_t(std::string name, std::string phoneNumber, profile_t* next):name(name),phoneNumber(phoneNumber),next(next){}    std::string name;    std::string phoneNumber;    profile_t* next;};class profileStack_t{public:    std::atomic&lt;profile_t*&gt; top_ptr;    profileStack_t(){        top_ptr = nullptr;    }    profile_t* pop(){        while (true) {            profile_t* ret_ptr = top_ptr;            if (ret_ptr == nullptr) return nullptr;//Empty stack            profile_t* next_ptr = ret_ptr-&gt;next;            if (top_ptr.compare_exchange_weak(ret_ptr, next_ptr)) {                return ret_ptr;            }        }    }    profile_t* slowPop(){        while (true) {            profile_t* ret_ptr = top_ptr;            if (ret_ptr == nullptr) return nullptr;//Empty stack            profile_t* next_ptr = ret_ptr-&gt;next;            printSync("T2 sleep with top pointer at ", ret_ptr, "\n");            printSync("This CAS must failed if T1 access\n");            sleep(2);            if (top_ptr.compare_exchange_weak(ret_ptr, next_ptr)) {                printSync("T2 Success\n");                return ret_ptr;            }            else{                printSync("T2 Failed\n");            }        }    }    void push(profile_t* obj_ptr){        while (true) {            profile_t* next_ptr = top_ptr;            obj_ptr-&gt;next = next_ptr;            if (top_ptr.compare_exchange_weak(next_ptr, obj_ptr)) {                return;            }        }    }};int main(){    using namespace std;    profile_t* A    = new profile_t("A", "010-1234-5678", nullptr);    profile_t* B    = new profile_t("B", "010-1111-1111", nullptr);    profileStack_t q;    q.push(B);    q.push(A);        auto t1 = thread([&amp;](){        sleep(1);//Let t2 gets pointer first.        auto popped1 = q.pop();        if(popped1 != nullptr){            printSync("T1 : pop ", popped1-&gt;name.c_str(), "(", popped1-&gt;phoneNumber, ") at ", popped1, "\n");            delete popped1;        }                profile_t* newA    = new profile_t("newA", "010-0000-0000", nullptr); //Produce after delete A for using the same address                auto popped2 = q.pop();        if(popped2 != nullptr){            printSync("T1 : pop ", popped2-&gt;name.c_str(), "(", popped2-&gt;phoneNumber, ") at ", popped2, "\n");            delete popped2;        }        q.push(newA);        printSync("T1 : push ", newA-&gt;name.c_str(), "(", newA-&gt;phoneNumber, ") at ", newA, "\n");    });        auto t2 = thread([&amp;](){        auto popped = q.slowPop();        if(popped != nullptr){            printSync("T2 : pop ", popped-&gt;name.c_str(), "(", popped-&gt;phoneNumber, ") at ", popped, "\n");            delete popped;        }    });    t1.join();    t2.join();    profile_t* popped;    while(true){        popped = q.pop();        if(popped == nullptr)            break;        cout &lt;&lt; "top : " &lt;&lt; popped-&gt;name.c_str() &lt;&lt; "(" &lt;&lt; popped-&gt;phoneNumber &lt;&lt;") at " &lt;&lt; popped &lt;&lt; "\n";        delete popped;    }}However, it has a problem(You can try this in own).Since it checks only the pointer to decide whether it changed or not,it can’t know when it just reuses the same address.Therefore, above scenario will be a problem.Let’s assume there are one CAS-based lock-free queue $Q$ and two threads $T_1, T_2$.$Q$ has $A$ and $B$ at the initial state.Now $T_1$ access $Q$, it caches $A$ as a top of stack and $B$ as a next element of the top.However $T_1$ fall a sleep right after.Now $T_2$ access $Q$, it pops $A$ and $B$. After that $T_2$ reuses $A$’s address and push it to the $Q$.Now $T_1$ wakes up, $T_1$’s CAS will be success because it checks only top of the stack.Therefore, top of the stack will be a dangling reference to the $B$.Now it is totally massed.]]></content>
      <categories>
        
          <category> concurrency </category>
        
          <category> problem </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Monty hall problem and monte carlo method]]></title>
      <url>/simulation/approximation/2020/11/21/MontyHole/</url>
      <content type="text"><![CDATA[  There is a famous problem known as “Monty hall problem”.There are three doors.One of the door has a car and others have a goat.If a person selects a door, host will open one door with a goat.Now, person gets one chance to change the selection or not.So the question is which one is better in between changing it or not?If there is nothing psychological relations, it is proven that changing the selection makes better decision.It can be easily extended to $n$ door problem.Let’s assume that $A$ is an event that chooses the right door at the first trial which $P(A) = \frac{1}{n}$.If a person choosed the right door, then change makes the selection wrong.If a person choosed the wrong door, then change may make the right selection between $n - 2$ doors.As a result, if a person changes a door it will be $\frac{1}{n} \times 0 + \frac{n - 1}{n} \times \frac{1}{n-2} = \frac{n-1}{n \times (n-2)}$.Which means it results in a better probability to win.It can be simulated by code below.#include &lt;time.h&gt;#include &lt;iostream&gt;int main(){    using namespace std;    int numDoor;    int numIteration;    int success = 0;    int failed = 0;    int price, selected, changed;    srand(time(NULL));    cout &lt;&lt; ("Enter the trial number: ");    cin &gt;&gt; numIteration;    cout &lt;&lt; ("Enter the number of doors: ");    cin &gt;&gt; numDoor;    bool door[numDoor];    for (int i = 0; i &lt; numIteration; i++){        for (int j = 0; j &lt; numDoor; j++){            door[j] = false;        }        price = rand() % numDoor;        door[price] = true;        selected = rand() % numDoor;        int hostSelected = rand() % numDoor;        while(hostSelected == selected || door[hostSelected]){            hostSelected = rand() % numDoor;        }        changed = rand() % numDoor;        while((changed == selected) || (changed == hostSelected)){            changed = rand() % numDoor;        }        if (door[changed]){            success++;        }        else{            failed++;        }    }    cout &lt;&lt; "# success " &lt;&lt; success &lt;&lt; ", # failed " &lt;&lt; failed &lt;&lt; ", Percentage " &lt;&lt; success / float(numIteration) * 100 &lt;&lt; std::endl;    return 0;}This is close to monte-carlo method.It just tries the change and check wheter it is price or not.Collect the data and guess the actual probability.            # trial      # door      # success      # fail      Probability(From experiment)      Probability(From theory)                  10000      3      6703      3297      67.03      $\frac{2}{3} \cdot \frac{1}{1} \cdot 100 = 66.\dot{6}$              10000      4      3768      6232      37.68      $\frac{3}{4} \cdot \frac{1}{2} \cdot 100 = 37.5$              10000      5      2608      7392      26.08      $\frac{4}{5} \cdot \frac{1}{3} \cdot 100 = 26.\dot{6}$              10000      6      2028      7972      20.28      $\frac{5}{6} \cdot \frac{1}{4} \cdot 100 = 20.8\dot{3}$      Here is one more example to compute the area of circle.#include &lt;iostream&gt;#include &lt;time.h&gt;int main(){    using namespace std;    constexpr int numIteration = 1000000;    srand(time(NULL));    int inSide = 0;    for(int i = 0; i &lt; numIteration; ++i){        float x = rand() % 10000 / float(5000) - 1; // -1 ~ 1        float y = rand() % 10000 / float(5000) - 1; // -1 ~ 1        if(x * x + y * y &lt;= 1){            ++inSide;        }    }    cout &lt;&lt; "Inside of circle : " &lt;&lt; inSide &lt;&lt; ", circle area may be " &lt;&lt; 4 * inSide/float(numIteration) &lt;&lt; std::endl;}It is known to be $\pi r^2$.Circle area is $3.14082$ with $r = 1$ from the experiment.Which means it works pretty well.]]></content>
      <categories>
        
          <category> simulation </category>
        
          <category> approximation </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Sorting algorithm analysis]]></title>
      <url>/algorithm/2020/11/15/SortingAlgorithm/</url>
      <content type="text"><![CDATA[From the previous algorithms, qsort is the slowest at the worst cases and others are always $O(n logn)$.However, qsort is known to be fastest at the most of cases.            Algorithm      Average speed      Worst speed      Extra memory usage                  Qsort      Fastest($O(n log n)$)      Slowest($O(n^2)$)      None($O(c)$)              Merge sort      Normal($O(n log n)$)      Normal($O(n log n)$)      Auxlist($O(n)$)              Heap sort      Normal($O(n log n)$)      Normal($O(n log n)$)      None($O(c)$)      #include &lt;iostream&gt;#include &lt;math.h&gt;#include &lt;time.h&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;#include &lt;fstream&gt;#include &lt;chrono&gt;void swap(int&amp; a, int&amp; b){    int c = a;    a = b;    b = c;}void downHeap(int lst[], int length, int idx){    if (2 * idx + 2 &lt; length){        if (lst[idx] &lt; lst[2 * idx + 1]){            if(lst[2 * idx + 1] &lt; lst[2 * idx + 2]){                swap(lst[idx], lst[2 * idx + 2]);                downHeap(lst, length, 2 * idx + 2);            }            else{                swap(lst[idx], lst[2 * idx + 1]);                downHeap(lst, length, 2 * idx + 1);            }        }        else if (lst[idx] &lt; lst[2 * idx + 2]){            swap(lst[idx], lst[2 * idx + 2]);            downHeap(lst,length, 2 * idx + 2);        }    }    else if (2 * idx + 1 &lt; length){        if (lst[idx] &lt; lst[2 * idx + 1]){            swap(lst[idx], lst[2 * idx + 1]);            downHeap(lst, length, 2 * idx + 1);        }    }}void listToHeap(int lst[], int length){    int height = 0;    int sumE   = 0;    while (sumE &lt; length){        height += 1;        sumE   *= 2;        sumE   += 1;    }    for(int i = height - 2; i &gt; -1; i -= 1){        for(int j = pow(2,i) -1; j &lt; pow(2,i+1) -1; j++){            downHeap(lst, length, j);        }    }}void heapSort(int lst[], int length){    listToHeap(lst, length);    for(int i = length-1; i &gt; 0; --i){        swap(lst[0], lst[i]);        downHeap(lst, i, 0);    }}int partition(int lst[], int fromIdx, int toIdx){    int selectPoint = (toIdx + fromIdx)/2;    swap(lst[fromIdx], lst[selectPoint]);    int criteria = lst[fromIdx];    int lIdx = fromIdx;    int rIdx = toIdx - 1;    while(lIdx &lt; rIdx){        while(lIdx &lt; rIdx){            if(lst[lIdx] &gt; criteria)                break;            lIdx += 1;        }        if(lIdx == rIdx)            break;        while (lIdx &lt; rIdx){            if (lst[rIdx] &lt;= criteria)                break;            rIdx -= 1;        }        swap(lst[lIdx], lst[rIdx]);    }    if (lst[lIdx] &gt; lst[fromIdx])        lIdx -= 1;    swap(lst[lIdx], lst[fromIdx]);    return lIdx;}void _qsort(int lst[], int fromIdx, int toIdx){    if(toIdx - fromIdx &lt;= 1)        return;    int mid = partition(lst, fromIdx, toIdx);    _qsort(lst, fromIdx, mid);    _qsort(lst, mid + 1, toIdx);}    void qsort(int lst[], int length){    _qsort(lst, 0, length);}void _mergeSort(int lst[], int fromIdx, int toIdx){    if (toIdx - fromIdx &lt;= 1)        return;    int mid = (toIdx + fromIdx)/2;    _mergeSort(lst, fromIdx, mid);    _mergeSort(lst, mid, toIdx);    int auxList[toIdx - fromIdx];    int idx1 = fromIdx;    int idx2 = mid;    for(int i = 0; i &lt; (toIdx - fromIdx); ++i){        if (idx2 &gt;= toIdx || idx1 &lt; mid &amp;&amp; lst[idx1] &lt;= lst[idx2]){            auxList[i] = lst[idx1];            idx1 += 1;        }        else if (idx1 &gt;= mid || idx2 &lt; toIdx &amp;&amp; lst[idx2] &lt; lst[idx1]){            auxList[i] = lst[idx2];            idx2 += 1;        }        else{            std::cout &lt;&lt; "Invalid situation" &lt;&lt; std::endl;            exit(1);        }    }    for(int i = 0; i &lt; (toIdx - fromIdx); ++i){        lst[i + fromIdx] = auxList[i];    }}        void mergeSort(int lst[], int length){    return _mergeSort(lst, 0, length);}int main(){    using namespace std;    srand(time(NULL));    std::chrono::duration&lt;double&gt; mergeTime;    std::chrono::duration&lt;double&gt; qTime;    std::chrono::duration&lt;double&gt; heapTime;        ofstream qDat("qSort.dat"), mDat("mergeSort.dat"), hDat("heapSort.dat");    for(int i = 0; i &lt; 1000; ++i){        int length = i * 9 + 100;        int lst1[length];        int lst2[length];        for(int j = 0; j &lt; length; ++j)            lst1[j] = rand()%10000;        for(int j = 0; j &lt; length; ++j)            lst2[j] = lst1[j];        auto from = std::chrono::system_clock::now();        mergeSort(lst2, length);        auto to = std::chrono::system_clock::now();        mergeTime = (to - from);                for(int j = 0; j &lt; length; ++j)            lst2[j] = lst1[j];        from = std::chrono::system_clock::now();        qsort(lst2, length);        to = std::chrono::system_clock::now();        qTime = (to - from);                for(int j = 0; j &lt; length; ++j)            lst2[j] = lst1[j];        from = std::chrono::system_clock::now();        heapSort(lst2, length);        to = std::chrono::system_clock::now();        heapTime = (to - from);        qDat &lt;&lt; length &lt;&lt; " " &lt;&lt; qTime.count()     &lt;&lt; "\n";        hDat &lt;&lt; length &lt;&lt; " " &lt;&lt; heapTime.count()  &lt;&lt; "\n";        mDat &lt;&lt; length &lt;&lt; " " &lt;&lt; mergeTime.count() &lt;&lt; "\n";    }    qDat.close();    mDat.close();    hDat.close();}One minor result is that worst-case of qsort is awefull enough to make interests.(Worst-cases for qsort was a list with elements with the same value)]]></content>
      <categories>
        
          <category> algorithm </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Heap sort]]></title>
      <url>/algorithm/2020/11/15/Heapsort/</url>
      <content type="text"><![CDATA[Heap sort is an in-place algorithm that sorts the list with heap.It first changes a list to the heap and then keeps poping it and push the maximum value to the end of the list.To change a list to the heap in-place, list is assumed to be form of complete binary tree.With this assumption, it read from bottom to up and downHeap to change complete binary tree to heap.This takes $O(n log n)$(Infact, it is known to be $O(n)$ with carefull analysis).After that, just pop head and push it to end of the list.As a result, it is $O(n log n)$ in-place sorting algorithm.def downHeap(lst, length, idx):    if 2 * idx + 2 &lt; length:        if lst[idx] &lt; lst[2 * idx + 1]:            if lst[2 * idx + 1] &lt; lst[2 * idx + 2]:                lst[idx], lst[2 * idx + 2] = lst[2 * idx + 2], lst[idx]                downHeap(lst, length, 2 * idx + 2)            else:                lst[idx], lst[2 * idx + 1] = lst[2 * idx + 1], lst[idx]                downHeap(lst, length, 2 * idx + 1)        elif lst[idx] &lt; lst[2 * idx + 2]:            lst[idx], lst[2 * idx + 2] = lst[2 * idx + 2], lst[idx]            downHeap(lst,length, 2 * idx + 2)    elif 2 * idx + 1 &lt; length:        if lst[idx] &lt; lst[2 * idx + 1]:            lst[idx], lst[2 * idx + 1] = lst[2 * idx + 1], lst[idx]            downHeap(lst, length, 2 * idx + 1)    else:        passdef listToHeap(lst):    height = 0    sumE   = 0    while sumE &lt; len(lst):        height += 1         sumE   *= 2        sumE   += 1    for i in range(height - 2, -1, -1):        for j in range(2 ** i - 1, 2**(i + 1) - 1):            downHeap(lst, len(lst), j)def heapSort(lst):    listToHeap(lst)    for i in range(len(lst)-1, 0, -1):        lst[0], lst[i] = lst[i], lst[0]        downHeap(lst, i, 0)#Test codeimport randomif __name__ == "__main__":    for i in range(1000):        lst = []        for j in range(1000):            lst.append(random.randint(0,10000))        targetIdx = random.randint(0,len(lst) - 1)        lst2 = list(lst)        #Will crush the qsort        #lst2[0] = 0        heapSort(lst2)        if sorted(lst) != lst2:            print('wrong result for heapSort')            raise    print('Every thing done well!')]]></content>
      <categories>
        
          <category> algorithm </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Quick sort and quick select]]></title>
      <url>/algorithm/2020/11/14/QuickAlgorithms/</url>
      <content type="text"><![CDATA[Quick sort and quick selection are algorithms based on the partition algorithm.Partition algorithm makes a two sublist($\mathcal{A},\mathcal{B}$) and one value(pivot$ = \gamma$).Every element in $\mathcal{A}$ is less or equal than $\gamma$.Every element in $\mathcal{B}$ is greater than $\gamma$.Notice that $\mathcal{A}$ can have some elements which have the same value with $\gamma$ but not $\gamma$ itself and $\mathcal{A},\mathcal{B}$ may be $\emptyset$.As a result, partition function makes $\gamma$ is the highest value of $\mathcal{A}$.Quick sort uses this property to sort the list.After partition function, $\mathcal{A}$ and $\mathcal{B}$ is sorted to each other by $\gamma$.Therefore, quick sort just sort recursively in $\mathcal{A}$ and $\mathcal{B}$.Quick selection do the simillar thing.Quick selection is an algorithm to find a $n$th element in the entire list.If the order of $\gamma$ is equal with what you seek for, it’s done.If the order of $\gamma$ is less than what you seek for, adjust the order to find and try to check $\gamma$ in $\mathcal{B}$ again.If the order of $\gamma$ is greater than what you seek for, adjust the order to find and try to check $\gamma$ in $\mathcal{A}$ again.It works like semi-quick sort, it sorts a little bit and tries to find the elements in $n$th order and keep going on.def partition(lst, fromIdx, toIdx):    '''    Q-sort partition algorithm    [fromIdx,   toIdx)    inclusive, exclusive    It is inplace, mutatable function    return list partition point of pivot    (smaller or equal than pivot) (pivot) (larger than pivot)    '''    selectPoint = int((toIdx + fromIdx)/2)    lst[fromIdx],lst[selectPoint] = lst[selectPoint],lst[fromIdx]    criteria = lst[fromIdx]    lIdx = fromIdx    rIdx = toIdx - 1    while lIdx &lt; rIdx:        while lIdx &lt; rIdx:            if lst[lIdx] &gt; criteria:                break            lIdx += 1        if lIdx == rIdx:            break        while lIdx &lt; rIdx:            if lst[rIdx] &lt;= criteria:                break            rIdx -= 1        lst[lIdx], lst[rIdx] = lst[rIdx], lst[lIdx]    if lst[lIdx] &gt; lst[fromIdx]:        lIdx -= 1    lst[lIdx], lst[fromIdx] = lst[fromIdx], lst[lIdx]    return lIdxdef _qsort(lst, fromIdx, toIdx):    if toIdx - fromIdx &lt;= 1:        return    mid = partition(lst, fromIdx, toIdx)    _qsort(lst, fromIdx, mid)    _qsort(lst, mid + 1, toIdx)    def qsort(lst):    _qsort(lst, 0, len(lst))def _qselect(lst, idx, fromIdx, toIdx):    if toIdx - fromIdx &lt;= 1:        return    mid = partition(lst, fromIdx, toIdx)    if mid == idx:        return    if idx &lt; mid: # target is inside of left handSide        _qselect(lst, idx, fromIdx, mid)    else:        _qselect(lst, idx, mid + 1, toIdx)        def qselect(lst, idx):    if len(lst) &lt;= idx:        return -1    _qselect(lst, idx, 0, len(lst))    return lst[idx]#Test codeimport randomif __name__ == "__main__":    for i in range(1000):        lst = []        for j in range(1000):            lst.append(random.randint(0,100))        targetIdx = random.randint(0,len(lst) - 1)        lst2 = list(lst)        #Highly crush the qsort        #lst2[0] = 0        qsort(lst2)        if sorted(lst) != lst2:            print('wrong result for qsort')            raise                #Lowly crush the qselect        #lst[0] = lst[0] - 1        #Highly crush the qselect        #lst[0] = lst[0] - 20        if qselect(lst, targetIdx) != lst2[targetIdx]:            print('wrong result for qselect')            raise    print('Every thing done well!')]]></content>
      <categories>
        
          <category> algorithm </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Merge sort]]></title>
      <url>/algorithm/2020/11/14/MergeSort/</url>
      <content type="text"><![CDATA[Merge sort is a naive way to sort with divide and conquer methodology.It needs $O(n)$ memory space unlike quick sort.However, it has only $O(n \log n)$ time complexity even at the worst-case.def _mergeSort(lst, fromIdx, toIdx):    if toIdx - fromIdx &lt;= 1:        return    mid = int((toIdx + fromIdx)/2)    _mergeSort(lst, fromIdx, mid)    _mergeSort(lst, mid, toIdx)    #Where extra space is needed.    auxList = [0] * (toIdx - fromIdx)    idx1 = fromIdx    idx2 = mid    for i in range(toIdx - fromIdx):        if idx2 &gt;= toIdx or idx1 &lt; mid and lst[idx1] &lt;= lst[idx2]:#choose idx1            auxList[i] = lst[idx1]            idx1 += 1        elif idx1 &gt;= mid or idx2 &lt; toIdx and lst[idx2] &lt; lst[idx1]:#choose idx2            auxList[i] = lst[idx2]            idx2 += 1        else:            print('Invalid situation')            raise    for i in range(toIdx - fromIdx):        lst[i + fromIdx] = auxList[i]        def mergeSort(lst):    return _mergeSort(lst, 0, len(lst))#Test codeimport randomif __name__ == "__main__":    for i in range(1000):        lst = []        for j in range(1000):            lst.append(random.randint(0,100))        targetIdx = random.randint(0,len(lst) - 1)        lst2 = list(lst)        #Will crush the qsort        #lst2[0] = 0        mergeSort(lst2)        if sorted(lst) != lst2:            print('wrong result for qsort')            raise    print('Every thing done well!')]]></content>
      <categories>
        
          <category> algorithm </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
